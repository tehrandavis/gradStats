--- 
title: "musings of the Professor"
author: "Tehran J. Davis"
date: "`r Sys.Date()`"
site: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Weekly examples and musings related to PSYC 7014."
---

# Weekly musings {-}

In these (roughly) weekly musings I will supplimenting the course texts and in-class examples with some written walk-thoughs. In previous years students have informed me that they got a lot out of these little jaunts. My hope is that they will be of some use to you.

If you want to follow along you can copy and paste the relevant code chunks into your own Rmd file and execute.

Note that in what follows code chunks and outputs are in blockquotes with the colored tint. Outputs that are generated by the code have hash marks in front of them.

```{r eval=FALSE, echo=FALSE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r setup, include=FALSE, echo=FALSE}
r <- getOption("repos")
r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
options(repos = r)
require(knitr)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

<!--chapter:end:index.Rmd-->

# Introduction to R / getting to know your data

This week we aim to suppliment the assigned readings with a few blurbs and examples in `R`. We will go over how to install packages and load data into the `R` environment, and will revisit our discussion of simple models with a walkthrough. Let's go!

Two of the first skills that you are going to need to learn are 1) how to install / load your required packages into the R environment and  2) how to import data from a file. More often than not, these will be the first two steps of your analysis (though if you are working on more complex projects you may find it easier to keep track of things if you load import your data and packages as needed). 


## Installing and loading an R package:

On of the great things about `R` is its exensibility via packages. For other great things about `R` (or how to sell `R` to your peers) see [this link](https://simplystatistics.org/2017/10/30/how-do-you-convince-others-to-use-r/) on [simplystatistics.org](https://simplystatistics.org). In fact this blog is a great resource for staying in contact with topics and new developments in data analysis.

From Peng's article:

_"With over 10,000 packages on CRAN alone, [there’s pretty much a package to do anything](https://youtu.be/yhTerzNFLbo). More importantly, the people contributing those packages and the greater `R` community have expanded tremendously over time, bringing in new users and pushing `R` to be useful in more applications. Every year now there are probably hundreds if not thousands of meetups, conferences, seminars, and workshops all around the world, all related to R."_

For example, in my own work I often find myself not only using statistical techniques such as growth curve modelling (package: `lmer`) but also advanced quantification techniques such as cross recurrence quantificantion analysis (package: `crqa`), MultiFractal Detrended Fluctuation Analysis (package: `MFDFA`), and Multivariate Sample Entropy Analysis. Ten years ago this involved 100's of lines of custom programming to carry out these analysis... today, there's an `R` package for that—i.e., someone else likely with far greater programming abilities than I has already done it.


### Installing a package using the GUI

For the point-and-click crowd, the easiest way to install an R package is to locate the `Packages` tab in your Window. From here you can click install. From there you can type in the package that you wish to install. So for example, let's install the `psych` package, which is useful for obtaining descriptive stats from your data (although to be honest I hardly use it). Check out an example from [this video on Box](https://uc.box.com/s/hfw6b4uo81wjtmukbk8jy0fx5ak40lus).

As you see the video, I left the `Install dependences` box ticked. *You should always install with dependencies*, this automatically installs other packages that may be required for `psych` (or whatever package of interest) to work. You may have also noticed when after you clicked OK, `R` entered this line of code into your `Console`.

```install.packages("psych")```


Which leads us to...

### Installing packages with `install.packages()`

A basic install of `R` comes with the `base` package with has an unbelievable large number of functions and analyses built in. However `base` `R` is accompanied by a steep learning curve, especially for those with no programming experience. In this case we will use many of the simple functions in `base` R, but for more complex analysis and plotting with use the wonderful `tidyverse` package. `tidyverse` piggybacks on the basic install, replacing the sometimes bewildering `R` syntax with more natural language. Here, we'll use this installation as an example of how to install using command-line.

Installing `tidyverse` (or any package for that manner) couldn't be easier. At the prompt (or in your notebook) simply type:

```{r eval=FALSE}
install.packages("tidyverse")
```

Congratulations you have installed over 70 new packages to your `R` environment!

You see, `tidyverse` is not a single package (as is usually the case), but a collection of packages that function seamlessless with one another abiding a shared ethos of data science practices (who knew there were ethoses in stats! A simple google search of `tidyverse` might lead you to believe that you have joined a cult with Hadley Wickham as your leader!). The core value is that data should be structured, analyzed, presented, and shared in a manner that is as transparent as possible. In this class, we aren't going to go the "full Hadley" (all the way down the rabbit hole) but we are going to abide many shared principles (it's just good science!).


Note for the `crqa` package I simply type `install.packages("crqa")` and for MFDFA `install.packages("MFDFA")`. You noticing a pattern here?

### An IMPORTANT note on knitting with install.packages:

One quirk with knitting from your source is that you will need to specify which CRAN mirror you will download the package from. The Comprehensive R Network (CRAN) is an online repository that houses almost all things R, including packages. There are multiple mirrors set up all over the globe (https://cran.r-project.org/mirrors.html) and you need to tell R which one is your preferred choice. The quirk is that you only need to do this when knitting/compiling. You don't need to do this for other everday use, but your source won't compile unless you fix this (see [here](https://stackoverflow.com/questions/33969024/install-packages-fails-in-knitr-document-trying-to-use-cran-without-setting-a) to read more about the error this produces.

Since your homework is going to involve knitting you should probably get in the practice of fixing this. There are two ways to resolve this issue. The first is to simple add the `repos` argument to your `install.packages()` command like so:

``install.packages(package, repos="http://cran.us.r-project.org")``

Using this method you would need add the `repos` argument and url to every item you wish to install.

A better, more efficient way is to provide a default repo at the outset. This can be accomplished by creating a new chunk at the beginning of your Rmd file (right underneath your header), and inserting the following:

    setRepositories(graphics = getOption("menu.graphics"),  
    ind = NULL, addURLs = character())
    r <- getOption("repos")
    r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
    options(repos = r)

### Loading packages with `library()`

Once you have installed a package, it remains stored on your hard-drive until you delete it (which you'll amost never do, packages take up so little disk space it's really no efficient to install and uninstall unless absolutely necessary). What many new users have difficulty getting used to is that just because a package is _installed_ does not mean that it is _loaded_. To load `tidyverse`, we typically type:

```{r}
library(tidyverse)
```

That's it, easy-peasy. Note that by default, only the `base` packages are typically loaded when you start a new session. Therefore, *a useful practice to get into is to load the necessary packages at the start of every session.*

Before moving on I'd like to point out two things. First, notice that when installing with `install.packages()` we included quotations around the package name, but with `library()` we did not. Knowing when to use quotes and when not to is one of the more frustrating things to grasp for beginners (and even is an annoyance for me from time to time). All I can say is with practice, it becomes automatic. In truth, in the case of `library()` whether you include the quotes or not doesn't matter (the default is to not), but for many functions, including `install.packages()`, the proper quotes do matter (for example: `install.packages(tidyverse)` will result in an error)

Second, while installing `tidyverse` installs >70 packages on your computer, `library(tidyverse)` only loads 12 or so primary packages. This isn't really much of an issue as we will be mostly using this common `tidyverse` packages, but it's worth mentioning for future reference.

### Loading packages with `require()`

_*Note that this section is a little more advanced and assumes you have taken a look at the Demos readings assigned for this week in particular those related to assigning variables (Ch 1) and, importantly, logicals (Ch 3). If you want, you can stop here and be perfectly able to install and load `R` packages. But for those that want to Level-Up, this section and the next outline what I believe is the best way to install and load libraries when creating R-Notebooks for sharing (i.e. turning in your assignments). Of course if you don't really care about the logic and just want to get the cheat code, you may proceed to the end of the next section*_

There is in fact another way to load a previously installed package, `require()`:

```{r}
require(tidyverse)
```

`require()` has the additional benfit of returning a "FALSE" logical output if the requested package hasn't been installed on your computer (see Demos, Ch 3.1 for more on logicals). This is in contrast to `library()` which just freaks out and reports an error (errors are bad as they can stop your execution). For example, let's save the output of the following actions to an object called `myVar`.

For example running:
```{r eval=FALSE}
myVar <- library(xkcd)
myVar
```

versus

```{r eval=FALSE}
myVar <- require(xkcd)
myVar
```

In both cases `xkcd` was not installed on your comp and an error was returned. However, with `library()`, nothing was saved to `myVar`, but `require()` retured a `FALSE` indicating that the requested package was not installed. Why do I bother to even bring this up?

### Installing and loading packages like a ninja

The central reason that we are using R is to get in the practice of data transparency and replicability. Ultimately, for every analysis that you perform you should be able to provide me with the appropriate syntax in your notebook file and I should be able to re-run each of your analyses step by step on my own computer. One important consideration is that you may be using packages that I don't have installed and loaded on my computer and vice versa. To deal with this you would need to include two lines for every package:

1. `install.packages("package")`
2. `library(package)`

If you use a lot of packages, this can become very repetitive (imagine using 10 packages).

A much more efficient way of doing things is to take advantage of that `FALSE` that `require()` returns. But first we need to install a package called `pacman`. Before we do this, input the next to lines seperately:
```{r}
show(require(pacman))
show(!require(pacman))
```

Note that the show function forces `R` to show the output. Recall from above that `require()` produces a `FALSE` if the requested package is NOT installed on your computer. So, assuming that `pacman` is not installed on your computer the first line will produce a `FALSE` and the second will produce a `TRUE`. 

So what is that second line then? NEGATION. 

Placing an exclation point in fron of a statement essentially reads as "not true". For now just understand that using this syntax we can create a line of code that checks to see *if* `pacman` is installed on your computer, and if *not* it installs it:

```{r}
if (!require("pacman")) install.packages("pacman")
```

As outlined in Demos, Ch. 3, the above is a conditional if-statement. It literally reads:

1. Check to see if the statement `!require("pacman")` is `TRUE` (i.e., TRUE = `pacman` is not installed on your computer).
2. If the above indeed returns a `TRUE`, then run `install.packages("pacman").

In theory you could run this line for every package, but that would be tedious as well and is not really a simpler solution. Fortunately `pacman` contains a function that simplifies this for us, `pacman::p_load()` function. To get a feel for what this function does run the following line:

```{r}
? pacman::p_load()
```

The `?` brings up an online help module for the named function. In this case the function is `p_load()` from the `pacman` package.

You see that `pacman::p_load()` checks to see if a package is installed, if not it attempts to install the package from CRAN and/or any other repository. After that it loads all listed packages.

Let's try a few packages that you haven't installed but are going to be useful to us later when we do ANOVA. FWIW I usually have a code chunk at the top of every notebook that uses this template, swapping in the various packages that I intend to use.

****** HERE'S THE CHEAT CODE!!!  *******

```{r}

# 1. check to see if pacman is on your computer and if not, let's install it:
if (!require("pacman")) install.packages("pacman")

# 2. install all other packages that we will be using:
pacman::p_load(afex,lmerTest,plyr,car)
```

This bit of code installed and loaded the `afex`, `lmerTest`, and `plyr` packages. To test that everything worked, try:

```{r}
data(obk.long, package = "afex")
afex::aov_ez("id", "value", obk.long, between = c("treatment", "gender"),
             within = c("phase", "hour"), observed = "gender")
```

Congrats! You've just run a 2x2 mxed effects ANOVA! We'll revisit what exactly is going on here in 8 weeks.

## Loading in data
Typically the file types that are used by beginners in R are plain text and delimited. They may have the extension "txt", "csv", or "dat" for example. These may become more sophisticated you progress, for example you can load in proprietery types like SPSS and STATA, but for this course we will mostly use plain text files (although later in the course I will show you how to load in Excel files).

### Loading in local data using a GUI:

As with loading packages, you can also load in a file containing data using the RStudio GUI. See [this video](https://uc.box.com/s/c6cdjxo4ygu8516tlxalhyvojb5n1lok)

Again, this is only preferred if you are not sharing your analysis. If you are sharing your analysis (as in this class) you need to do the command line. Fortunately, RStudio creates the appropiate command-line for you to copy and passte. For example for my computer it's:

```{r}
LexicalDescisionData <- read.delim("~/UCedu/Courses/gradStats/week1/LexicalDescisionData.txt")
```

I've uploaded this file to Box in the "exampleData" folder. Please feel free to download the file and follow along is you wish. Note that *if you do elect to load in via the GUI you need to be sure to copy the output to your Rmd source*.

### importing data from the web

You might be saying to yourself, "but Tehran the entire reason you've got us learning `R` is for transparency and openess with our data. How would I be able to share in my code a data file that resides on my hard drive?!?"" 

Correct, you can't, but you can upload it to the internet and someone can access it from an online repository. Personally, I like to use Github, but we'll save that for some advanced stuff later in the semester for those so inclined.

For now, several of our in-class examples will come from the Howell textbook. Howell has an online repository on his website that contains data sets for examples in the text:

We can load in this data from the web using the `read.delim()` function from above. Let's assign it to an object `RxData`
```{r}
RxData <- read.delim('https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat',
                     header = T,sep = "")
```

To see what's going on with the additional calls in this line, run the following line to get help:
```{r eval=FALSE, include=TRUE}
? read.delim
```

A document file should show up in your help tab, containing examples and describing what different arguments are doing. Search for `header` and `sep` and try to figure out what's going on.

### Downloading data

The example above just pulls data directly from a url, what if you want to download the data file directly onto your computer and load it from there?

Well, _there's a package for that_... `downloader`. Let's install this package and download the data from above:

```{r}
# within the R code sections, hashtags create comments, sections of code that are not interpreted by the computer, but may serve to inform others (and typically yourself later in life) about what exactly in the hell is going on here. GET IN THE PRACTICE OF COMMENTING YOUR CODE. You'll thank yourself later. Here I'm using comments to inform you step-by-step what each line is doing:

# install and load "downloader" package, this assumes you have "pacman" installed and loaded *see section 1.1.6 above:
pacman::p_load(downloader)

# get the url of the file you want to download and assign it to an object ("dataURL"):
dataURL <- "https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat"

# decide on what name you want to give the file. In this case I'm extracting using the basename from the web url: Tab2-1.dat
filename <- basename(dataURL)

# download the file to your current R-project folder:
download(url = dataURL, filename)
```

Keep in mind that objects are just placeholders. So if I was so inclined I could have accomplished all of the above with just one line:
```{r}
download(url = "https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat", destfile="Tab2-1.dat")

# destfile is a parameter for naming what you download.
```

From here I could just import the `Tab2-1.dat` file from my computer using the GUI method above.

## Looking ahead...
This is probably a good place to stop for now. In the meantime try running the following 4 commands (assuming that you have imported `RxData`) and think about what they are returning:

```{r eval=FALSE}
class(RxData)
names(RxData)
head(RxData)
summary(RxData)
```

<!--chapter:end:01week.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Measures of distribution, central tendency, PLOTS... oh my!

We covered a lot of ground this week talking about measures of central tendency and introducing ourselves to distributions. When you get a chance be sure to check out some of the in-class workshopping code and examples as they are posted on Box. These will help you remember what we did in the workshop and you can compare them to your source and output.

This week I want to expand upon a few things that were touched upon in the workshop and in lecture. This one might be a little lengthly, but it should serve to supplement your assigned readings and help you a little with the home work. Before jumping in, let's install and load the necessary packages in R. This week we will make use of `psych` and several of the core packages of the `tidyverse`. I'll also be using `data.table` which provides another way of importing data (in addition to the examples from last week). Using the `cheat code` from last week's walkthrough

```{r}
# 1. check to see if pacman is on your computer and if not, let's install it:
if (!require("pacman")) install.packages("pacman",repos="http://cran.us.r-project.org")

# 2. install all other packages that we will be using:
pacman::p_load(psych, tidyverse, data.table)
```


## Getting measures of central tendency

In this week's workshop, we touched upon using `psych::describe()` to generate summary stats of data in a data frame, as well as generating a simple histogram using the `hist()` function. Our starting point was data from an example provided in the Howell textbook.

```{r}
# importing data, here I'm using data.table. I prefer data table as it produces truncated output. 
exampleData <- data.table::fread("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat")
```

Invoking `class(exampleData)` tells us that exampleData is a data frame. Invoking `names(exampleData)` provides us with the header names:

```{r}
class(exampleData)
names(exampleData)
```

From here we discussed how to call upon a specific column in a data frame using the `$` operator:
```{r}
# column of Trial(s):
exampleData$Trial
# column of RxTime (reaction time)
exampleData$RxTime
```

and how to get some important summary statistics on the entire data frame using `psych::describe`:
```{r}
psych::describe(exampleData)
```

You may also get summary statistics for individual groups. In this case the `NStim` column represents experimental conditions (num. of stimuli = 1, num. of stimuli = 3, num. of stimuli = 5). To take a look at the summary stat by condition we used the `psych::describeBY()` function:
```{r}
psych::describeBy(exampleData,group = exampleData$NStim)
```

We also noted that some summary stat information can be obtained using the `summary()` function. The basic install comes with the `summary()` function that, when applied to a data frame provides a minimal number of summary stats. NOTE: the summary() function is a jack-of-all trades and when used in other instances may provide drastically different outputs (e.g., it may summarize a linear model). For this data set:
```{r}
summary(exampleData)
```

In general I prefer `psych::describe()` to `summary()` when getting descriptive stats, but introduce `summary()` here to use it provide an example further in this week's chapter.

## Plotting the distribution with a histogram

### Using `hist()`
In class we used the `hist()` call on a vector of scores to produce a quick and dirty histogram. For example to produce a histogram of `RxTime` from our `exampleData` dataset we can:
```{r}
hist(exampleData$RxTime) # R auto-selects number of bins
hist(exampleData$RxTime, breaks = 5) # create 5 bins 
```

The second line above forces `R` to create 5 bins. If you recall from Howell, one rule of thumb for the number of bins (there are many) is to use the square root of the number of observations (N). To do that here we can first use the `length()` function to get the number of observations (i.e., the length of the `exampleData$RxTime` vector of scores):

```{r}
length(exampleData$RxTime) # length of vector
```

*ASIDE:* Note that length works on vectors, but not on entire data frames (multiple columns of data). Vectors are 1 dinmensional... thing of a line of data. Data sets (or data frames) are 2D (colmuns and rows of data). Since each column contains our variables, the number of obervations corresponds to the number of rows in the data set. So, to get the "length" you just need to count the number of rows:

```{r}
nrow(exampleData) # number of rows in the data set (each row corresponds to an observation)
```

Returning to the 'optimal' number of bins, be can now put together the following:
```{r}
N <- length(exampleData$RxTime)
hist(exampleData$RxTime, breaks = sqrt(N)) # create square root of N bins 
```

### Plotting a histogram with `ggplot()`

*This section assumes you have completed this week's DataCamp assignment that includes a wonderful introduction to plotting with `ggplot()`. If you have yet to do so, please go there first.*

Now let's try fitting a normal curve to our histogram. For this we are going to move into more complex ways of plotting data using `ggplot2` from the `tidyverse`. You'll be introduced to the `tidyverse` in this week's DataCamp homework assignments. I suggest looking there and at the Demos Ch. 10 & 11 for another walkthrough. The Field text, Ch. 4 also covers ggplot extensively.

Fisrt let's use `ggplot()` to create a histogram. To get a feel for each step, run through this code line by line in RStudio: 

```{r}
# load in tidyverse if you haven't already
pacman::p_load(tidyverse)

# intiial step i identify data and grouping parameters:
histPlot1 <- ggplot2::ggplot(data = exampleData, aes(x=RxTime))
histPlot1 # note that this produces a blank slate but the parameters are locked in.

# take histPlot 1 (blank plot) and paint a layer on top of it
# tell ggplot what to do (this is where you actually build the graphics):
histPlot2 <- histPlot1 + 
  geom_histogram(binwidth = 5, 
                color = "orange", # what color is the outline of bars
                fill = "green") # what color to fill the bars with
histPlot2 
```

I intentially made a hideous looking plot above to show you what the additional arguments do. Also note that with `ggplot()` you modify the width of the bin `binwidth` rather than the number of bins, or `breaks` as in `hist()`.

Okay, now to add a curve representing the normal distribution. One important thing to note is that the histogram that is ultimately produced here has probability density (think like % of scores) on the y-axis instead of frequency (# of scores).

```{r}
# take histPlot 1 (blank plot) and paint a layer on top of it
# tell ggplot what to do (this is where you actually build the graphics):
histPlot3 <- histPlot1 + 
  geom_histogram(binwidth = 5, 
                 color = "black",
                 fill = "white",
                 aes(y=..density..)) # turns plot into density plot
histPlot3 
```

notice that histPlot3 is a density plot rather frequency plot from above

```{r}
# add a normal curve to density plot:
histPlot4 <- histPlot3 + 
  stat_function(fun = dnorm, # generate theoretical norm data
                color = "red", # color the line red
                args=list(mean = mean(exampleData$RxTime), # build around mean 
                sd = sd(exampleData$RxTime))) # & standard devation

histPlot4
```

Howell also mentions a better alternative to the normal curve, a kernel density plot. To fit a kernel density plot to our histogram we can invoke:

Howell also mentions a better alternative to the normal curve, a kernel density plot. To fit a kernel density plot to our histogram (histPlot3) we can invoke:

```{r}
# hisPlot 3 was the base histogram without curve
histPlot3 + geom_density()
```

and let's make the x-axis label a little more transparent with `xlab()`
```{r}
histPlot3 + geom_density() + xlab("Reaction time (ms)")
```

Any guesses on how to change the y-axis label to "Percent of scores"?

## Plotting your data in a boxplot:

Let's recreate the boxplot from slide 7 of the lecture (Figure 2.15 - Howell). Before we proceed, I would like to draw you attention to a characteristic of the `NStim` column in our `exampleData` dataset.

```{r}
psych::describe(exampleData)
```

### turning numeric data into categorical

As you can see we get a mean, sd, median, etc for `NStim`, BUT SHOULD WE? The number of stimuli is not numerical properly... but instead represent experimental categories. The same can be said for the `YesNo` data. We need to tell `R` that these data are categories, or _factors_ and not continuous numbers. To do so we can call on the `factor()` function as below.

```{r}
# convert a numeric to a categorical (i.e., make a factor)
factor(exampleData$NStim)
```

When you run the line above you'll note at the end of the output it mentions "Levels: 1 3 5". This is telling you that in this vector, there are three levels, or conditions in this factor- number of stimuli = 1; number of stimuli = 3; number of stimuli = 5. To connect this factorized vector to your original data set, you can either create a new column in the data set or overwrite the original `NStim` column that you are replacing. For beginners I would recommend adding a new column.

```{r}
# 1. create a new vector attached to the original data frame
# To do this you can create a blank column by invoking 
# [dataset$newcolumn name] and then fill it with the factorized data:

exampleData$categoricalNStim <- factor(exampleData$NStim)
psych::describe(exampleData) 
```
The asterisk next to categoricalNStim lets you know it's categorical. Aternatively you can use `summary()`:

```{r}
summary(exampleData)
# also note that summary() now tells you the number of obsevations 
# rather than trying to compute descriptive stats


# 3. or you can overwrite the original column
# Be careful doing this. Once you overwrite you
# can't go back

exampleData$NStim <- factor(exampleData$NStim)
psych::describe(exampleData) # star lets you know it's categorical
summary(exampleData)
```

### boxplotting using `ggplot()`

To create a boxplot of the entire data set (regardless of `NStim`) we can:
```{r}
boxPlot <- ggplot2::ggplot(data = exampleData, aes(y=RxTime))

boxPlot1 <- boxPlot + geom_boxplot()

boxPlot1
```

Note that the `x-axis` in this plot is meaningless.

Now let's take a look at the data that will be boxplotted as a function of `NStim` groups. Here I want to note that `psych::describeBy` has an additional argument to display the quantiles:

```{r}
psych::describeBy(exampleData,group = exampleData$NStim,quant=c(.25,.75))
```

To plot this we need to tell `R` to put different distributions at different points along the `x-axis`

```{r}
boxPlot <- ggplot2::ggplot(data = exampleData, aes(y=RxTime, x = categoricalNStim))

boxPlot1 <- boxPlot + geom_boxplot()

boxPlot1
```

Before leaving I want to show what would've happended in the latter case if we didn't correct `NStim` and left it as a numeric variable rather than converting to a categorical:

```{r}
# re-importing original data
exampleData <- data.table::fread("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat")

# creating the boxplot using exact code from chunk above:
boxPlot <- ggplot2::ggplot(data = exampleData, aes(y=RxTime, x = NStim))

boxPlot1 <- boxPlot + geom_boxplot()

boxPlot1
```

## Assessing the data for normality

As mentioned in class, one of the key underlying assumptions of our inferential stat is that the data being analyzed is normally distributed. For this example I'll be using some data that I had intended to work from in this week's workshop. This data is located on the Box drive in `weeklySchedule/week2/workshop/InClass_1.csv`. To proceed, you may download it from BOX drive to your computer and import, or you can access it directly from the shared folder...

```{r}
InClass_1 <- read.delim("https://uc.box.com/shared/static/3u49i5zrtpz5pvtmv566eizobd01oqby.csv", sep = ",")
```

and now plotting this data related to `BehaviorX` with a normal curve overlayed as in slide #19:
```{r}
# load in tidyverse if you haven't already
pacman::p_load(tidyverse)

# identify data and grouping parameters:
histPlot <- ggplot2::ggplot(data = InClass_1, aes(x=BehaviorX)) +
  geom_histogram(binwidth = 5, 
                  color = "black",
                  fill = "white",
                  aes(y=..density..)) + 
  stat_function(fun = dnorm, 
                 color = "red",
                 args=list(mean = mean(InClass_1$BehaviorX),
                 sd = sd(InClass_1$BehaviorX)))
histPlot
```

As you can see the distribution is approximately normal. We can get a further assessment of skew and kurtosis using `psych::describe()`:
```{r}
psych::describe(InClass_1)
```

As conveyed here, our values for skew and kurtosis fall within ideal ranges.

### Q-Q Plots
Another method of assessing normalitly is by using a quantile-quantile, or Q-Q plot. A Q-Q plot is the plot of the sample quantiles from your data against a theoretical normal distribution. Q-Q plots typically use a straight line `y=x` as an assessment device where any deviation away from the straight line indicates deviation from normality.

To assess our data we can invoke both of following functions on our data:
```{r}
qqnorm(InClass_1$BehaviorX)
qqline(InClass_1$BehaviorX)
```

Looking pretty good! Check out `InClass_1$BehaviorY` on your own.

### Generating a theoretical normal distribution.

We can also generate a random normal distribution using the `rnorm()` function. By filling in the appropriate arguments we can create a set of data of a desired length with a desired mean and standarad deviation.

```{r}
numSamples <- 20
set.seed(1) # we'll come back to this next class
randomGenerated <- rnorm(n = numSamples,mean = 45,sd = 5.0)
ranGen_df <- data_frame(randomGenerated) # coerce to a data frame for ggplot

# creating a histogram:
# note I did this all in one step:

normalPlot <- ggplot2::ggplot(data = ranGen_df, aes(x=randomGenerated)) + 
  geom_histogram(binwidth = 5, 
                 color = "black",
                 fill = "white",
                 aes(y=..density..)) +
  stat_function(fun = dnorm, # generate theoretical norm data
                color = "red", # color the line red
                args=list(mean = mean(ranGen_df$randomGenerated), # build around mean 
                sd = sd(ranGen_df$randomGenerated))) # & standard devation

normalPlot
```

see what happens as you increase the number of samples from 20 to 30 to 50, 100, 1000, 10000. What does this suggest about ideal sample size?


## Subsetting your data

Your homework this week asks you to subset your data by condition and create a plot for each resulting condition (or group). For example in our `exampleData` there are 3 conditions by `NStim`: 1, 3, and 5. Using `psych::describeBy()` we can get summary data for each:
```{r}
psych::describeBy(exampleData,group = exampleData$NStim)
```
*but we cannot use this to generate a plot*. In order to do this we need to split the original data file into three parts, one for each `NStim` condition. DataCamp introduces you to the `dplyr::filter()` function this week, which extracts data from a larger set given some criteria. For example to pull our `NStim == 1` data from the `exampleData` data set we can:

```{r}
dplyr::filter(exampleData,exampleData$NStim==1)
```

To create plots related to this data you can save the above result to a new object and then create your plots as described above. For example, to get `RxTime` for `NStim == 1`:

```{r}
NStim1_data <- dplyr::filter(exampleData,exampleData$NStim==1)

# you can use hist() here but I'm going to do ggplot()
NStim1_plot <- ggplot2::ggplot(data = NStim1_data, aes(x=RxTime)) + 
  geom_histogram(binwidth = 5, 
                 color = "black",
                 fill = "white",
                 aes(y=..density..)) +
  stat_function(fun = dnorm, # generate theoretical norm data
                color = "red", # color the line red
                args=list(mean = mean(NStim1_data$RxTime), # build around mean 
                sd = sd(NStim1_data$RxTime))) # & standard devation

NStim1_plot
```

Go back and do this with `NStim==2` and `NStim==3` for practice. Be sure to create new objects for each subset (e.g. `NStim2_data`) and plot (`NStim2_plot) so that you don't overwrite what you've done before.

This is a good place to stop for this week. However if you want to continue on, in the next section I'm going to cash out all of that rambling about "the importance of means" that I've been doing

## Looking ahead: Means and better models 


Hold this one in your back pocket for now. This is the 2nd week in a row that I've stressed the importance of the mean. Last week, I said that the mean is the best guess for any individual score if you know nothing else other than the data is normally distributed. BUT it's a very limited predictor. 

Here we will use `R` to extend our in-class example of the mean as the best model if you've got nothing else going for you. We'll also use this to get some more practice with variable assignment and working with vectors.

Revisting our in class example from Week 1:


```{r}
tehran <-  38
nabiha <- 28
james <- 24
chris <- 22
margaret <- 27
emily <- 22
sarah <- 24
nate <- 26
tyra <- 22
julia <- 24
angela <- 28
allie <- 24
daniel <- 29
sierra <- 24
```
Everyone's age (numerical) is assigned to an object (their name). From here there are several ways to get a mean age for the class. You could just simply follow the formula, the *sum* of the individual ages divided by the number of people in the class:

```{r}
sum(tehran, nabiha,james,chris,margaret,emily,sarah,nate,tyra,julia,angela,allie,daniel,sierra) / 14
```

Alternatively, you could use the `mean()` function. However, mean requires that all of these number be bound together into a vector (see Demos online text, 1.6). This is simply accomplished by using the concationation function,`c()`:

```{r}
# first we concatonate our data:
ages <- c(tehran, nabiha,james,chris,margaret,emily,sarah,nate,tyra,julia,angela,allie,daniel,sierra)

# now we get the mean:
mean(ages)
```

Of the two options, the second is much more preferable. Once you get your individual data vectored (grouped) into assigned object you can perform functions on that object very easily, like:

```{r}
# doubling everyone's age:
ages * 2
# getting the log of everyone's age:
log(ages)
# getting the square root:
sqrt(ages)
# cubing:
ages^3
```

or perhaps most important for our present concerns, getting the mean, median, standard deviation, minimum value, maximum value, skew and kurtosis of our ages:

```{r}
psych::describe(ages) # gets both min and max
```


### mean as highest probability:

Okay, now we are going to demonstrate that if you know nothing else about your data, the mean is the best guess in terms of probability. We know that if your data are normally In this case we can plot our age data to a density histogram. I'm also going to create a vertical line on the plot indicating the mean age:

```{r}
classAges_df <- data_frame(ages) # coerce to a data frame for ggplot

# creating a histogram:
# note I did this all in one step:

classAgesPlot <- ggplot2::ggplot(data = classAges_df, aes(x=ages)) + 
  geom_histogram(binwidth = 1, 
                 color = "black",
                 fill = "white",
                 aes(y=..density..)) +
  stat_function(fun = dnorm, # generate theoretical norm data
                color = "red", # color the line red
                args=list(mean = mean(classAges_df$ages), # build around mean 
                sd = sd(classAges_df$ages))) + 
  geom_vline(xintercept=mean(classAges_df$ages),
             color="red",
             linetype="dashed")


classAgesPlot
```

So not too far off from the actual best guess, mode = 24, which represents over 30% of our scores, but still pretty close.

The next two sections involve a little bit of complex coding that will be rehearsed in next week's workshop. For now I have two goals, to 1. demonstrate the mean produces the least amount of summed error and 2. by virtue of that minimized the sum-of-squared differences, or SS. The latter is arguably more important as SS is used to calculate variability and is most diectly used in assessing inferential models.

### mean as fulcrum point

This week we also decribed the mean as the balance point for our data, that is that the sum of all deviation scores from the mean = 0: $\sum{X-\bar{X}} = 0$. That is the mean values produces the least amount of summed error, 0. 

Again, imagine that you are encountering someone in class and are asked to guess their age (and WIN! WIN! WIN!). The only info that you have is the range of our ages:

```{r}
range(ages)
```

In what follows, I'm going to iterate through each possible age (`guess`) within our range and get the sum of difference scores, or `sumDiff`. `sumDiff` is obtained by subtracting each age from a given value (see Howell 2.8). Typically this value is the mean.  For example say I guessed 27:

```{r}
# 1. guessed age:
guess <- 27
# 2. resulting difference scores, squared
diffScores <- (ages-guess)
# 3. the sum of squares:
sumDiff <- sum(diffScores)

# 4. show the sumDiff (example)
show(sumDiff)
```

Now, I'm going to iterate through every possible age in the range, and save the resulting `sumDiff`s to a vector, `sumDiffguess`:

```{r}

# get ages, repeat of previous chunk:
ages <- c(tehran, nabiha,james,chris,margaret,emily,sarah,nate,tyra,julia,angela,allie,daniel,sierra)

# create a vector of all possible integers within our range:
rangeAges <- min(ages):max(ages)

# create an empty vector to save our resulting sum of differences
sumDiffguess <- vector()
guessedAge <- vector()

# here "i" indexes which value from the range to pull, for example when i=1, we are pulling the first number in the sequence
for(i in 1:length(rangeAges)){
  guess <- rangeAges[i]
  diffScores <- (ages-guess)
  sumDiff <- sum(diffScores)
  # save results to sumDiffguess
  guessedAge[i] <- guess
  sumDiffguess[i] <- sumDiff
}

diffAges_df <- data_frame(guessedAge,sumDiffguess) #combine the two vectors to single data frame
show(diffAges_df)
```

Now let's plot the resulting sum of difference scores as a function of guessed age. The vertical line represents mean age. As you can see this is where our function crosses 0 on the y-axis.

```{r}
# note I did this all in one step:

sumDiffPlot <- ggplot2::ggplot(data = diffAges_df, aes(x=guessedAge, y=sumDiffguess)) + 
  geom_point() + geom_line() +
    geom_vline(xintercept=mean(ages), # using actual ages from 
             color="red",
             linetype="dashed")


sumDiffPlot
```

### mean as minimizing random error

Of course, in our statistics the sum of squared-differences (or SS) $\sum{(X-\bar{X})^2}$ is our primary measure rather than the sum of difference in the last example. SS is used to calculate varience $\frac{\sum{(X-\bar{X})^2}}{N-1}$and in turn standard deviation, which is just the square root of the variance. Both measures we use to describe variability in a distibution of data. When we know nothing about our data we assume that the scores, and therefore the resulting difference or _error_ from the mean is _randomly_ distributed. Hence the term _random error_. 

First, let's calculate the resulting SS for each possible guess (similar to above):

```{r}
# create a vector of all possible integers within our range:
rangeAges <- min(ages):max(ages)

# create an empty vector to save our resulting sum of differences
SSguess <- vector()
guessedAge <- vector()

# here "i" indexes which value from the range to pull, for example when i=1, we are pulling the first number in the sequence
for(i in 1:length(rangeAges)){
  guess <- rangeAges[i]
  diffScores <- (ages-guess)
  SSDiff <- sum(diffScores^2)
  # save SS to SSguess
  guessedAge[i] <- guess
  SSguess[i] <- SSDiff
}

SS_Ages_df <- data_frame(guessedAge,SSguess) #combine the two vectors to single data frame
show(SS_Ages_df)
```

Now, let's see what happens when we plot the resulting SS as a function of guessed age:
```{r}
SS_Plot <- ggplot2::ggplot(data = SS_Ages_df, aes(x=guessedAge, y=SSguess)) + 
  geom_point() + geom_smooth() +
    geom_vline(xintercept=mean(ages), # using actual ages from 
             color="red",
             linetype="dashed")


SS_Plot
```

As you can see we get parabola with a minima at our mean. 

### but we can do better than that

The mean is good as a stab in the dark, but as you can see our SS is still pretty high. If we have other useful information about the nature of our age data, then maybe we can use that to make better preditions. These extra bits of info are our *predictors*. For example, let's combine our age data with info on how many years everyone has been out of undergraduate. For the purposes of this example, I'm just going to make this data up:

```{r}
tehran <-  13
nabiha <- 5
james <- 2
chris <- 1
margaret <- 3
emily <- 1
sarah <- 4
nate <- 2
tyra <- 1
julia <- 3
angela <- 4
allie <- 4
daniel <- 5
sierra <- 3

# create a vector of years since UG:
yearsUG <- c(tehran, nabiha,james,chris,margaret,emily,sarah,nate,tyra,julia,angela,allie,daniel,sierra)
```

Lets combine `ages` to `yearsUG` into a single `data_frame` and create a scatterplot:

```{r}
model_df <- data_frame(ages, yearsUG)

# plot age as a function of yearsUG 
model_plot <- ggplot2::ggplot(model_df, aes(x=yearsUG,y=ages)) + geom_point()

model_plot
```

Keeping in mind that our focus this semester will be in linear models. As a simplification, this means that we are looking for an equation of the line that best fits our data (minimizes the resulting SS). The equation for the line that represents the *mean* would be:

$$Y = mean\ age + 0 * yearsUG$$

This means years since undergrad does not figure into the mean model (it gets multiplied by zero. This results in a horizontal line with mean age as the y-intercept.
```{r}
model_plot <- model_plot + geom_hline(yintercept = mean(model_df$ages), color="blue")
model_plot
```

Imagine the difference scores as a straight vertical lines connecting each data point to the means model line. Now compare the means model (blue line) to a model including `yearsUG` as a predictor (red):

```{r}
model_plot <- model_plot + stat_smooth(color="red",method = "lm", se=F)
model_plot
```

As you may have intuited the `lm` in the above call stands for "linear model". Congrats, you just plotted a linear regression!!!! The `se` adds an error ribbon around the regression line. In this case I used `F` or FALSE to turn it off.

What's important here is that from a visual inspection the `yearsUG` model provides a better fit. Why, because it's reduced the resulting difference scores (of SS). The sum of the distances from each point to this line with `yearsUG` as a predictor (red line) is less than the means model (blue line). This is what we mean by "accounting for varience". The `yearsUG` model accounts for more varience than the means model—why because it reduces the SS. A critical question that we strive to answer with our statistical tests (like the *F-test*) is whether the model using additional predictors accounts for more variance above a critical threshold that it may be deemed *significant*, or unlikely due to chance.

As I mentioned, this is something that will become critically important in a few weeks. For now I just wanted to make this connection to your understanding of the role of the mean while it was still fresh.

<!--chapter:end:02week.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Sampling distributions and building the logic of NHST

So this week is typically one of the more confusing weeks for those that do not have much background in stats and programming because I'm asking you to do two conceptually demanding things. 
From the stats side: I'm asking you to imagine that the sample data that you collect is not _sui generis_ (I always wonder if I'm using that term correctly), but part of a larger theoretical distribution of possible (but materially non-existent) sample data sets. The number of these sets is indefinitely many... which is a way of saying really large without invoking notions of infinity. 

On the programming side: I'm asking you to build simulations of these theoretical sampling distributions by using a `for()` loop; programming `R` to proceed in a recursive, iterative loop for a very larger number of instances.

In the class lecture we laid out the logic of the sampling distibution (of means), the difference distribution (of means) and the null distribution (of means). In the workshop we made it as far as simulating the sampling distribution and using it to obtain important measures (e.g., standard error) and walk through and example `for()` loop that can be used to generate the distribution of means. Here I'm going to extend that work to show how to get both the difference distribution and the null distribution. This inolve minor tweaks of what was done in class.

Please follow along by entering the commands into your own console. First let's load in the appropriate packages for this week:

```{r}
pacman::p_load(psych, tidyverse, multicon)
```

## The sampling distribution

To begin let's start off with a control population. For the sake of example, let's imagine that this population represents the obtained IQ scores of  students in Tehran J. Davis High School (apparently I'm feeling myself) that were taught using a standard curriculum. The 5000 students (it's a really big school) in this population have a mean IQ of 103 with a standard deviation of 14. Note that this mean and sd are FACTS... they are true values that exist independent of my measure. Also note that in real work, these truths about the population are rarely known.

We can create a simulated population using the follow chunk of code; note that anytime you are generating "random" distibutions or "random" sampling in `R` you'll need to input the `set.seed(1)` command where I do to ensure that you get the same numbers I do:

```{r}
# population of 5000 with an approximate mean = 103 and sd = 14
set.seed(1)
pop_basic <- rnorm(n = 5000,mean = 103,sd = 14)
```

When we take a look at what the above simulation generates, we see that the obtained values are sightly different than requested (it is random after all). BUT AGAIN, for the puposes of example these are still the facts of the population.

```{r}
mean(pop_basic)
multicon::popsd(pop_basic)
```

Wait... what's that `multicon::popsd()` mess?!?!? 

*Remember* that calculating the standard devation of a population is different than calculating for a sample, where the sample SD requires an adjustment for the degrees of freedom. Simply, the population SD uses $N$ in the denominator, where the sample SD uses $N-1$. The `sd()` fuction calculates sample SD. To get population SD, we can use the function `popsd()` from the `multicon` package.

From this population, the Princpal of TJD-HS tells us that we can only administer our IQ test to 250 students. This represents our sample:

```{r}
mySample <- sample(x = pop_basic,size = 250)
psych::describe(mySample)
hist(mySample)
```

If we run the above code again, we see that we get different measures for mySample:

```{r}
mySample <- sample(x = pop_basic,size = 250)
psych::describe(mySample)
hist(mySample)
```

and in neither case is the observed sample mean identical to the true population mean.

These differences highlight the notion of **Variability due to chance**-the fact that statistics (e.g., means, SDs) obtained form samples naturally vary from one sample to the next due to the particular observations that are randomly included in the sample.

The term **sampling error** is used to refer to variability due to chance, in that, the numerical value of a sample statistic will probably deviate (be in error) from the parameter it is estimating.

In class we mentioned that we can build a theoretical distribution of **sampling error**, a sampling distribution of means. Similar to what you accomplished in the workshop, this can be done with a `for()` loop that does the following:

1. pull a sample of a given size from the population
2. get the mean of that sample
3. save that sample mean to a vector
4. repeat 1-3

How many times do we repeat? Theoretically an infinite number of times. Because we can't wait for infinity, lets just use a really big number like 100,000 (or 1e+05)

Buiding the sampling distribution (using `pop_basic` from above):
```{r}
# create an empty vector to store the sample means
sampling_distribution <- vector()

# use set.seed to ensure our numbers match:
set.seed(1)

# start our loop at 1 and stop it at 100,000
for (i in 1:100000){ 
  # 1. get a sample of 250 students from pop_basic:
  mySample <- sample(x = pop_basic,size = 250)
  
  # 2. get the mean of the obtained sample:
  mySample_mean <- mean(mySample)
  
  # 3. save that mean to our vector, sampling_distribution in the i-th     
  #     location. (Think of this like telling R what line of the paper to 
  #     write on so that you don't overwrite your data)
  sampling_distribution[i] <- mySample_mean
  
  # 4. everything is repeated in the brackets until i=100000
  }
```

Lets take a look at our sampling distribution:
```{r}
hist(sampling_distribution,main = "Theoretical sampling distribution")
```

and the grand mean of the sampling distribution (the mean of means if you will):

```{r}
mean(sampling_distribution)
```

Again not quite the TRUE mean of the population, but pretty darn close.


## Standard error of the mean

We also noted that the standard error (SE) of the mean is simply the standard deviation of the sample distribution. That is, how much does the mean vary due to chance provided repeated sampling. In this case it can be found by:
```{r}
sd(sampling_distribution)
```

However, deriving the SE in this way assumes that we are free to sample and re-sample the population a large number of time (in this case 100 thousand) when in reality we may have neither the time, resources or inclination to do so.  Instead, we use the following equation to provide an estimate of SE provided our sample:

$$SE=\frac{SD_{sample}}{\sqrt{N_{sample}}}$$
So in this case, for a given `mySample` from the population `pop_basic`:
```{r}
set.seed(1)
mySample <- sample(x = pop_basic,size = 250)
mySample_se <- sd(mySample)/sqrt(250)
show(mySample_se)
```

Compared to the theoretically derived SE above, the estimate is not too far off (0.895 v. 0.884). In class, we went a step further and estimated SE for each individual sample in our distribution and compared the mean estimated-SE to the stardard deviation of the entire distribution. Note how this changes our steps slightly:

```{r}
# create an empty vector to store the sample means
# create another to store sample SEs
sampling_distribution <- vector()
sample_ses <- vector()

# use set.seed to ensure our numbers match:
set.seed(1)

# start our loop at 1 and stop it at 100,000
for (i in 1:100000){ 
  # 1. get a sample of 250 students from pop_basic:
  mySample <- sample(x = pop_basic,size = 250)
  
  # 2. get the mean of the obtained sample;
  #     also get the estimated SE:
  mySample_mean <- mean(mySample)
  mySample_se <- sd(mySample)/sqrt(250)
  
  # 3. save that mean and se the respective vectors:
  sampling_distribution[i] <- mySample_mean
  sample_ses[i] <- mySample_se
  
  # 4. everything is repeated in the brackets until i=100000
  }
```

and now the SE calculated from the distibution compared to the mean estimated SE:
```{r}
sd(sampling_distribution)
mean(sample_ses)
```

Again, not identical, but pretty close.

## The difference distribution

Imagine for the sake of example, we are interested in IQ. A central debate (as I understand the literature) at present is whether IQ is inherently static or can be improved with education. Indeed the implication of this can be quite controversial (IQ testing is like the third rail of understanding intelligence... making statements about intelligence is like the third rail of psychology... but I digress).

In addition to our population of students taking the basic cirriculum, we also have an equal number of students taking an advanced cirriculum. Assuming that we have already controlled for other factors (e.g., baseline IQs from both groups the same before treatment) we would like to address the following question using data obtained from each group: "Does the enhanced cirriculum result in higher IQ?"

First let's create our populations:

```{r}
set.seed(1)
pop_basic <- rnorm(n = 5000,mean = 103,sd = 14)
pop_advanced <- rnorm(n = 5000,mean = 108,sd = 13)
```

Again, you'll note that in this example we're omnipitent and know the truth of the matter (the true measures)... in the real world no one is all-seeing and knowing. You'll also notice that I kept the SD for each population roughly equivalent. This *homogeniety of variences* (i.e., SDs should be approximately equal) is another assumption of our non-parametric tests (like ANOVA). Here I've made it so, but if in true the variences are not equal then we may have to adjust how we proceed with our analyses. More on that in a few weeks!

And now to create our difference distribution using the same programming logic as above:

```{r}
# create an empty vector to store the difference in means
difference_distribution <- vector()

# use set.seed to ensure our numbers match:
set.seed(1)

# start our loop at 1 and stop it at 100,000
for (i in 1:100000){ 
  # 1. get a sample of 250 students from pop_basic,
  #     then get a sample from pop_advanced:
  mySample_basic <- sample(x = pop_basic,size = 250)
  mySample_advanced <- sample(x = pop_advanced,size = 250)
  
  # 2. get the means of the obtained samplse;
  #     
  mySample_basic_mean <- mean(mySample_basic)
  mySample_advanced_mean <- mean(mySample_advanced)
  
  # 3. save differences between means to our vector, difference_distribution
  #     in the i-th location. 
  difference_distribution[i] <- mySample_advanced_mean-mySample_basic_mean
  
  # 4. everything is repeated in the brackets until i=100000
  }
```

And now to take a look at the resulting difference distribution:
```{r}
mean(difference_distribution)
hist(difference_distribution, main = "Theoretical difference distribution")
```

As anticipated (because we know all) we end up with a difference distribution with a mean difference of about 5. What we are asked to consider is whether this difference (or any observed difference, really) is enough to say that the likelihood of it occuring by chance is sufficiently small (i.e. is $p < .05$). For this we need to create a null distribution.

## The Null distribution

The logic of constructing a null distribution rests on the claims made by the null hypothesis, that the means of the two (or more) populations in question are identical:

$$\mu_1=\mu_2$$

In a larger sense, if the null hypothesis is indeed true it suggests that the two populations may indeed be one single population. Here, we know that the null hypothesis is NOT true (again, all seeing and knowing).

```{r}
mean(pop_advanced)
mean(pop_basic)

# are the 2 means exactly equal?
mean(pop_advanced)==mean(pop_basic)
```

In the context of our everday experiments, we do not know whether the null is indeed true or not. Based upon the samples that we take we make an inference as to whether we have sufficient evidence to reject the null. We do so if  the  probability of our observed data, given the null is true, is sufficiently low. In other words, here we are asking: "IF the null is true, THEN what is the likelihood that we would get our observed differences in means. If the likelihood is low, then we may have reason to believe the null hypothesis to be suspect.

Returning to the logic of the null distribution, it assumes that our experimental group, `pop_advanced`, and our control group, `pop_basic` are from the same population. Thus, any samples obtained betwixt should be equal, and the differences in sample means should approximate 0 (never exactly 0 due to sampling error). However, by virtue of our experimental design we are _implicitly assuming that, in fact the two are different from one another_. This implicit assumption means that the **null distribution** cannot be created by creating a **difference distribution** between the two groups. 

So how to go about the business of creating a null distribution, then? We create a difference distribution that guarentees that all data is from the same population. In this case we are creating a difference distribution where we take repeated samples from the **same population** and compare their differences. Based on this distribution, we can make claims regarding the probability of observed differences between scores assuming they are from the same population. Again sampling error guarentees that these differences are not exactly zero. 

Typically the population we choose is our control population (group). In this case the logical control is `pop_basic`. Modifying the `for()` loop that we used for the difference distribution, we create a null distribution like so:

```{r}
# create an empty vector to store the difference in means
null_distribution <- vector()

# use set.seed to ensure our numbers match:
set.seed(1)

# start our loop at 1 and stop it at 100,000
for (i in 1:100000){ 
  # 1. get a sample of 250 students from pop_basic,
  #     then get another 250 students from pop_basic:
  mySample_basic_1 <- sample(x = pop_basic,size = 250)
  mySample_basic_2 <- sample(x = pop_basic,size = 250)
  
  # 2. get the means of the obtained samples;
  #     
  mySample_basic_1_mean <- mean(mySample_basic_1)
  mySample_basic_2_mean <- mean(mySample_basic_2)
  
  # 3. save differences betwwen means to our vector, difference_distribution
  #     in the i-th location. 
  null_distribution[i] <- mySample_basic_1_mean-mySample_basic_2_mean
  
  # 4. everything is repeated in the brackets until i=100000
  }
```


And now to take a look at the resulting null distribution:
```{r}
mean(null_distribution)
hist(null_distribution, main = "Theoretical null distribution")
```

In class we made mention that due to the assuption of equal means and equal variences that the null distribution should approximate the form of the standard distribution were $\mu=0$ and $\sigma = 1$. This fact allows us to say something about the probability of an observed difference as a probability of obtaining a Z-score that far removed from 0.

## Probabilty of observed differences

Continuting on from the previous section, we have our null distribuition of IQ scores, appropriately named `null_distribution`. This is the theoretical distribution of differences assuming the two populations are the same. Simply put, this is the distribution of differences we might expect if our advanced cirriculum did not have any impact. You'll note that even assuming that the two groups are identical still yeilds some pretty large differences by chance:

```{r}
max(null_distribution)
min(null_distribution)
```

but that the probability of those extremes is very, very low (but still exists which is why we never _prove_ the alternavtive hypothesis). Let's say I take a sample from `pop_basic` and `pop_advanced` and find that my means differ by 3. Given the null distribution, what is the probability that I will observe a difference this large? To derive this probability, we take advantage of what we know about the standard distribution, where we have a known probability of obtaining any given Z-score.

The process of answering the above is as follows: 
1. transform the observed difference to a Z-score using the mean and SD of the null distribution; 
2. calculate the probability of that extreme of a score. 

Here is step 1:

```{r}
# 1. convert the observed difference to a z-score:
Zscore <- (3-mean(null_distribution))/sd(null_distribution) 
show(Zscore)
```

I want to pause here to note that the obtained Z-score is positive. As a result the score is on the right side of the distribution. Our probability function, `pnorm()` returns the probability of an obtained score or lower (the cummulative probability). However, we want the probability of the observed score or *more extreme*. In this case *more extreme* means _greater than_. Conversely if the observed Z score is less than 0, then *more extreme* means _less than_. I bring this up as this determines what value we use to calculate the desired probability. In the latter case, _less than_ we can simple take the output of `pnorm()` and call it a day. However, in cases like ours, when $Z>0$ we need to use `1-pnorm()`. 

OK, on to the calculation:

```{r}
Zscore <- (3-mean(null_distribution))/sd(null_distribution) 
1-pnorm(Zscore)
```

Based on a criteria of rejecting the null if $p<.05$ we would reject here. BUT NOTE: not necessarily because it's less than .05. Remember that if I have a two-tailed test, my criteria requires that I split my $\alpha$. So in truth I need to obtain a value less than .025 in either direction.

## A note about sample size

So I got a pretty low _p-value_, but you might say it was expected given that my sample size was 250 for each group. Let's see what happens if I drop the sample size down to 10 students per group, creating a new null distribution, `null_distribution_10`:

```{r}
# create an empty vector to store the difference in means
null_distribution_10 <- vector()

# use set.seed to ensure our numbers match:
set.seed(1)

# start our loop at 1 and stop it at 100,000
for (i in 1:100000){ 
  # 1. get a sample of 250 students from pop_basic,
  #     then get another 250:
  mySample_basic_1 <- sample(x = pop_basic,size = 10)
  mySample_basic_2 <- sample(x = pop_basic,size = 10)
  
  # 2. get the means of the obtained samplse;
  #     
  mySample_basic_1_mean <- mean(mySample_basic_1)
  mySample_basic_2_mean <- mean(mySample_basic_2)
  
  # 3. save differences betwwen means to our vector, difference_distribution
  #     in the i-th location. 
  null_distribution_10[i] <- mySample_basic_1_mean-mySample_basic_2_mean
  
  # 4. everything is repeated in the brackets until i=100000
  }
```

What does this distribution look like?
```{r}
hist(null_distribution_10, main = "Theoretical null distribution")
mean(null_distribution_10)
```

And let's take a look at the probability of an observed difference of 3 in this case. Not that I'm using the `pnorm()` shortcut so I don't need to calculate my Z by hand:

```{r}
1-pnorm(q = 3,mean = mean(null_distribution_10),sd = sd(null_distribution_10))
```

As we can see, *size matters*. Given that we are omnipotent in this case, we know the truth of the matter is that the two populations are in fact different, and that the mean of `pop_basic` is not equal to the mean of `pop_advanced`. Thusm what we have here is a failure to reject the null hypothesis when it is indeed false. Hmm, what kind of error is that again? This last example is going to figure prominently in our discussions of power and what power is, properly defined. For now, I think this is a good place to stop this week.

For additional practice, walk through and attempt to *recreate* (in your own notebook) the `week3_inclass.Rmd` located on BOX in this weeks workshop folder. Be sure that you understand the logic of your steps, and how I've answered the questions presented in the file. This should be good prep for your homework.

<!--chapter:end:03week.Rmd-->

# Probability and the Binomial Distribution

## A few words on where we are at so far...

Last week we introduced sampling distributions and importantly the Null sampling distribution. Most importantly we talked about how the null sampling distribution allows us to make assessments on our observed data relative to the Null hypothesis. To remind ourselves here, the null hypothesis assumes that data collected from two or more samples comes from the same population. As such, the descriptive statistics of each of those samples should be nearly identical within a certain amount of tolerance.

A key point in our understanding, is that the null distribution (of mean differences) should approximate the standard distribution, with a mean $\approx$ 0, and a SD $\approx$ 1. As we pointed out a few lectures back, we can do wonderful things with this distribution, including assessing the probability of an obtained difference value assuming the null hypothesis is true (see last week's write-up). This week, we unpack further notions of probability and how they may be used to allow us to make assessments on important aspects of our data... and further how that may allow us to make decisions about our unterlying statistical (and further, theoretical) assumptions.

For the next few weeks, these two critical assumptions about the nature of our observed data will be paramount:

1. that the data (its distribution) is randomly obtained from a normal population.
2. that our measures of the data (including both the tools we use and the resulting desciptions of the data) are independent of one another. 

We've spent some time at length with Assumption #1. What do I mean in Assumption #2? Here's an example that I use in my Perception course:

>Say I step on a scale and it says that I'm 180 lbs. I then walk over to a stadiometer and get my height measured and it tells me I'm 71 inches tall. Right after this I return to the original scale. If the measurement of my weight and height were independent of one another, then the scale should again read 180 lbs. However, if in some stange reality the measurement of my height changes the measurement of my weight, then all bets are off—the second time on the scale it could say I'm 250lbs, 85lbs, who knows. 

One of the lessons of quantum mechanics (see Stern-Gerlach experiments) is that we do live in that strange reality—at least at the quantum level. Most of you have probably encountered the uncertainty principle which is the physical manifestation of this issue. More pertainent for us consider the example in class:

>We have a theory that adults with children are more likely to wear seatbelts than adults without children. Our null hypotheis is that adults with children are no more likely to wear seatbelts than adults without children.

Here, everyone is being measured in two ways, 1. the seatbelt measurement (do you wear a seatbelt, yes or no?) and 2. the parenthood measurement (do you have a child, yes or no?). If the two measures are independent of one another, then the seatbelt measurement should not have an effect on the parenthood measurement and vice versa. If they do, then the measures are not independent of one another. Of course, in this case, it doesn't mean that the being measured for seatbelt-wearing may instantaneously turn you into a parent (Yikes), but it does mean that being identified as a parent may increase the probability that you are a seatbelt wearer and vice versa.

Hopefully from this example, you now see the relationship between the null and research (alternative) hypotheses—the research hypothesis is structured in such a way that it assumes a violation of the null, in this case a violation of the independence assuption. In many cases we will be testing whether the evidence supports such a violation.

So returning to why probability is so important, the probabilities related to the null distribution have been worked out by women and men far smarter than I. THESE PROBABILITIES ARE KNOWN and tell us the likelihood that our null assumptions have not been violated given our present set of data.

## Okay on to this week's example analyses:

For the following examples, we will be using functions that come pre-installed in `R` (`base` library) as well as the `tidyverse`. Let's go ahead and install the `tidyverse`.
```{r}
pacman::p_load(tidyverse)
```

## Classic example: The coin flip

Let's make a wager. I'll let you flip a single coin 10 times, and if you get 6 or more heads I'll give you \$250. If not, you have to give me \$250. Should you take this bet? To answer this question you'll want to assess the likelihood of getting an observed outcome, in this case $Heads \geq 6$, provided a sample size (10).

We *assume* that the likelhood of getting Heads on any single flip is .5, (assuming that the coin is far and not a cheat). We also *assume* that the coin flips are independent of one another, that is any single flip does not influence or change the likehood of an outcome on subsequent flips. *In light of our discussion in the opening section, I hope you see that depending on the outcome, it may cause us to re-evaluation these assumptions*.

For now, let's assume that both are indeed true and that I'm not a cheat.

### How many heads would we get if we flipped a fair coin 10 times?

In class we noted that with a known sample space and a known sample size we can calculate the *expected outcome*. In this case our sample space is {Heads, Tails} and our sample size is 10 (flips). The probability of observing a Heads in our sample space is .5. Therefore:

$$expected\ Heads = p(Heads) \times N(flips)\\
expected\ Heads = .5 \times 10 = 5$$

Of course, reality does not often abide our expectations, and given what we know about sampling error you're more likely to not get exactly 5 flips than you are. So, what we're really interested in is what is the likelihood of our expectation, and fortunately for us, over the long-run independent/random outcomes can be predicted probabilistically. There are several ways to address this in `R`.

#### Simulation method
The first method involves running a large number of simulations and assessing the probability based upon the outcomes. To run a single simulation, you can use the `rbinom()` function. Below I am running a simulation of 1 coin (`n`), with a sample size of ten flips (`size`), and a known (or really assumed) probability of .5 (`prob`).

```{r}
# set our seeds to get the same numbers:
set.seed(1)

# n = number of coins
# size = number of flips
# prob(ability)
rbinom(n = 1,size = 10,prob = .5) 
```

In this simulation you got 4 flips and I'm off to cop a new pair of retro Jordans with your money... or I suppose if I'm being sensible, you've paid for this week's daycare. In any event, you lost. But we know that not much can be learned about the likelihood of an outcome with a sample size of 1. Let's try running this scenerio using a simulation of 100K coins.

To run this simulation, you could:

1. run the above 100K times, write down the number (yea, sure bud)
2. build a `for()` loop (like you did last week)
3. run it in a single line by modifying the `rbinom()` call:

```{r}
set.seed(1)
numberHeads <- rbinom(n = 100000,size = 10,prob = .5)
```

Let's plot this using `ggplot()`:

```{r}
# must convert vector to a data frame
numberHeads_df <- data_frame(numberHeads)

p <- ggplot2::ggplot(data = numberHeads_df, aes(x=numberHeads)) + 
        geom_histogram(binwidth=1,boundary=-0.5,fill="red", col="grey") + 
        scale_x_continuous(limits=c(0,10), breaks = 1:10)
show(p)
```

As you can see in the plot above, we get exaclty 5 heads a little less than 25K out of 100K, or about 25% of the time. That's the eyeball test, How might we go about obtaining the exact probabilities from this simulation? We can take advantage of logical statements. Logicals produce outputs that are either `TRUE` or `FALSE`. More, and this is terribly useful for us, `R` also reads those `TRUE` or `FALSE` outcomes as `1` or `0` where `TRUE=1` and `FALSE=0`. Take the following vector for example:

```{r echo=FALSE}
c(TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE)
```

`R` reads this as a numeric:

```{r echo=FALSE}
as.numeric(c(TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE))
```

Hopefully, its apparent now that you can get info about the number of `TRUE`s by performing mathematical operations on this vector. For example, above you see that there are 4 `TRUE`s out of 10 possible. That means that we get `TRUE` 40% of the time. We can obtain this probability in `R` using the `mean()` function. How? Remember that the mean is the sum of scores divided by the number of scores. In this case the sum is 4; the number of scores is 10; and 4/10 is .40.

Returning to our simulation of 100K samples, `numberHeads`, we can do the following to get the probability of 5 (see Demos, Ch. 3 on logicals for explanation of operators like `==`):
```{r}
mean(numberHeads==5)
```

So the probability of getting exactly 5 heads is 0.245.

Returning to our wager, based on our simulation what's the probability of getting 6 or more heads?

```{r}
mean(numberHeads>=6)
```
About 38%. You probably shouldn't take my bet.

#### Non-simulation method

Simulations are useful for helping us visualize our scenerios and use pseudo real data to test our underlying assumptions. But most times you just want the answer... in fact often  that will suffice because as I mentioned, in most normal circumstances the probabilities have already been worked out. How can we get the results above without simulating 100K coins? By using two functions in `R` that belong to the same family as `rbinom()`: `dbinom()` and `pbinom()`.

`dbinom()` returns the exact probabilty of an outcome, provided a generated probility density function. It takes in arguments related to the numnber of successful outcomes (x), the sample size (size) and the independent probability of a sucessfull outcome (prob).

So the probability of 5 Heads (successes) on 10 flips with a fair coin would be entered:
```{r}
dbinom(x = 5,size = 10,prob = .5)
```

We see that this value is pretty close to our simulated outcome, confirming that the simulation was indeed correct, if not entirely necessary.

We can also use this function to build a table for the individual probabilities of all possible outcomes. To see how, first lets consider the space of possible outcomes in this scenerio. At one extreme, I could flip a coin 10 times and get no Heads. At the other I could get all Heads. So the set of possible outomes can be express as a sequence from 0 to 10. Recall that you can create such a sequence using the `:` operator:

```{r}
0:10
```

With this in mind you can modify the previous code like so:

```{r}
dbinom(x = 0:10,size = 10,prob = .5)
```

The output gives be the resulting probabilities in order from 0 Heads to 10 Heads.

Let's create a table to make this easier to digest. First I'm going to create a vector of `numberHeads` to show all possbilites from 0 to 10. Second, I will run `dbinom()` as above to test each possibility, saving that to a vector `probHead`. Finally I will combine the two into a data frame called `probTable`:

```{r}
# 1. range of possibilites
numberHeads <- 0:10
# 2. prob of outcome
probHead <- dbinom(x = numberHeads,size = 10,prob = .5)
# 3. combine to data frame
probTable <- data_frame(numberHeads,probHead)
# 4. Show the data frame (table)
show(probTable)
```

Contrats you've just created a Binomial Distribution Probability Table for this scenerio. Note that you're asked to do the same in this week's homework! (Disregard the single line `for()` method mentioned in the homework. It dawned me that this would be a way to do it without for loops!)

*Returning to our wager of 6 or more heads*, we would use `pbinom()`. `pbinom()` returns probabilities according to the cumulative density function, where the output is the likelihood of obtaining that score or less. Note that `pbinom()` takes similar arguments to `dbinom()` but asks for `q` instead of `x`. For our purposes `q` and `x` are the same thing... number of outcomes.

So for example the probability of obtaining *5 or less* Heads provided our scenerio may be calculated:

```{r}
pbinom(q = 5,size = 10,prob = .5)
```

Given what we know about the Compliment Law, we can compute the probability of 6 or more Heads as:
```{r}
1 - pbinom(q = 5,size = 10,prob = .5)
```
which again matches with our simulation.

## Changing the parameters

So obviously if the coin is a fair coin, your shouldn't take the bet. Let's imagine that we kept the same wager, but to entice you to bet, I tell you that this coin lands on Heads 65% of the time? Should you take the bet?

To test this new sceneria all you need to do is change the probability parameter. Let's just skip the simulations and just assess using `dbinom()` and `pbinom()`.

### Constructing a table of outcome probabilities:

As before we'll use `dbinom()` to create a table, simply modifying the `prob` argument to `.65`:
```{r}
# 1. range of possibilites
numberHeads <- 0:10
# 2. prob of outcome
probHead <- dbinom(x = numberHeads,size = 10,prob = .65)
# 3. combine to data frame
probTable <- data_frame(numberHeads,probHead)
# 4. Show the data frame (table)
show(probTable)
```
How does this table compare to the one above?

### Assessing cummulative probability

And now, to get the probability of 6 or greater:
```{r}
1 - pbinom(q = 6,size = 10,prob = .65)
```

Ahhh... the odds are ever so slightly in your favor.

What if we changed the bet: you get 60 Heads out of 100 flips?
```{r}
1 - pbinom(q = 60,size = 100,prob = .65)
```

Yea, you should really take that bet! Fortunately for me, I wasn't born yesterday.

How about 12 out of 20?

```{r}
1 - pbinom(q = 12,size = 20,prob = .65)
```

Nope, I'm not taking that bet either. Does 3 out of 5 interest you?
```{r}
1 - pbinom(q = 3,size = 5,prob = .65)
```

## Catching a cheat

OK. Last scenerio. Let's imagine that I am not a total sucker, and we reach a compromise on "30 or more Heads out of 50 flips". You run to your computer and calculate your odds and like your chances (maybe I am a sucker)!
```{r}
1-pbinom(q = 30,size = 50,prob = .65)
```

You flip 50 times but only get 27 heads. Astounded, because those odds really were in your favor, you label me a liar and a theif. Are you justified in doing so? This scenerio essentially captures our discussion at the outset, how far of a deviation warrants us being skeptical that our original assumptions were true. In this case the original assumptions were that *1. each coin flip is independent* and *2. the independent probability of getting a Heads is 0.65*. We typically set our threshold at $p<.05$, which remember for a two tailed test means that we are on the lookout of extreme values with a $p<.025$.
```{r}

```

So essentially we are asking if the probability of obtaining exeacty 27 Heads given this scenerio is less than 2.5%:

```{r}
dbinom(x = 27,size = 50,prob = .65)
```

You Madam/Sir have besmirched my honor!

OK, well is 27 isn't enough, then how low (or high) do you need to go to pass the critical threshold? To answer this we need to construct a table of probabilities:

```{r}
# 1. range of possibilites
numberHeads <- 0:50
# 2. prob of outcome
probHead <- dbinom(x = numberHeads,size = 50,prob = .65)
# 3. combine to data frame
probTable <- data_frame(numberHeads,probHead)
# 4. Show the data frame (table)
show(probTable)
```

One could visually inspect the entire table noting which outcomes have a corresponding probability less than .025. But the whole point of learning `R` is to let it do the heavy lift for you. In this case you can ask `R`:

>"Hey, R. Which outcomes in have a probability less than .025"

or more exactly:

>"Which rows in my data frame have a `probHead` less than .025"

This is accomplished using the `which()` function:

```{r}
which(probTable$probHead<0.025)
```

This gives us indices of all of the rows that meet this criteria. BUT KEEP IN MIND, our list of possible outcomes started at `0` not `1`. But there's no such thing in `R` as row 0 (now Python on the other hand...). That means in the output above that `1` actually refers to 0 Heads, `2` to `1 Head` and so on. You could stop here and just know that you need to subtract the above output by 1 to get the correct result. Or you could address this in `R` in the following ways:

First, the quick/dirty/limited/bad way. Keep in mind this only works because we know that we can apply a subtraction rule:
```{r}
which(probTable$probHead<0.025)-1
```

*The correct below way will work for cases in which adjusting the index is unknown*. Recall from [Demos Chapter 2](https://ademos.people.uic.edu/Chapter2.html) that you can get a specific value from a vector or data frame by using its index using the `[]` operator such that `[row,column]`. Think of this like how you would get a certain cell in Excel.

For example the get the 17th row of our data frame `probTable`:
```{r}
probTable[17,]
```
Note that if you want all rows or columns, you leave that index blank (as I did above where I wanted both columns). Keeping in mind that `which(probTable$probHead<.025)` gave us a vector of indecies, we can rewrite the previous chunk like so to get the important rowns in a data frame structure:

```{r}
crticalValues <- which(probTable$probHead<.025)
probTable[crticalValues,]
```

Again this would ask us to look through the whole table, but we can apply the same logic to just show us the corresponding `numberHeads` as a vector (Remember that you'll need to specify the data frame when refering to the columns):

```{r}
crticalValues <- which(probTable$probHead<.025)
probTable$numberHeads[crticalValues]
```
Note that since vectors don't properly have rows or columns, you just use a single value in the `[ ]`.

So, if you got 1 less head, *then* you can call me a cheat. Conversly if you managed to get 39 heads or above, I might have reason to believe that you are somehow gaming me.

## One last thing... Permutations, combinations, and building functions

In class we noted the equations for calculating permutations and combinations. For example, permutations:

$$P^N_r=\frac{N!}{(N-r)!}$$

So assuming that I have for colored balls (green, white, red, blue) how many possible permutations assuming I only draw two?

Here we can use the `factorial()` function:
```{r}
N <- 4 # number total
r <- 2 # number draw

factorial(N)/factorial(N-r)
```

Before leaving you, I want to note one more thing that you is possible in `R`... constructing your own functions. This might be especially useful when you have a operation that you are performing over and over again that has to built in function. Functions take the general format:
```{r eval=FALSE}
myfunction <- function(input parameters seperated by commas){
  
  do something to the input parameters
  
  return(output)
  }
```

For example, with permutations, I can create a function, let's call it `pFunc` that takes `N` and `r` as input parameters and performs the operation for me:

```{r}
pFunc <- function(N,r){
  factorial(N)/factorial(N-r)
}
```

After you run these lines of code, `pFunc` is saved to your environment. From here on out we can simply type:

```{r}
# four balls, draw 2
pFunc(N = 4,r = 2)

# four balls draw 4
pFunc(N = 4,r = 4)

# 100 balls, draw 8
pFunc(N = 25,r = 8)
```

Given what we've just done, try to construct a function for Combinations, given that:

$$C^N_r=\frac{N!}{r!(N-r)!}$$

OK... signing out for today.

<!--chapter:end:04week.Rmd-->

# Chi-square and associated methods

This week we will be working through how to conduct and report Chi-square, $\chi^2$, analyses in `R`. As mentioned in class we can use $\chi^2$ in several ways: to assess goodness of fit (assuming all is fair), to test for independence, to compare models (see you next semester), and a derivative of the Chi-square logic to test for inter-rater reliability.

In what follows we will walk through each of these scenarios, focusing on how to conduct the analyses in `R`. For each analysis we will also provide examples of how to construct frequency tables depending on what information is available to you. In some cases you are already provided with pre-tallied frequencies, in other cases you may be given raw data and need to create the tallied frequencies yourself. I realize in class I walked you through several methods of how get the appropriate results. Here I'm just going to focus on the most simple and efficient way to do this.

Here are the packages we'll be using:
```{r}
pacman::p_load(tidyverse,  
               kableExtra, # creating html tables for this page 
               epitools, # calculating odds ratios
               lme4, # for the comparing models example
               vcd, # Cohen's kappa
               gmodels # makes obsolete a lot of our workshop
               )
```

Also, if you are using a newer version of `R`, you'll need to install `zoo`. `vcd` depends on `zoo` to work. If prompted in your Console with the question: "Do you want to install from sources the package which needs compilation?", type in "no":

```{r}
pacman::p_load(zoo) # this is a package that needs to be installed, but needs a response from you to do so.
```

## Goodness of fit test
The Goodness of fit test examines how "close“ observed values are to those which would be expected if things were ‘fair’ or equal. Measures of **goodness of fit** test to see if the discrepancy between observed and expected values is a significant discrepancy using the following formula:

$$\chi^2 = \sum{\frac{(O-E)^2}{E}}$$
Where $O$ and $E$ refer to the observed frequency and expected frequency respectively. 

### What to do if you are given a table with tallies

In class I asked you to consider the data from your Howell text, Table 6.1, where the data are taken from a study testing therapeutic touch as a "real" phenomena. Therapeutic touch postulates that one may heal a patient by manipulating their "human energy field". If this is true, and if patients are indeed sensitive to therapeutic touch, then we would expect that patients should be able to report the effect (position of the healers hand) greater significantly greater than chance. The resulting frequencies and expected were:


```{r echo=FALSE, results='asis'}
# building a matrix in R, enter the items, specify the number of rows,  are the items filled in across rows or down columns.
table6.1 <- matrix(c(123,157,280,140,140,280),ncol=3,byrow=TRUE)

# give the table row and column names:
rownames(table6.1) <- c("Observed","Expected")
colnames(table6.1) <- c("Correct","Incorrect","Total")
table6.1 <- as.table(table6.1)

# format the table for web presentation:
kable(table6.1, format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

In this case, the tallies are already done for you. You just need to enter them into `R` as a frequency table. 

#### Creating the frequency table.

The first important step is to build a matrix object that contains the table. This can be accomplished using the `matrix()` function. In this case we need to create a 2×2 matrix to place our four values in. The critical thing to remember when entering your values is that you need to go in order either down columns or across rows. By default `R` prefers you to go down columns, so we'll just do that here. For simplicity's sake let's just create an object to store these values. Since we are doing down columns, the entry order should be 123,140,157,140,280,280.

```{r}
table_values <- c(123,140,157,140,280,280)
```

We can then create matrix using the `matrix()` function with the following arguments:

+ `data`: the values you are putting into the matrix, in this case `table_values`
+ `nrow` or `ncol`: the number of rows or columns in the matrix; you only need to enter one or the other
+ `byrow`: `TRUE` or `FALSE`, will the values be entered into the matrix going across by rows (`TRUE`) or down by columns (`FALSE`). The function default is `FALSE`, meaning data is entered by column.

So to get our numbers into a matrix named `table_6.1` we:

```{r}
table_6.1 <- matrix(data = table_values,nrow = 2)
# no need to enter byrow= as I'm just using the default setting
show(table_6.1)
```

Before moving on, we need to convert this matrix to a table object. This can be accomplished in one fell swoop by adding `as.table()` to the previous step in a `pipe`, `%>%`. Piping was mentioned in the Intro to Tidyverse DataCamp assignment. Basically it says take the previous output and now perform this additional function. Steps that would normally take multiple lines can be done in a single line:

```{r}
table_6.1 <- matrix(data = table_values,nrow = 2) %>% as.table()
# the pipe operator %>% is from tidyverse. See Demos
show(table_6.1)
```

Almost there, not we just need to give our table meaningful names, ABC is not going to cut it. This can be accomplished using `rownames()` and `colnames()` and assigning a vector of the appropriate names:

```{r}
rownames(table_6.1) <- c("Observed","Expected")
colnames(table_6.1) <- c("Correct","Incorrect","Total")
show(table_6.1)
```

And that's it. Your table is now in `R` and ready for the next step.

#### Running the Chi-square

For a simple Goodness of fit test I suggest using the `chisq.test()` function. Remember from class that this has several important arguments:

+ `x`: a vector (1 dimensional) or matrix (2-dimensional: rows, columns) of the observed data
+ `p`: a vector (1 dimensional) or matrix (2-dimensional: rows, columns) of the expected data expressed as raw numbers, $E$ or as probabilities $E/N$.
+ `rescale.p`: if you use the raw expected values then this needs to be set to `TRUE`
+ `correct`: perform a Yates correction when calculating the $\chi^2$

So now that we have constructed our `table_6.1`, I would suggest calculating the $\chi^2$ as follows:

1. create a matrix of the observed data
2. create a matrix of the expected data
3. submit these matrices to `chisq.test()`:
  + correction or no correction: I typically prefer to perform the correction, **BUT ultimately is unnecessary here as `correction` only applies to 2 by 2 tables.**
  + rescale or no rescale: I typically prefer to enter my raw expecteds and then set `rescale.p=TRUE`

So If I were running the Goodness of fit test based upon the data above I might:

```{r}
# matrix of observed:
observed <- matrix(data=c(123,157),nrow = 1)

# matrix of expected:
expected <- matrix(data=c(140,140),nrow = 1)

# run the chi-square:
chisq.test(x = observed, p = expected,rescale.p = T)
```

That said, remember you only need could just list the observeds:
```{r}
chisq.test(x = observed)
```

#### Using your built `R` table

You may have noticed that I elected to create my `observed` and `expected` objects by entering the data by hand. "Then why on earth did you have us make that damn table?", you ask? Well I could have also got these values by indexing the table (see Demos, chapter 2). For example:

```{r}
table_6.1[1,1:2]
```
gives me my observeds and

```{r}
table_6.1[2,1:2]
```

gives me my expecteds. Knowing this I could just run the above `chisq.test()` as:

```{r}
chisq.test(x = table_6.1[1,1:2])
```

### Assuming all things are NOT equal

Let's assume that we are dealing with a Goodness of fit that involves three or more categories. For example, taken from Howell Table 6.3 a game of Rock, Paper, Scissors. 

**R-ninjas** Below is the code that I use to create the tables you see on this page. Note that the `kable()` function is what allows pretty web tables:
```{r}
table6.3 <- matrix(c(30,21,24,25,25,25),ncol=3,byrow=TRUE) # I'm a byrow = T person
rownames(table6.3) <- c("Observed","Expected")
colnames(table6.3) <- c("Rock","Paper","Scissors")
table6.3 <- as.table(table6.3); show(table6.3) # the semicolon here seperates the lines

kable(table6.3, format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Assuming equal likelihood of a person selecting Rock, Paper, or Scissors I can just run this test as I did in the previous section. Note that in the code below I'm not even bothering with the expecteds because they are assumed equal:

```{r}
observed <- matrix(data = c(30,21,24), nrow = 1)
chisq.test(x = observed)
```

**BUT WHAT IF MY EXPECTEDS AREN'T EQUAL???** That is, what if I know that people are twice as likely to choose Rock than Paper or Scissors. In this case the respective probabilities would be Rock = .5, Paper = .25, Scissors = .25). I can simply adjust `p=` in my `chisq.test()` function.
```{r}
expected_probabilities <- matrix(data = c(.5, .25, .25), nrow=1)
chisq.test(x = observed, p=expected_probabilities)
```

As you can see, $\chi^2$ has been adjusted to reflect these new probabilities.

### Real data is likely NOT pre-tallied
In the above examples, the observeds and expecteds were already tallied for you. In the real-world however, you'll need to do the tallying yourself (or get your RA to do it). Let's imagine this scenario running a goodness of fit test on a game of Paper, Rock, Scissors, Lizard Spock. This data can be found using this link:

```{r}
prslOutcomes <- prslOutcomes <- read.delim("https://raw.githubusercontent.com/tehrandavis/statsData/master/prslOutcomes.txt")
```

Note that you want to be sure that the data object is stored as a `data.frame` or `data_frame` and if not convert it to one. Importing a data set from a file typically defaults to a `data.frame` but here is how to check and convert if necessary:

```{r}
class(prslOutcomes) # check class of object to ensure data.frame
prslOutcomes_df <- data.frame(prslOutcomes) # convert if necessary, not necessary here but an example
```


And let's take a look at the data:


```{r}
prslOutcomes
```

In the data set above, each line represents a single play out of 160 total.

When dealing with real data you'll need to get the actual frequency counts that go into the data. There are several ways to get these counts. 

### getting counts the in-class way (not recommended)

We mentioned in class that you can use the `plyr::count()` function to get tallied data. One of the frustrating things about `R` is that with so many packages being made, many authors us the same names for their functions. In the case of `count()` note that the following give you different results:


```{r}
count(prslOutcomes)
dplyr::count(prslOutcomes)
plyr::count(prslOutcomes)
```

Only `plyr::count()` gives us the counts per category. Let's save this to an object called `counts`:

```{r}
counts <- plyr::count(prslOutcomes)
show(counts)
```


We can simply grab the frequency data from the `freq` column that `plyr::count()` has created:
```{r}
chisq.test(x=counts$freq)
```

### getting counts the `table()` way (my new recommendation)
An alternative to the above would be to create a table from your raw data. Given something I just discovered after class yesterday, I think this may be the better way, if for nothing other than being consistent. In this case we create a table from the data frame of raw data using the `table()` function:

```{r}
table(prslOutcomes)
```

You can simply plug this table into the `chisq.test()` function:
```{r}
chisq.test(x=table(prslOutcomes))
```

And if we had prior knowledge that people tend to choose Lizard Spock 50% of the time, Rock 30%, and the remaining two 10% each? would you modify the previous call?


## Contingency Table Analysis in R

We use contingency tables to assess the relative independence of 2 categorical variables. Consider the example taken from the Howell text, Table 6.4: 
```{r echo=FALSE}
table6.4 <- matrix(c(33,251,284,33,508,541,66,759,825),ncol = 3,byrow = T)
colnames(table6.4) <- c("Yes","No","Total")
rownames(table6.4) <- c("Nonwhite","White","Total")

#show(table6.4) # or fancy
kable(table6.4, format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

The above table assesses whether the frequency of being handed the death penalty (Yes or No) is contingent on race (White or Non-White). In class I walked you through several ways of running the $\chi^2$ of independence test assuming you are given a table that has both observeds and marginal totals or just observeds. Ultimately this was done to show you how to calculate your expecteds in either scenario and perform the requisite steps flowing from there.

**BUT BUMP ALL DAT... THAT'S RIGHT I SAID BUMP IT!**

One of the cool things about `R` (for me at least) is discovering more efficient ways of doing things, and after class I got the thinking "there's got to be a simpler way of dealing with Contingency Tables than I'm used to". And in fact there is. 

Ladies and Gents, I give you the `gmodels` package:
```{r}
pacman::p_load(gmodels)
```

Assuming we have the above table, we can use the `gmodels:CrossTable()` function. But first we need to recreate the table including **only our observed values** and save it to an object; here I'm saving it to `table6.4_observed`:

```{r}
table6.4_observed <- matrix(c(33,251,33,508),ncol = 2,byrow = T)
colnames(table6.4_observed) <- c("Yes","No")
rownames(table6.4_observed) <- c("Nonwhite","White")
show(table6.4_observed)
```

From here we just throw this table into `gmodels::CrossTable`. Run `?gmodels::CrossTable` to get a feel for what arguments this function takes and what other things it can do. For now, I'm especially concerned with:

> expected: If TRUE, chisq will be set to TRUE and expected cell counts from the Chi-Square will be included

> chisq: If TRUE, the results of a chi-square test will be included

So if we run this with the argument `expected=TRUE` then it calculates and displays our expecteds AND runs the $\chi^2$:
```{r}
gmodels::CrossTable(table6.4_observed,expected = T)
```

Voila!! The key at the top of the output gives you the info that's in each line of each cell, where:

+ line 1 is observed N
+ line 2 is expected N (it calculates them for you!!)
+ line 3 is the $\chi^2$ contribution of that cell (remember the sum of the cells gives you $\chi^2$)
+ line 4 is the proportion of that cell to the row total
+ line 5 is the proportion of that cell to the column total
+ line 6 is the proportion of that cell to the grand total

and then BAM!!! The end of the output gives you your $\chi^2$ test both with and without the Yates correction.

### Okay, but what about with REAL DATA:

OK. Let's take a look at our old professor income data. In this case, instead of actual income values, let's just take a look at Men v. Women faculty and make a judgment on whether or not they make greater than 50K per year. 

A raw data set may be found at the address below:
```{r}
FacultyIncome_chi <- read.delim("https://raw.githubusercontent.com/tehrandavis/statsData/master/FacultyIncome_chi.txt")
```

As in the REAL DATA example above, we can simply use the `table()` function as we did above to get tallies from this data frame:

```{r}
table(FacultyIncome_chi)
```

and throw this table into the `gmodels::CrossTable` function:

```{r}
gmodels::CrossTable(table(FacultyIncome_chi),expected = T)
```

And we're done. No need to get `count()` or use the `xtabs()` function or specify a formula. Life is good!

## Effect Sizes and Odds Ratios

Extending this to Table 6.5 from the Howell text, assessing the relationship between childhood sexual abuse (0 incidents, 1,2,3) and abuse as adult (yes,no):


```{r echo=FALSE}
table6.5 <- matrix(c(512,54,227,37,59,15,18,12),ncol = 2,byrow = T)
rownames(table6.5) = c("0","1","2","3+")
colnames(table6.5) = c("No","Yes")


kable(table6.5, format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

And now a Chi-square:
```{r}
table6.5 <- matrix(c(512,54,227,37,59,15,18,12),ncol = 2,byrow = T)
rownames(table6.5) = c("0","1","2","3+")
colnames(table6.5) = c("No","Yes")

show(table6.5)

gmodels::CrossTable(table6.5,expected = T)
```

As I mentioned in class, we are not only interested in independence in itself, but also the the relationship between the frequency of our observed outcomes. This allow us to further interpret our data.

For example we might be interested in not only whether instances of sexual abuse as a child affect the likelihood of experiencing abuse as an adult, but also how much more likely one is to be abused as an adult with increased childhood incidence.

For this we can use odds ratios, which essentially act as a measure of effect size, or magnitude of the effect, for frequency data. Odd ratios may be calculated as by establishing a ratio of the binomial outcomes (in this case Yes v No) for each group (Incidence: 0,1,2,3+). 

To do this "by hand" you can simply take the appropriate columns and perform division to get the odds for each group. In this case you can just divide the values in column 2 of `table6.5` by column 1 (because I'm interested in the relationship of number of incidents, Yes column). In themselves, the odds tell us the likelihood of occurrence:

```{r}
odds <- table6.5[,2]/table6.5[,1]; show(odds)
```


We then calculate each odds ratio by dividing each of the odds by a "control". In this case the first group, 0 incidents is the logical control.

```{r}
odds_ratios <- odds/odds[1]; show(odds_ratios)
```

One important bit. Since we are dividing all odds by our control group, the odds ratio for the control will always be 1. Another way to think of this is that the odds ratio for our null condition will always be 1. **Deviations away from 1 are deviations away from the null hypothesis.** This makes since... if incidence did not have an effect we would expect the odds of adult abuse to be the same for all groups.

Rather than doing this by hand, however, you can simply use `epitools::oddsratio()`:
```{r}
epitools::oddsratio(table6.5)
```

Note that by default `epitools::oddsratio()` assumes that your table is constructed with your rows as groups and your columns as outcomes (YorN, Success or Fail, etc.). It also assumes that your first row is your control, and the second column is your column of interest. It this is not the case you need to reformat your table to fit.

## Fisher's exact test

Fisher's exact test is a statistical significance test used in the analysis of 2×2 contingency tables. Although in practice it is employed when sample sizes are small (expected < 5), it is valid for all sample sizes.

It involves: 1. determining all possible tables that could be formed using same marginal totals observed and 2. calculating the probability of the observed data from whether it falls in the extreme region of all possible tables (i.e., < .05 probability). 

The `gmodels::CrossTable()` function is nice as it will also run a Fisher's exact test for you as well. Returning to our examples from the previous section, Table 6.4 from the text:

```{r echo=FALSE}
kable(table6.4_observed, format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Remember we only need our observed data:

```{r}
table6.4_observed <- matrix(c(33,251,33,508),ncol = 2,byrow = T)
colnames(table6.4_observed) <- c("Yes","No")
rownames(table6.4_observed) <- c("Nonwhite","White")
show(table6.4_observed)
```

And now to run the Fischer test by adding an additional argument to `gmodels::CrossTable()`:

```{r}
gmodels::CrossTable(table6.4_observed,fisher = T)
```

Note that this provides a general Fischer test, as well as directional tests (greater than, less than). How might we interpret this output?

The test uses the odds ratio obtained from the 2×2 data, in this case 2.02, and compares it to the ratio that would be obtained assuming independence, which is always 1 (see previous section on Odds Ratios. You may also notice the 95% confidence intervals. If the interval for any of the tests contains 1 then it must be that $p>.05$.

## Cohen's kappa

Cohen's kappa is a measure of inter-judge agreement and used when we want to assess the reliability of judges or raters, or inter-rater reliability. For example.

Two judges interview 30 adolescents and rate them as exhibiting (1) no behavior problems, (2) internalizing behaviors, (3) externalizing behaviors:

```{r Construct Cohens Kappa - Table, echo=FALSE}
kappaExample <- c(15,2,3,20,1,3,2,6,0,1,3,4,16,6,8,30) %>% matrix(.,ncol = 4,byrow = T) %>% as.table()
rownames(kappaExample) <- c("NoProblem","Internalizing","Externalizing","Total")
colnames(kappaExample) <- c("NoProblem","Internalizing","Externalizing","Total")

kable(kappaExample,format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

The frequency of Judge 1's reports move down columns, Judge 2's reports move across rows. What is being conveyed on this table are the intersection of those reports. For example moving across the top row: there were 15 adolescents that both judges agreed had "No Problem""; there were 2 that Judge 1 viewed as "Internalizing" and Judge 2 viewed as "No Problem"; there were 3 that Judge 1 viewed as "Externalizing" and Judge 2 said "No Problem", etc.

In class we mentioned that we can get a measure of agreement based upon the observed agreements (along the diagonal in this table) and their expecteds (the amount of agreement we might expect purely due to chance). Based upon that, when can calculate from by hand:

```{r calculate Cohens Kappa by Hand}
N <- 30

KappaAgree <- c(15,3,3)
kappaExpec <- c((16*20)/30,(6*6)/30,(8*4)/30)

kappa <- (sum(KappaAgree)-sum(kappaExpec))/(N-sum(kappaExpec))

show(kappa)
```

Alternatively assuming we have saved this table as an object in `R` we can use the `Kappa` function from the `vcd` package:

Constructing the table, note that I'm using piping to build my table in a single line (vector ➔ matrix ➔ table): Also note that I an using the table structure describe above (Rater 1 along columns, Rater down rows)

```{r Cohens Kappa - if we have the table}
kappaExample_table <- c(15,2,3,1,3,2,0,1,3) %>% matrix(.,ncol = 3,byrow = T)
rownames(kappaExample_table) <- c("NoProblem","Internalizing","Externalizing")
colnames(kappaExample_table) <- c("NoProblem","Internalizing","Externalizing")

show(kappaExample_table)
```

And now Cohen's Kappa:

```{r}
vcd::Kappa(kappaExample_table)
```

Note that `psych` also has a `cohen.kappa()` function, but the data needs to be structured differently.



That's it for now. Be sure to check back later. 
```


<!--chapter:end:05week.Rmd-->

# Correlation and Regression

In this week's class we covered **correlation** and **regression**. In what follows we will revisit the ideas from this week's lecture making explicit callbacks to topics from slides and using an example from the text. The examples from this page involve R. For a tutorial on how to perform similar analyses in SPSS, please check the appropriate video on the course Box Drive. Please note that this vignette assumes that you have the following packages installed and loaded in R:

```{r}
pacman::p_load(car, # new qqPlot function
               cowplot, # apa plotting
               tidyverse, # tidyverse goodness
               psych,
               lm.beta # getting standard coefficient
               ) 
```

## The Relationship b/tw Stress and Health

**From Howell (Section 9.2):** Wagner, Compas, and Howell (1988) investigated the relationship between stress and mental health in first-year college students. Using a scale they developed to measure the frequency, perceived importance, and desirability of recent life events, they created a measure of negative events weighted by the reported frequency and the respondent’s subjective estimate of the impact of each event. This served as their measure of the subject’s perceived social and environmental stress. They also asked students to complete the Hopkins Symptom Checklist, assessing the presence or absence of 57 psychological symptoms.

This data (and most data mentioned in the text) can be accessed directly from Howell's [companion website](https://www.uvm.edu/~dhowell/methods8/).

```{r}
# Data from Table 9.2 Howell
example <- read_table("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab9-2.dat")

psych::describe(example)
```

### Testing our assumptions

Here we are interested in the relationship between perceived **Stress** (as measured on a scale factors the number and impact of negative life events) and the presence of psychological **Symptoms**. As with any other parametric analysis, we first need to see if our data adheres to the assumptions of **normality** and **homogeniety of varience**. We can use the tools we've been employing for the last few weeks test these assumptions:

First **Stress**:

```{r}
hist(example$Stress,breaks = 10)
car::qqPlot(example$Stress)
shapiro.test(example$Stress) # see Field (2014), Sec 5.6.1
```

Eyeballing the plots, our **Stress** measures look slightly skewed, but not egregiously so. This is confirmed by our obtained `skew` and `kurtosis` values. The final function `shapiro.test()` performs a Shapiro-Wilkes test ($W$) of normality by comparing our observed distribution against a theoretical normal. Here the null hypothesis is that the $observed == theoretical$, where $p < .05$ indicates that the observed distribution is *not normal*. Our obtained p-value confirms suggests the possibility that **Stress** measures deviate from normal. However, as noted in the Field text (5.6.1) we need to be careful using the Shapiro-Wilkes test on large samples. Ultimately you need to make a judgment on whether or not all of the evidence available leads you to the conclusion of non-normality. In this case I would trust the normality of this data due to what I see in the Q-Q plot (very few large deviations from the normal line).

**Now Symptoms**

```{r}
hist(example$Symptoms,breaks = 10)
psych::describe(example$Symptoms)
car::qqPlot(example$Symptoms)
shapiro.test(example$Symptoms) # see Field (2014), Sec 5.6.1

```

**Symptoms** does not pass either the eyeball or the Shapiro-Wilkes tests. These data are positively skewed. One way of dealing with non-normal data is by performing a logarithmic transformation (see Field, 5.8). This has already been performed for this data, **lnSymptoms** is a natural logarithmic transform of **Symptoms**. With your own data, this can be accomplished quite simply in R by using the `log()`:

**Question 1. Perform a natural log transform of Symptoms and save it to a new column in your data frame `TransformedData`. Assess `TransformedData` using a qqPlot:**

```{r}
example$TransformedData <- log(example$Symptoms)
```
```{r}
car::qqPlot(example$TransformedData)
```

## Plotting the data

One of the first things that you should do is plot your data. Plotting gives you a sense of what is going on with your data. In fact, **YOU SHOULD NEVER TAKE A TEST RESULT AT FACE VALUE WITHOUT FIRST LOOKING AT YOUR DATA!!** Beware of [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)!

Let's plot using `ggplot`:

**Question 2: create a ggplot scatterplot**
```{r}
p <- ggplot(example, aes(x = example$Stress, y = example$lnSymptoms)) + geom_point() + theme_cowplot() + xlab("Stress") + ylab("lnSymptoms")

show(p)
```

## Covariance and Correlation

Both the Howell and Field texts offer excellent overviews of **covariance** and **correlation** so I won't go into too much depth here. Briefly, let's make a few connections to ideas that we've already encountered. 

### Covariance

Recall that **variance** may be calculated as: &nbsp; 
$s_{x}^{2}=\frac{\sum \left ( x_{i}-\bar{X} \right )^{2}}{n-1}$

Where the numerator is the sum of squared differences from each score to the sample mean, and the denominator is our degrees of freedom.

Variance tells us to what degree scores in a particular sample variable deviate from its mean. With this in mind, **covarience** is a statement about the degree to which two sampled variables deviate from their respective means. Consider we have sampled two measures, *X* & *Y* from a population. When addressing the degree to which *X* and *Y* co-vary, we are asking the question: "To what degree and in what direction does *Y* move away from its mean as *X* moves from its mean?" 

As such, the formula for covariance is simply an extension of the formula for variance that we already know: 
$$s_{xy}=\frac{\sum \left ( x_{i}-\bar{X} \right )\left ( y_{i}-\bar{Y} \right )}{n-1}$$

So, in order to calculate the covariance we need the calculate the sum of the cross-product of the sum of squared differences of our two variables (*X* = **Stress**; *Y* = **lnSymptoms**) and divide that number by of degrees of freedom. We could go about the business of calculating this "by hand":

```{r covariance and correlation by hand}

# N = number of rows in data frame
N <- nrow(example)


#mean Stress:
meanX <- mean(example$Stress)

# and same for lnSymptoms:
meanY <- mean(example$lnSymptoms)

# plug these values into our equation:

# sum of cross product
numerator <- sum((example$Stress-meanX)*(example$lnSymptoms-meanY))

# degrees of freedom
denominator <- (N-1)

# covariance:
covXY <- (numerator/denominator) %>% print()
```

The covariance of Stress and lnSymptoms is **1.336**.

**Question 3: Alternatively, covariance may be calculated quickly in `R` using the `cov()` function:**

```{r}
cov(example$Stress,example$lnSymptoms)
```

### Correlation

It may be tempting to calculate our **covariance** and stop there, but covariance is a limited measure. What I mean by this is that covarience indexes the degree of relationship between two specific variables, but doesn't allow for more general comparison across situations. For a quick example, lets multiply both **Stress** and **lnSymptoms** by two. In R, (assuming all columns are `numerics`) we can accomplish this by multiplying the entire data frame by 2:
```{r warning = FALSE}
example.by2 <- example * 2
head(example.by2)
```

We would like to think that multiplying every value by a constant should have no effect on our general interpretation of the relationships in our data as the overall relationship in our data remain the same. To help make this apparent, let's imagine that I weigh 200 lbs (yea... imagine) and my wife weighs 100 lbs. So I weigh twice as much as my wife. Over the next year we both go on a binge fest and both double our respective weights—I'm now 400 lbs and my wife is 200 lbs. I still weigh twice as much as my wife. Coincidentally the fact that relationships remain unchanged in spite of these sorts of mathematical transformations is why we could perform the natural logarithmic transform earlier and not feel too guilty.

Well what happens when I get the covariance of my `example.by2` data?:
```{r warning = FALSE}
cov(example.by2$Stress,example.by2$lnSymptoms)
```

**The covariance changes!!** This is a problem, and is why, in order to usefully convey this data we report the **correlation**. The correlation is a *standardized* covariance.

The unit of measurement we'll use for standardization is the standard deviation. We can standardize the covariance in one of two ways:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. Standardize our variables (into z-scores) and then calculate the covariance: $$r_{xy}=\frac{\sum z_{x}z_{y}}{n-1}$$

```{r warning = FALSE}
zX <- scale(example$Stress)
zY <- scale(example$lnSymptoms)

corXY <- (sum(zX*zY) / (N-1)) %>% print()
```

or

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Calculate the covariance and standardize it (by dividing by the product of the standard deviation):  $$r_{xy}=\frac{\sum \left ( x_{i}-\bar{X} \right )\left ( y_{i}-\bar{Y} \right )}{(n-1)s_{x}s_{y}}$$

```{r warning = FALSE}

# get SD of X and Y:
sdX <- sd(example$Stress)
sdY <- sd(example$lnSymptoms)

# using the covXY calculated above:
covXY / (sdX*sdY)
```

Again, in `R` we don't need to do this by hand, there are in fact several functions. A more comprehensive look can be found in Field 6.5.3. The simplest function is `cor()`, with outputs the correlation as a single value. However, I prefer to use `cor.test()` as it tends to provide the most immediately useful data. You can input your data into this function in two ways:

```{r eval=FALSE}
cor.test(example$Stress,example$lnSymptoms)
```
or
```{r warning = FALSE}
cor.test(~Stress+lnSymptoms,data = example)
```

I prefer the latter as it uses formula notation which is the sort of notation we will use for regression and later t.tests, and ANOVA. Finally, if you don't want all the additional gobble-dee-gook from `cor.test` you can simply attach `$estimate` to the end of either the function or object like so:
```{r warning = FALSE}
cor.test(~Stress+lnSymptoms,data = example)$estimate
```

How did I know that I could do this you ask? To get a quick list of things that you can pull from the attributes, try the `attributes()` function. 

**Question 4** save the `cor.test()` output of the correlation between `Stress` and `lnSymptoms` as an object `corOut`. Submit `corOut` to the `attributes()` function. Grab the `p.value` and `estimate`:

```{r}
corOut <- cor.test(~Stress+lnSymptoms,data = example)
attributes(corOut)

corOut$p.value
corOut$estimate
```

Our correlation is expressed in terms of the **Pearson's product-moment correlation coefficient** (or $r$, for short). Here $r$ = .529.

Coincidentally, returning to our problematic transformed data `example.by2`, we find that the obtained $r$ is identical:
```{r warning = FALSE}
cor.test(~Stress+lnSymptoms,data = example.by2)
```

Note that when our sample size is small we may need to adjust $r$. This is because the sampling distribution of $r$ is not normally distributed.

The formula for this adjustment is: &nbsp;&nbsp;&nbsp; $r_{adj}=\sqrt{1-\frac{\sum \left (1-r^2 \right )\left ( n-1 \right )}{n-2}}$

For now, we can calculate this by hand, but later we will see that it will be provided by another function (or at least its squared value will).
```{r warning = FALSE}

# Pearson's r:
rXY <- cor.test(~Stress+lnSymptoms,data = example)$estimate

# adjusted r:
rXYadj <- sqrt(1-((1-rXY^2)*(N-1)/(N-2))) %>% print()
```

## Sigfnificance testing $r$
It may be useful to perform tests of significance on $r$. Typically, there are two types of tests that we perform:

1. a test of the observed $r$ to $r=0$, and
2. a test of the difference between two $r$ values. 

Regarding the first case, the test that the observed correlation is different from 0, Howell correctly notes the following (Section 9.11): 
- "a test on b is equivalent to a test on r in the one-predictor case we are discussing in this chapter. If it is true that $X$ and $Y$ are related, then it must also be true that $Y$ varies with $X$—that is, that the slope is nonzero. This suggests that a test on $b$ (beta coefficient) will produce the same answer as a test on $r$, and we could dispense with a test for $b$ altogether."

The reverse is also true. In the single predictor case, if one has a test of $b$ this obviates the need to run an independent test of $r$. As you will see below, we indeed do have a simple test that gives us the significance of the beta coefficient, $b$.

In the second case, the difference between $r$ values, we may invoke the `paired.r()` function from the `psych` package. Using the $r$ values from the example in Section 9.11 of the Howell text, "Testing the Difference Between Two Independent $r$s":

```{r}
psych::paired.r(xy = .50, xz = .40,n = 53,n2 = 53,twotailed = T)
```
where:

- `xy`: the correlation in the first data set
- `xz`: the correlation in the second data set
- `n`: the number of samples in the first data set
- `n2`: the number of samples in the second data set
-`twotailed`: run a two-tailed test, `TRUE` or `FALSE`

This output tells us the resulting Fisher $z$ score and corresponding probability.

**Question 5**: For example assume that you sample 35 men and find a correlation of .42 between Symptoms and Stress. In another sample of 42 women you find a correlation of .51. Perform a test to see if those two correlations are different from one another.

```{r}
psych::paired.r(xy = .42, xz = .51,n = 35,n2 = 42,twotailed = T)
```


## Fitting the data to a model

As social scientists, we are not only concerned with observation, but also with explanation. Measures of correlation provide us with the former; we use regression methods to build models in service of the latter. Again both the Field and Howell texts provide excellent overviews of regression, so I won't repeat much of what they say here.

For the purposes of this vignette (and course) we will limit ourselves to linear regression, that is describing a line that best fits our data. By "fit" we mean a line than when compared to our observed data minimizes our squared residuals.

To perform a linear regression we may call upon the `lm()` function. This function uses formula notation `outcome variable ~ predictor variable(s)`. A simple regression has a single predictor. More often in our analyses we are concerned with the relative effects of multiple predictors, but this is multiple regression and saved for the Spring course. Here, we may be interested in the degree to which perceived **Stress** contributes to the number of **lnSymptons**, or using formula terminology: "How do lnSymptoms (outcome) vary as a function of Stress (predictor)". This is represented in R as `example$lnSymptoms~example$Stress`

## Linear models in R

Our model assigned to `example.model`:
```{r warning = FALSE}
example.model <- lm(example$lnSymptoms~example$Stress)
print(example.model)
```

While this output only gives us our coefficients (intercept and slope of our line), our object `example.model` has the class `lm`. This simply means that R understands that this object is storing a linear model. Hiding behind this spartan output is a multitude of info that may be accessed via its `attributes` or it may be thrown into other functions for additional info and analysis. 

### Attributes of class `lm`:
As I mentioned above, `lm` objects contain various attributes that may be accessed using the names convention, `$nameOfAttribute`.

For example, returning to the `example.model` object we created above, we may get a glimpse of its attributes by:
```{r warning = FALSE}
attributes(example.model)
```

This command tells us the names of several attributes (values) that are stored in `example.model`. Typically we are interested in the `$coefficients`, `fitted.values` (aka *predicted values*), and `$residuals`. Recall that in our model `fitted.values` are the predicted values of **lnSymptoms** for each value of **Stress**, that is those values that fall along the line of best fit. `residuals` are the difference between our observed values of **lnSymptoms** and those predicted by the model. Mathematically `example.model$residuals` is equivalent to:
```{r eval=FALSE}
example$lnSymptoms - example.model$fitted.values
```

### Testing the residuals

An important assumption test for our model is that the residuals are normally distributed. In fact some argue that this is more important than than having normal distributions in the raw data itself. Quickly, we can test this using the regular methods:

```{r eval=FALSE}
hist(example.model$residuals,breaks = 10)
car::qqPlot(example.model$residuals) %>% show()
psych::describe(example.model$residuals)
```

**Question 6: Run a Shapiro-Wilkes test on our `example.model` residuals. How do we feel about the residuals here?**
```{r}
shapiro.test(example.model$residuals)
```


### Using your `lm` object with other functions

As I mentioned above you may also use `lm` objects with other functions. For example, for our model `example.model`:
```{r eval=FALSE}
plot(example.model)
```
produces a host of successive plots, including a plot of "Residuals v. Fitted", A Q-Q plot that includes possible outliers, and a leverage plot. The last is outside of the scope of this course, but may be useful for you to remember in the Spring!

A useful call for our present purposes is:
```{r eval=TRUE}
summary(example.model)
```
which provides us with info about the coefficients, their standard error, *t*-scores, and corresponding p-values (Pr>|t|). It also provides us with our $r^2$ and adjusted $r_{adj}^2$ (which corresponds to our $r$ and $r_{adj}$ from above). 

Coincidentally we can also call the individual attributes of this summary as well:

```{r warning = FALSE}
summary(example.model) %>% attributes()
summary(example.model)$coefficients
```


We'll return to each of these later, but first let's return to our plot:


### Plotting the regression line
It's useful practice to plot the regression line. In fact, I would typically recommend doing this at the outset of the analysis (way back at the beginning) but we had a few things to cover make this step clear. To add a simple regression line to our previous plot we can use the `geom_smooth()` function. `geom_smooth()` takes the following arguements:

-`method`: what kind of regression line do you want to create? in this case we are performing a linear regression, so `lm`.

-`mapping`: is you did not specify your `aes()` in the original call you need to specify them here (whats on the x-axis, y-axis, etc).

-`level`: create a ribbon specifying a confidence interval of each predicted value (here I'm using 95% CI).

-`color`: what color do you want the line.

-`linetype`: solid line? dotted line? dashed line? default is solid.


```{r}
# creating a new plot with our original scatter plot as a template:
p <- ggplot2::ggplot(example, aes(x = example$Stress, y = example$lnSymptoms)) +
  geom_point() + 
  theme_cowplot() + 
  xlab("Stress") + ylab("lnSymptoms")

smooth_p <- p + geom_smooth(mapping = aes(x = Stress,y = lnSymptoms), method=lm, level = 0.95, color="black")
show(smooth_p)
```

The shaded ribbon around the line of regression represents the 95% CI of the estimate at each value of the predictor. The size of the CI may be adjusted by using the `level` parameter in `geom_smooth()`.

Comparing this plot to our `example.model` output:
```{r}
summary(example.model)
```

From this plot it becomes apparent that the model coefficients obtained above represent the values of the slope and intercept of the regression line. 

- The beta coefficient conveys the *slope* of the line—unit change in outcome per unit change in predictor. Typically this is expressed as *b* in text or **B** in SPSS. In `R` output, the beta estimate is tied to the name of the corresponding predictor variable. In our case that's `example$Stress`. 

- The **intercept** tells us what the value of the outcome would be if the predictor was 0. In most instances the intercept is not terribly useful. For example if you were looking at the relationship between height and weight it would make no sense to concern yourself with instances where height is absolutely 0. Obviously, a caveat here is if the value 0 is meaningful for your analysis. For example perhaps you are looking at depression as a function of alcohol consumption and want to explicitly say something about people that have never had a single drink in their life. 

One way of making a meaningless intercept meaningful is by centering your data (subtracting every value of your predictor from a constant). For example, if you center around the mean, the intercept will tell you the predicted value of the outcome variable at the mean value of the predictor.

Just for giggles, lets plot our mean-centered data. We mean center `Stress` by subtracting each value of `Stress` from its mean:
```{r}
ggplot2::ggplot(data = example) +
  # plot each data point as a small red open circle
  geom_point(aes(x=(Stress-mean(Stress)), y=lnSymptoms), color = "black", size = 2, shape = 19) +
  geom_smooth(aes(x=(Stress-mean(Stress)), y=lnSymptoms), method=lm, level = .95, color="black") +
  theme_cowplot() + 
  xlab("Stress") + ylab("lnSymptoms")
  
```

as you can see slope doesn't change, but the values become centered around the intercept.

### A note on degrees of freedom of our model
The general equation for calculating the requisite degrees of freedom of a model is $n-k$ where $n$ is the number of observations (or later conditions) and $k$ is the number of parameters that are required to be known for the calculation.

Recall how we calculate variance / standard deviation. We take the sum of the squared deviations from the *mean* $\sum(X-\bar{X})$ and divide by $(n-1)$. Remember that we are subtracting that 1 because in order to calculate the variance we must keep one thing constant, the **mean**. In this case the **mean** is our parameter that must be known in order to calculate the variance.

We can now ask the question "what must be known in order to derive the line of best fit?", or more simply what must one know in order to create a particular line. We must know **both** the line's **slope** and its **intercept**. Knowing slope alone leads to the indeterminism presented in Fig A (identical slopes, infinitely-many intercepts), intercept alone leads to indeterminism presented in Fig B (identical intercept, infinitely-many slopes). We need **both** to define a particular line. Just like the **mean** above, these are the two parameters that must be *locked-in* to derive our model.

```{r echo=FALSE, fig.height=4, fig.width=8}
figA <- ggplot() +
  xlab("y-intercept")+
  xlim(c(-10,10))+ylim(c(-10,10))+
  geom_vline(xintercept = 0, linetype="dashed", color = "black", size=1) +
  geom_abline(intercept=c(0,0), slope=1) +
  geom_abline(intercept=c(0,3), slope=1) +
  geom_abline(intercept=c(0,6), slope=1) +
  geom_abline(intercept=c(0,9), slope=1) +
  geom_abline(intercept=c(0,-3), slope=1) +
  geom_abline(intercept=c(0,-6), slope=1) +
  geom_abline(intercept=c(0,-9), slope=1)


figB <- ggplot() +
  xlab("y-intercept")+
  xlim(c(-10,10))+ylim(c(-10,10))+
  geom_vline(xintercept = 0, linetype="dashed", color = "black", size=1) +
  geom_abline(intercept=c(0,0), slope=-1.5) +
  geom_abline(intercept=c(0,0), slope=-1) +
  geom_abline(intercept=c(0,0), slope=-.5) +
  geom_abline(intercept=c(0,0), slope=0) +
  geom_abline(intercept=c(0,0), slope=.5) +
  geom_abline(intercept=c(0,0), slope=1) +
  geom_abline(intercept=c(0,0), slope=1.5)

cowplot::plot_grid(figA,figB, labels = c("A", "B"))
```

A simple way of restating this is that your model degrees of freedom are the: $$number \space of \space observations -  number \space of \space predictors$$

where at least one of your predictors is used as a constant (the intercept) and the remainder provide your $\beta$ coefficients.

For simple regression we have 1 predictor and 1 constant so our degrees of freedom are $n-k$; where $k=2$. However, in your future **there be be dragons**... models that may have multiple predictors and multiple intercepts (multiple regression, mixed effects / multi-level models, etc).

### Interpreting the output

With our plot in hand, we can return to our `summary(example.model)` output:

- **Call**: tells us the formula we originally input for this model
- **Residuals**: descriptive stats of the residuals
- **Coefficients**: provides the estimated values, standard error of the estimates, and NHST of the intercept and beta coefficient. In both cases the null hypothesis is Estimate = 0. Again for the intercept this may or may not be useful. For example you might use it to test whether people who haven't had a single drink of alcohol have no depression at all. The test of `beta` tests against the null hypothesis of zero correlation (relationship). All tests are conducted provided the t-value on $n-k$ degrees of freedom (see next bullet point)

- **Residual standard error**: This is calculated by first getting the sum of the squared residuals and dividing that number by the degrees of freedom of the model. In this case the df equals the number of total observations (107) minus the number of parameters ($k$) that were calculated by the model in order to generate the predicted values (2). This may be interpreted as a measure of goodness of fit of the model, and perhaps more importantly its predictive value (see Howell 9.7).

- **Multiple R-squared** and **Adjusted R-squared**: The **coefficient of determination** calculated from $r$ or $r_{adj}$. $r^2$ tells us the degree to which our model accounts for observed variance in the observed outcomes. On was to think about this is, to what degree does variation in our predictor explain the observed variation in our outcomes. More on this below.

- Finally, this output gives is the resulting $F$-ratio and corresponding $df$ for an $F$-test. This is a significance test for our $r^2$. Does our model explain a significant amount of the variance?

### The coeff. of determination as index of explanatory value

Here we may turn to a (a little bit longer than expected) digression involving J.S. Mill and *exact* and *inexact* sciences. One philosophical conceit is that we can never really empirically observe causes, but only effects. That is, I can never truly observe that one billiard ball *caused* another to move, I can only say with a high degree of certainty that I observed the first billiard ball striking the second and subsequently the second moved at a particular velocity. In order to deal with this epistemological crisis (how can you really ever know anything!!!) we build explanatory models. The degree to which our model is able to account for the range of our previous observations tells us how useful our model parameters (predictors) are in explaining a phenomenon in question. 

For example, to explain what I observe about the contact and subsequent motion of billiard balls I might build a model that takes into account the mass and velocity (speed and direction) of the first ball, the mass and velocity of the second ball and the point and angle of contact. Presuming I use these as predictors, I can account with varying degrees of success the heading and direction of the balls, or the *outcome*. In fact, in this example, I'm likely to be very successful in describing most, **but not all** observed outcomes. That is, there are some variations in the outcomes that just using these parameters does not result in a successful account. I can refine my *billiard contact model* to include things like  the friction caused from the felt, the force of gravity, the angle of the table top, etc., and it's likely the case that with each new predictor my model becomes better. However at some point I  begin to reach diminishing returns (see over-fitting next semester).

This is why physics *appears* to be an exact science (well, mechanical physics at least). They've had centuries to develop and refine their models about the mechanical workings of our very local corner of the universe to the point that these models are able to account for a very high degree of variation in observed phenomena. These models typically have a very high $r^2$ and as such tend to have high predictive value. Which is why with some understanding of gravitational force, mass, and a few simple calculations, high school students all over country are able to do [this](https://www.youtube.com/watch?v=46CJmWlxfck) in their physics lab.

Indeed within some disciplines having an $r^2$ < .9 means you model isn't good enough. Typically this is the case when the phenomenon under observation is not complex (hope that doesn't offend). As you move away from physics and chemistry (or at least the low complexity phenomena under each discipline), the explanatory value of models tends to decrease. Us folks in the social sciences, we are moving into inexact-science-land. Weather patterns, economies, ecosystems, and human behaviors are highly complex; in many cases in inordinate number of contributing factors may be present in an observed outcome. As a result degree of variation that these models may account for is typically diminished, especially in the social sciences. For example depending on the phenomena and sub-discipline, an $r^2$ of .3 might mean you're cooking with gas! In fact, for some phenomena, a reported $r^2$ that is too high might be a cause for suspicion ("they weren't so much cooking with gas so much as they were cooking the books").

As a final note, its typically poor practice to get a significant $r^2$ and just stop there. Remember, this is just a claim that your model accounts for significantly more variation than 0%. If you were to have a model that accounted for 6% ($r^2 = .06$) of the variance but was significant ($p<.05$), you need to ask yourself whether that model is useful at all. It certainly has very little predictive value (I wouldn't bet my life... or even a few bills on it predicting a future outcome), and its true explanatory value is quite questionable. 
<br>
<br>
<br>
![](https://imgs.xkcd.com/comics/linear_regression.png)


## Write up and presentaion

Our obtained results can be reported in several different ways. However, typically it's best practice to report both the $beta$ coefficient with corresponding $t-test$; as well as the general statistics about the model including the $r^2$ and corresponding $F-test$. Prior to any reporting of the stats, you also need to articulate the structure of the model.

I think a good template to start from is:

>"[Clearly restate the alternative hypothesis identifying operationalized variables]. To test this hypothesis the data were [what type of analysis?] with [what is your dv] as the outcome and [what is your predictor] as our predictor. The resulting model was significant, r^2=XXX; _F_(df1,df2)=F-ratio, _p_=p-value and revealed a significant relationship between the [covariables], _b_=XX; _t_(df)=XX, _p_=XX$."

For example, with this data you may report it as:

>"We hypothesized that increases in self-reported measures of Stress would correspond to increases in the number of self-reported symptoms. Given that our Symptoms data showed a significant violation of the normality assumption, we transformed these scores using a log-transformation (lnSymptoms). To test this hypothesis the data were submitted to a simple linear regression with lnSymptoms as the outcome and Stress as our predictor. The resulting model was significant, $r^2$=.28; $F$(1,105)=40.73, p<.001$, and revealed a significant positive relationship between the Stress and Symptoms, $b$=.008; $t$(105)=6.38, $p$<.001."

That said, in many circles it's typical practice  to report the *standardized* $beta$ coefficients ($\beta$). These are the $beta$ coefficients we would have obtained if we had initially converted both our *Stress* and *lnSymptom* values to $z$-scores. We can retroactively obtain these coefficients by using the `lm.beta()` function from the `lm.beta` package (duplicate names, confusing I know). We can then input an `lm` class objects into the `lm.beta()` function and get the summary of those results. The new column `Standardized` is reported rather than `Estimate`:

**Question 7** Use `lm.beta::lm.beta()` to obtain the standard coefficients. How would you report these results?

```{rs}
lm.beta::lm.beta(example.model) %>% summary()
```

So amending the example reporting paragrah above:

>The resulting model was significant, $r^2$=.28; $F$(1,105)=40.73, $p$<.001, and revealed a significant positive relationship between the Stress and Symptoms, $\beta =.528$; $t(105)=6.38$, $p<.001$."

Coincidentally you may have noticed that this value is the correlation that we established earlier. Again I point to Howell 9.11 for how these two are similar in the single predictor case.

## Advanced plotting

Depending on what info you are trying to convey, there are several ways of dressing up your plot. In this section we'll cover three things that you can do with the plots above:
- adding text information like $r^2$ and the equation of the regression line
- placing two (or more) plots side-by-side
- placing two sample (and regression lines) on the same plot

### Adding text to the plot

Text may be added to a plot using the `annotate()` function in `ggplot2`. This function has several arguments: the type of annotation (typically `geom="text"`), the `x` position of the plot (measured in units of the x axis), the `y` position of the plot, the `label` or text that you want to add (in string form) and the whether or not `R` needs to parse the text. For simple text like "Hi my name is.." doesn't need parsing. For more complex text like some of what we will be doing (text that needs to be italicized, superscripts, etc) you need to set `parse=TRUE`. Finally you can adjust the size of the text.

We can create the necessary text for out plot by hand by simply adding these repective values in format that we construct. However, there are a few leaps and bounds that we need to do in order to format the text and plug in our values. For the same of simplicity just take this at face value for now, but feel free to follow-up in your free time to see exactly what is going on here:

```{r}
# putting together my equation, telling R
# to add superscripts and italics:
equation <- italic(y) == 4.301 + 0.009 %.% italic(x)*","~~italic(r)^2~"="~.280

# turn it into an expression for R to parse:
equation <- as.expression(equation) %>% as.character()


```


and now add it to my previous plot using `annotate()`
```{r}
# create scatter plot
p <- ggplot2::ggplot(data = example) +
  # plot each data point as a small red open circle
  geom_point(aes(x=Stress, y=lnSymptoms), colour = "black", size = 2, shape = 19) + theme_cowplot()

# adding the regression line
p <- p + geom_smooth(aes(x=Stress, y=lnSymptoms), colour = "black", method=lm, level = 0.95)

# adding text
p <- p + annotate(geom = "text", x=20, y=5, label=equation, parse = T, size = 4)

show(p)
```

You'll note that my text is centered at the intesection of where `x=20` and `y=5`. You may need to play around with these values to get this right.

### addnig text like a R-ninja:

The other useful thing to remember is that you can access different parts of an `lm` class object using its attributes (see above). In this case we can call on various attributes of `example.model`. For example we can get the coefficients by:
```{r}
example.model$coefficients
```

we call also get them by using the `coef()` function:
```{r}
coef(example.model)
```

the same can be said about the $r^2$. It's available from the summary model attributes:
```{r}
# list our attributes:
summary(example.model) %>% attributes()

#we want "r.squared":
summary(example.model)$r.squared
```

knowing this we can actually create a function to do the above for us automatically (see I told you building you own functions would com in handy). True, there are a few things that we haven't covered yet in the code, so feel free to look them up:

```{r}
# create a function called lm_eqn
lm_eqn = function(m) {

  l <- list(a = format(coef(m)[1], digits = 2) %>% unname(),
      b = format(abs(coef(m)[2]), digits = 2) %>% unname(),
      r2 = format(summary(m)$r.squared, digits = 3));

  if (coef(m)[2] >= 0)  {
    eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,l)
  } else {
    eq <- substitute(italic(y) == a - b %.% italic(x)*","~~italic(r)^2~"="~r2,l)    
  }

  as.character(as.expression(eq));                 
}
```


after running the chunk  above (or executing the function in to memory) you can just plug in you `lm` object into the new function `lm_eqn()`:

```{r}
# plug in the lm.model
equation <- lm_eqn(example.model)

# create scatter plot
p <- ggplot2::ggplot(data = example) +
  # plot each data point as a small red open circle
  geom_point(aes(x=Stress, y=lnSymptoms), colour = "black", size = 2, shape = 19) + theme_cowplot()

# adding the regression line
p <- p + geom_smooth(aes(x=Stress, y=lnSymptoms), colour = "black", method=lm, level = 0.95)

# adding text from "equation" object above
p <- p + annotate(geom = "text", x=20, y=5, label=equation, parse = T, size = 4)


show(p)
```

### Placing plots side-by-side in a grid

You can place two or more plots in a grid using the `plot_grid()` function from the `cowplot` library. You can check out an [intro to cowplot here](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html). Indeed, `cowplot` does so much more than grid plotting! `cowplot` can be installed from CRAN, so we can simply:

```{r}
pacman::p_load(cowplot)
```

The `plot.grid()` function has several arguments. For now, we'll just focus on a few, including the *list of plots to include*, *labels for each plot*, and their *arrangement* specified in the number of rows and columns. For the purpose of this example and the next, let's create a new data set that takes a look at the data above as broken down by men and women:

```{r}
# read in the data above:
example2 <- read_table("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab9-2.dat")

# create a gender vector of 57 women and 50 men:
gender <- c(rep("w",57),rep("m",50))

# randomize gender using sample
set.seed(1) # just so we randomize the same
gender <- sample(gender)

# add gender to our example 2 data.frame
example2$gender <- gender
show(example2)
```

Now that we have our data set we can separate out women's data from men's data. We've done this before using the `filter` function in `dplyr`. Another way is to use the `subset()` function:
```{r}
womenData <- subset(example2, gender=="w")
menData <- subset(example2,gender=="m")
```

now we can create two separate plots...
```{r fig.height=4, fig.width=8}
# women plot
wPlot <- ggplot2::ggplot(data = womenData, aes(x=Stress, y=lnSymptoms)) +
  # plot each data point as a small red open circle
  geom_point(colour = "red", size = 2, shape = 1) +
  geom_smooth(method=lm, level = 0.95) +
  theme_cowplot()

# men plot
mPlot <- ggplot2::ggplot(data = menData, aes(x=Stress, y=lnSymptoms)) +
  # plot each data point as a small red open circle
  geom_point(colour = "red", size = 2, shape = 1) +
  geom_smooth(method=lm, level = 0.95) +
  theme_cowplot()

```

And plot them side-by-side. To do so, in the `cowplot::plot_grid()` function I need to enter the name of each plot; the corresponding labels that I choose, and tell `plot_grid` how many columns there should be in my grid:

```{r}
plot_grid(wPlot,mPlot, labels = c("A","B"), ncol = 2)
```

Note here I've labeled the women's plot "A" and men's plot "B". I can label these whatever my hear desires, though typical convention is lettering. I can also stack them vertically by specifying `nrow` instead of `ncol`:

```{r}
plot_grid(wPlot,mPlot, labels = c("W","M"), nrow = 2)
```

### Two series on the same damn plot
*on the same damn plot, on the same damn plot...*

For our last trick, let's plot both men and women's data on the same figure. This can be accomplished by using our original `example2` data frame and using `aes()` to specify different shape, colors, and regression lines as a function of gender:
```{r}
# add color and shape arguments here. These arguements in aes() indicate that
# different shapes and colors should be used for each level of gender:
bothPlot <- ggplot2::ggplot(data = example2, aes(x=Stress, y=lnSymptoms, shape=gender,color=gender)) +
  # only need to specify size here:
  geom_point(size = 2) +
  geom_smooth(method=lm, level = 0.95)

show(bothPlot)
```

Of course to make this APA I'll need to add my `cowplot()` to the end. I'll also need to perform a few manual tweaks of the color, `scale_color_manual()` takes a vector of color names.

```{r}
bothPlot <- ggplot2::ggplot(data = example2, aes(x=Stress, y=lnSymptoms, shape=gender,color=gender)) +
  # only need to specify size here:
  geom_point(size = 2) +
  geom_smooth(method=lm, level = 0.95) + 
  theme_cowplot() + 
    
  # colors, make one series black and the other grey
  # greys have a numeric value 10, 20, 30 etc that determines how light:
  scale_color_manual(values = c("black", "grey60"))


show(bothPlot)

```

Note that I can also specify `linetype` aesthetic in `geom_smooth()` for different lines:

```{r}
bothPlot <- ggplot2::ggplot(data = example2, aes(x=Stress, y=lnSymptoms, shape=gender,color=gender)) +
  # only need to specify size here:
  geom_point(size = 2) +
  geom_smooth(method=lm, level = 0.95, aes(linetype=gender)) + 
  theme_cowplot() + 
    
  # colors, make one series black and the other grey
  # greys have a numeric value 10, 20, 30 etc that determines how light:
  scale_color_manual(values = c("black", "grey60"))


show(bothPlot)

```


<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>


<!--chapter:end:06week.Rmd-->

# Testing differences in means: t-test

This week we covered when and how to conduct a $t-test$. We use a t-test to assess whether the observed difference between sample means is greater than would be predicted be chance. Both the Field text and Howell text do a wonderful job of explaining t-tests conceptually so I will defer to those experts on matters of the underlying their statistical basis. Instead my goal this week is to walk through some examples on performing, interpreting, and reporting t-tests using `R`. 

This walkthough assumes that the following packages are installed and loaded on your computer:
```{r}
pacman::p_load(tidyverse, car, cowplot, lsr)
```


## Things to consider before running the t-test

Before running a `t.test` there are a few practical and statistical considerations that must be taken. In fact, these considerations extend to every type of analysis that we will encounter for the remainder of the semester (and indeed the rest of your career) so it would be good to get in the habit of running through your checks. In what proceeds here I will walk step by step with how I condunct a `t.test` (while also highlighting certain decision points as they come up).

### What is the nature of your sample data?

In other words where is the data coming from? Is it coming from a single sample of participants? Is it coming from multiple samples of the SAME participants? Is it coming from multiple groups of participants. This will not only determine what analysis you choose to run, but in also how you go about the business of preparing to run this analysis. Of course, truth be told this information should already be known before you even start collecting your data, which reinforces an important point, your central analyses should already be selected BEFORE you start collecting data! As you design your experiments you should do so in a way in which the statistics that you run are built into the design, not settled upon afterwards. This enables you to give the tests you perform the most power, as you are making predictions about the outcomes of your test _a priori_. This will become a major theme on the back half of the class, but best to introduce it now.

For this week, it will determine what test we will elect to perform. Let's grab some sample data from the Howell text (Table 7.3):


> Description from Howell: Everitt, in Hand, et al., 1994, reported on family therapy as a treatment for anorexia. There were 17 girls in this experiment, and they were weighed before and after treatment. The weights of the girls, in pounds, is provided in the data below:

```{r}
Tab7_3 <- read_delim("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab7-3.dat", 
                     "\t", escape_double = FALSE, trim_ws = TRUE)
```

So what is known: we have 17 total participants from (hypothetically) the same population that are measured twice (once Before treatment, and once After treatment). Based upon the experimental question we need to run a paired-sample (matched-sample) test. (Although I'll use this data to provide an example of a one-sample test later on).

### What is the structure of your data file?

Before doing anything you should always take a look at your data:

```{r}
show(Tab7_3)
```

So what do we have here, three columns:

- `ID`: the participant number
- `Before`: participants' weights before treatment
- `After`: participants' weights after treatment

Most important for present purposes this data is in `WIDE` format—each line represents a participant. While this might be intuitve for tabluar visualization, many statistical softwares prefer when `LONG` format, where each line represents a single *observation* (or some mixed of WIDE and LONG like SPSS).

** [See this page for a run down of WIDE v. LONG format] (https://www.theanalysisfactor.com/wide-and-long-data/)

** More, for those that are interested I suggest taking a look at these two wonderful courses on DataCamp:

- [Working with Data in the Tidyverse](https://www.datacamp.com/courses/working-with-data-in-the-tidyverse)
- [Cleaning Data in R](https://www.datacamp.com/courses/cleaning-data-in-r)

#### Getting data from WIDE to LONG

So the data are in wide format, each line has multiple observations of data that are being compared. Here both `Before` scores and `After` scores are on the same line. In order to make life easier for analysis and plotting in `ggplot`, we need to get the data into long format (`Before` scores and `After` scores are on different lines). This can be done using the `gather()` function from the `tidyr` package. We touched briefly on this function in this week's lecture. Here we go into a little more detail.

Before gathering, one thing to consider is whether or not you have a column that defines each subject. In this case we have `ID`. This tells `R` that these data are coming from the same subject and will allow `R` to connect these data when performing analysis. That said, for `t.test()` this is not crucially important—`t.test()` assumes that the order of lines represents the order of subjects, e.g., the first `Before` line is matched to the first `After` line. Later on when we are doing ANOVA, however, this participant column will be important an we will need to add if it is missing.

**Using `gather()`:** It may first make sense to talk a bit about the terminology that this function uses. For every data point you have a `key` and a `value`. Think of the `key` as how the data point is defined or described, while the `value` is the measure of the data point. Typically in research we describe the data in terms of the condition under which it was collected—so if it helps, think of the `key` as your IV(s) and the `value` as your DV. 

With this data, our `values` would be the weights of each participant. The `keys` would be how they are differentiated, `Before` and `After`. So for value I would input "weight" and for key I might choose "treatment" noting that the levels are Before-treatment and After-treatmemt. 

One other thing that I have to consider is that I don't want every column of my data gathered. For example in this case I don't want my `ID` column gathered, but rather duplicated. Let's look at what happens if I just gather:

```{r}
gather(data = Tab7_3,key = "treatment",value = "weight")
```

That's not right! In order to overcome this I need to exclude the column(s) that I don't want gathered. What `R` will do is copy those appropriately. This can be accomplished by stating with columns for the original data set you do not want gathered at the end of the `gather()` function, placing a "-" (negative sign) in front of them. In this case I don't want the first column, `ID` gathered, so:

```{r}
# "-1" here means exclude the first column
gather(data = Tab7_3,key = "treatment",value = "weight",-1)

# or, "-ID" means exclude the column named ID, both will work. 
# note that I'm just making this output invisible to avoid duplicates:
gather(data = Tab7_3,key = "treatment",value = "weight",-ID) %>% invisible()
```

Ok data is structured correctly, on to the next step.

### Testing assumptions

Remember that you should always test to see if the data fit the assumptions of the test you intend to perform. In this case, we need to assess two things:

#### Is the data normally distributed?

Knowing the design of your experiment also has implications for testing your assumptions. For example, whether you have a paired (matched) sample design (e.g., two samples from the same participants) or an independent sample design (e.g., two groups) determines how you go about the business of testing the normality assumption. If you have an independent samples test, you test each sample seperately, noting measures of skew, kurtosis, inspecting the qqPlot, and Shapiro-Wilkes test (though acknowledging that SW is very sensitive). However, if you are running a paired (matched) samples test, you need to be concerned with the distribution of the difference scores. In the present example we are comparing participants' weights `Before` treatment to their weight `After`. 

First, let me save my gathered data to a data_frame `Tab7_3_gathered` and then `filter()` accordingly for `Before` and `After` (though note that you could elect to gather after this step and simply use the `Before` and `After` columns for the original dataset):

```{r}
Tab7_3_gathered <- gather(data = Tab7_3,key = "treatment",value = "weight",-ID)
beforeTreatment <- filter(Tab7_3_gathered, treatment=="Before") 
afterTreatment <- filter(Tab7_3_gathered, treatment=="After") 
```

And now compute the difference scores, and run my assumption tests:
```{r}
diffWeights <- beforeTreatment$weight - afterTreatment$weight

psych::describe(diffWeights)
car::qqPlot(diffWeights)
shapiro.test(diffWeights)
```


#### Is the data variability homogeneous?

Another important assumption is that the variablility within each sample is similar. For a t-test this can be tested by using the `leveneTest()` from the `car` package:

```{r}
# using long-format enter as a formula:
car::leveneTest(weight~treatment, data=Tab7_3_gathered, center="mean")
```

You'll note above I elected to mean center my samples. This is consistent with typical practive although "median" centering may be more robust (both Field and Howell reading this week get into the particulars of this test).

Given that my obtained `Pr(>F)`, or p-value of Levene's F-test, is greater than .05 I may elect to assume that my variances are equal. However, if you remained skeptical, there are adjustments that you may make. This includes adjusting the degrees of freedom according to the Welch-Satterthwaite. Recall later on that we are looking at our obtained $t$ value with respect to the number of $df$. This adjustment effectively reduces the $df$ in turn making your test more conservative.

### Getting the descriptive stats and plotting the means.

Finally, as we will be performing a test of difference in means, it would be a good idea to get descriptive measures of means and variability for each group. Indeed, these data were already obatined when we used `psych::describe()` to assess the normailty of each sample. Here I'll just do it again to get these values:

```{r}
psych::describeBy(Tab7_3_gathered$weight,group = Tab7_3_gathered$treatment)
```

Typically along with the mean, you need to report a measure of variability of your sample. This can be either the SD, SEM, or if you choose the 95% CI, although this is more rare in the actual report. See the supplied HW example and APA examples on BOX for conventions on how to report these in your results section.

* Plotting in ggplot *

I've mentioned several times the limits and issues with plotting bar plots, but they remain a standard, so we will simply proceed using these plots. But I'll note that boxplots, violin plots, bean plots, and pirate plots are all modern alteratives to bar plots and are easy to execute in `ggplot()`. Try a Google search. 

In the meantime, to produce a bar plot in `R` we simply modify a few of the arguments that we are familiar width. 

Here is the code for plotting these two groups:

```{r}
ggplot(data = Tab7_3_gathered, aes(x=treatment, y=weight)) +
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .1) +
  scale_y_continuous(expand = c(0,0)) + 
  theme_cowplot()

```

Breaking this down line-by-line:

- `ggplot(data = Tab7_3_gathered, aes(x=treatment, y=weight))`: standard fare for starting a `ggplot`. See the Appendix treatment on intro to `ggplot` to refresh your memory (or the DataCamp course).

- `stat_summary(fun.y = "mean", geom = "col")`: `stat_summary()` gets summary statistics and projects them onto the geom of your choice. In this case we are getting the mean values, `fun.y = "mean"` and using them to create a column plot `geom = "col"` . 

- `stat_summary(fun.data = "mean_se", geom = "errorbar", width = .1)` : here we are greating error bars, `geom = "errorbar"`. Important to note here is that error bars require knowing three values: mean, upper limit, and lower limit. Whenever you are asking for a single value, like a mean, you use `fun.y`. When multiple values are needed you use `fun.data`. Here "mean_se" requests Standard error bars. Other alternatives include 95% CI "mean_cl_normal" and Standard deviation "mean_sdl". The `width` argument adjusts the width of the error bars.

- `scale_y_continuous(expand = c(0,0))`: Typically `R` will do this strange thing where it places a gap bewteen the data and the `x-axis`. This line is a hack to remove this default. It says along the y-axis add `0` expansion (or gap).

- `theme_cowplot()`: quick APA aesthetics.

You may also feel that the zooming factor is off. This may especially be true in cases where there is little visual discrepency between the bars. To "zoom in" on the data you can use `coord_cartesian()`. For example, you might want to only show the range between 70 lbs and 100 lbs. When doing this, be careful not to truncate the upper limits of your bars and importantly your error bars. 

```{r}
ggplot(data = Tab7_3_gathered, aes(x=treatment, y=weight)) +
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .1) +
  scale_y_continuous(expand = c(0,0)) + 
  theme_cowplot() +
  coord_cartesian(ylim = c(70,100))
```

Additionally, to get this into true APA format I would need to adjust my axis labels. Here capitalization is needed. Also, because the weight has a unit measure, I need to be specific about that:

```{r}
ggplot(data = Tab7_3_gathered, aes(x=treatment, y=weight)) +
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .1) +
  scale_y_continuous(expand = c(0,0)) + 
  theme_cowplot() +
  coord_cartesian(ylim = c(70,100)) +
  xlab("Treatment") + 
  ylab("Weight (lbs)")
```

Finally, you may have notice that the order of Treatment on the plot is opposite of what we might like to logically present. In this case the "After" data comes prior to the "Before" data on the x-axis. This is because `R` defaults to alphabetical order when loading in data. To correct this I can use `scale_x_discrete()` and specify the order that I want in `limits`:

```{r}
ggplot(data = Tab7_3_gathered, aes(x=treatment, y=weight)) +
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .1) +
  scale_y_continuous(expand = c(0,0)) + 
  theme_cowplot() +
  coord_cartesian(ylim = c(70,100)) +
  xlab("Treatment") + 
  ylab("Weight (lbs)") + 
  scale_x_discrete(limits=c("Before","After"))
```

All good (well maybe check with Sierra first)! One other thing to consider (although please do not worry about it here) is the recent argument that when dealing with repeated measures data you need to adjust you error bars. See [this pdf]( http://www.tqmp.org/Content/vol04-2/p061/p061.pdf) by Richard Morey (2005) for more information on this issue.

## Performing the t-test (Paired sample t-test)

Okay, now that we've done all of our preparation, we're now ready to perform the test. We can do so using the `t.test()` function. In this case, the experimental question warrants a `paired` samples t-test. Given that our Levene's test failed to reject the null, we will assume that our variances are equal. 

Again, since we've got long-format data we will use the formula syntax:

```{r}
t.test(weight~treatment,data=Tab7_3_gathered,paired=T, var.equal=T)
```

The output provides us with our $t$ value, the $df$ and the $p$ value. It also includes a measure of the 95% CI, and the mean difference. Remember that the null hypothesis is that there is no difference between our two samples. In the case of repeated measures especially, it makes sense to think of this in terms of a difference score of change, where the null is 0. The resulting interpretation is that on average participants' weight increased 7.26 pounds due to the treatment, with a 95% likelihood that the true mean change is between 3.58 lbs and 10.95 lbs. Important for us is that 0 is not in the 95% CI, reinforcing that there was indeed a non-zero change (rejecting the null).

## Measuring effect size

As always, rejecting the null due to a significant $p$ value is not the end of the story. The absolute null is almost always guarenteed to be false. The question is whether or not you have compelling evidence to demonstrate it so. For example I likely could have collected data from 1000 participants here and found a "significant" difference with a weight gain of 0.5 lbs ($p<.05$). Would we feel like that was compelling evidence of the effect of the treatment? Probably not!

Whenever you report any statistic, you need to report an effect size. For $t$ tests, the appropriate effect size measure is Cohen's D. Cohen's D expresses the observed difference as a ratio of the standard deviation of the sample(s)—in this respect its conceptually similar to our understanding of the standard distribution where we understand the magnitude of any score as expressed in the number of standatd deviations away from a 0 mean. Here, the mean difference betweem means (7.26) is divided by either the pooled standard deviations of both samples (weighted average) or the standard deviation of one of the samples. For example using the Before group SD:

$$D = \frac{\mid \bar{X}_{before}-\bar{X}_{after} \mid}{SD_{before}} = 
\frac{7.26}{5.02}$$

Typically you will use the pooled standard deviation, though in some circumstances it may make sense to use the standard deviation of a single group. For example if your variences are unequal you may elect to use the SD of the control group, that way you understand the observed change in scores in scales of magnitude to the control.

Let's calculate Cohen's D the pooled variance, and only using the varience of a single group. We use `lsr::cohensD()` with the formula syntax:

```{r}
# pooled SD
lsr::cohensD(weight~treatment,data = Tab7_3_gathered,method = "pooled")

# using SD of the first group (alphabetical order): After
lsr::cohensD(weight~treatment,data = Tab7_3_gathered,method = "x.sd")

# using SD of the first group (alphabetical order): Before
lsr::cohensD(weight~treatment,data = Tab7_3_gathered,method = "y.sd")
```

As you can see you end up with very different values depending on what you choose. As I said, the default is the "pool" your variences. If you elect to do otherwise you'll need to mention it in your report.

## Other $t$ tests:

### One sample:

The data in our example warranted running a paired t-test. However, as noted we can run a `t.test()` to compare a single sample to a single value. For example it might be reasonable to ask whether or not the 17 adolescent girls that Hand, et al., 1994 treated were different from what would be considered the average weight of a teenaged girl. A quick Google search suggests that the average weight of girls 12-17 in 2002 was 130 lbs. How does this compare to Hand et al.'s participants `Before` treatment? We can run a one sample t-test to answer this question:

```{r}
beforeTreatment <- filter(Tab7_3_gathered,treatment=="Before")
t.test(beforeTreatment$weight, mu = 130)
```

Yes, this group of girls was significantly underweight compared to the national average.

### Independent samples example

We run an independent samples t-test when we have no reason to believe that the data in the two samples is meaningfully related in any fashion. Consider this example from Howell, Table 7.7 regarding Joshua Aronson's work on stereotype threat:

> Joshua Aronson has done extensive work on what he refers to as “stereotype threat,” which refers to the fact that “members of stereotyped groups often feel extra pressure in situations where their behavior can confirm the negative reputation that their group lacks a valued ability” (Aronson, Lustina, Good, Keough, Steele, & Brown, 1998). This feeling of stereo- type threat is then hypothesized to affect performance, generally by lowering it from what it would have been had the individual not felt threatened. Considerable work has been done with ethnic groups who are stereotypically reputed to do poorly in some area, but Aronson et al. went a step further to ask if stereotype threat could actually lower the performance of white males—a group that is not normally associated with stereotype threat.

> Aronson et al. (1998) used two independent groups of college students who were known to excel in mathematics, and for whom doing well in math was considered impor- tant. They assigned 11 students to a control group that was simply asked to complete a difficult mathematics exam. They assigned 12 students to a threat condition, in which they were told that Asian students typically did better than other students in math tests, and that the purpose of the exam was to help the experimenter to understand why this difference exists. Aronson reasoned that simply telling white students that Asians did better on math tests would arousal feelings of stereotype threat and diminish the students’ performance.

Here we have two mutually exclusive groups of white men, those controls and those under induced threat.. Importantly we have no reason to believe that any one control man's score is more closely tied to any individual experimental group counterpart than any others (we'll return to this idea in a bit).

Here is the data:

```{r}
Tab7_7 <- read_delim("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab7-7.dat", delim = "\t")
```

As before, let's take a look at the file structure:

```{r}
show(Tab7_7)
```

I want to look at this example as it give is an opportunity to deal with another common issue in data cleaning. If you take a look at `Group` you see it's either `1` or `2`. Based upon Howell Table 7.7 we can deduce that `Group 1` are the control subjects and `Group 2` are the threat subjects. Using numbers instead of names to identify levels of a factor is a convention from older methods and software. In more modern software you don't need to do this sort of dummy coding (the software works this out in the background).

If you want to change this, you can use the `recode()` function from `dplyr` package in the `tidyverse` (https://dplyr.tidyverse.org/reference/recode.html). For what it's worth there are several other ways to do this including a `recode()` function in car. See http://rprogramming.net/recode-data-in-r/ for examples.

Here I'm just going to overwrite the `Group` column with the recoded names:

```{r}
Tab7_7$Group <- dplyr::recode(Tab7_7$Group, "1"="Control", "2"="Threat")
show(Tab7_7)
```

An now to run the requisite assumption tests. Note that in this case I am running an Indepednent samples tet, so I need to test the assumptions on each sample seperately:

*Control:*
```{r}
controlGroup <- filter(Tab7_7, Group=="Control")
qqPlot(controlGroup$Score)
```

*Threat:*
```{r}
threatGroup <- filter(Tab7_7, Group=="Threat")
qqPlot(threatGroup$Score)
```

*Homogeniety of Variance:*

```{r}
car::leveneTest(data=Tab7_7, Score~Group)
```

*T-test*

The Levene's test failed to reject the null so I may proceed with my `t.test` assuming variances are equal. Note that `paired=FALSE` for independent sample tests:

```{r}
t.test(data=Tab7_7, Score~Group, paired=FALSE, var.equal=T)
```

This output gives us the $t$-value, $df$ and $p$-value. Based on this output I may conclude that the mean score in the Control group is significantly greater than the Threat group.

Just as an example, let's set `var.equal` to `FALSE`:
```{r}
t.test(data=Tab7_7, Score~Group, paired=FALSE, var.equal=F)
```

Comparing the outputs you see that in this case `R` has indicated that it has run the test with the Welsh correction. Note that this changes the $df$ and consequently the resulting $p$ value. That this change was neglible reinforces that the variances were very similar to one another. However in cases where they are not close to one another you may see dramatic changes in $df$.

In `R`, the `t.test()` function sets `var.equal=FALSE` by default. Why you ask? Well, you can make the argument that the variances are ALWAYS unequal, its only a matter of degree. Assuming variances are unequal makes your test more conservative, meaning that if the test suggests that you should reject the null, you can be slightly more confident that you are not committing Type I error. At the same time, it could be argued that setting your `var.equal=TRUE` in this case (where the Levene test failed to reject the null) makes your test more powerful, and you should take advantage of that power to avoid Type II error.

## Independent or Paired Sample?

It is safe to assume that anytime that you are collecting data samples from the same person at two different points in time that you need to run a paired-samples test. However, it would not be safe to assume that if the samples are coming from different groups of people that you always run an independent samples test. Remember the important qualifier mentioned above: That there no reason to believe that any one participant in the first group is is more closely related to any single counterpart in the second group than the remaining of others. In our Independent test example we have no reason to assume this is the case, we assume that members of the Control and Threat groups were randomly selected. But what if we instead recruited brothers or twins? In this case, it may make sense to treat members of the two groups as paired; brothers have a shared history (education, socio-economic level, family dymanic, etc) that would make their scores more likely to be related to one another than by random chance. Howell makes a similar point in Exercise Question 7.19 at the end of the chapter.



OK. That's it for this week. Be sure to check the Appendix for an additional write-up connecting t-test (and ultimately ANOVA) to the General Linear Model and regression analyses that we performed last week.



<!--chapter:end:07week.Rmd-->

# Analysis of Variance I: the One-way ANOVA

In this week's class we covered **ANOVA**. A major emphasis in the lecture was that ANOVA (much like many of the other tests that we cover in this course) is an extension of the general linear model. In many respects, ANOVA and regression are conceptually identical—whereas in linear regression our predictor variables are typically continuous (or, in some cases ordinal) we usually reserve ANOVA for instances when our predictors are discrete or nominal. This would be the difference say in predicting weight as a function of height (linear regression) in contrast to weight as function of hometown (Dayton, Youngstown, Cleveland, Cincinnati). Given that both the Howell (Chapter 11) and Field (Chapter 10) do a wonderful job of explaining the underlying principles of ANOVA, I won't spend too much time here rehashing what is already available there. Field, especially does an excellent job of demonstrating how even though regression and ANOVA are often treated differently in terms of research focus (e.g., observation v. experimentation) and data focus (correlation/goodness of fit v. comparing means) they are indeed one and the same. Here, my goal is to reinforce this idea using examples in `R`, as well as providing a practical tutorial that will serve as our entry point into ANOVA. As always, for an example of how to perform these analyses in SPSS please [check the appropriate course materials on Box Drive](https://uc.box.com/v/psych7014).

## Required packages

This write-up assumes that you have the following packages installed and loaded in R:

```{r warning = FALSE, message=FALSE}
# check to make your computer has pacman installed, if not install
if (!require(pacman)){install.packages('pacman')}

# use pacman to check, install, and load necessary packages
pacman::p_load(car,
               cowplot, 
               tidyverse,
               psych)
```



## ANOVAs for comparing means

We typically understand ANOVA as a method for allowing us to compare means from more than two samples. Two see how this connects with what we have learned from regression lets use the data provided in Howell's main example from Chapter 11.1 (see book for background info). 


## Pre-processing the data:

To start, lets download Eysenck's (1974) `dataset`:
```{r downloading dataset}
dataset <- read_delim("https://www.uvm.edu/~dhowell/methods8/DataFiles/Ex11-1.dat", delim = "\t")

show(dataset)
```

### Recoding the factors (if dummy coded)
As we can see right off the bat the data is number (dummy) coded, where 1 = 'Counting', 2 = 'Rhyming', 3 = 'Adjective', 4 = 'Imagery' and 5 = 'Intentional'. My advice for what to do if you get dummy coded data is to create a corresponding column in your data set that contains the factors in nominal (name) format. 

Recall from last week that we can use the `recode()` function to reassign the dummy variables. In that case we recoded for the purpose of plotting. Here, we are recoding the levels of a factor. Here, the preferred function is an extension of `recode()`: `recode_factor()`. When we `recode_factor()` we have the added benefit of automatically _factorizing_, telling `R` to treat the IV as a factor. Let's create a new column `dataset$GROUP_FACTOR` that contains this data:


```{r renaming dummy codes}
# assigning the appropriate names for the dummy codes
dataset$GROUP_FACTOR <- dplyr::recode_factor(dataset$GROUP, "1" = "Counting","2"="Rhyming","3"="Adjective","4"="Imagery","5"="Intentional")
```

### Renaming column headers
(optional)
And now, just to be clear, let's rename the original `GROUP` to `GROUP_NUM`. This can be accomplished by using the `names()` function. Heres the logic:

`names()` gives you the column names of your data frame:
```{r}
names(dataset)
```

You can isolate any individual name using indexing:
```{r}
names(dataset)[1]
```

And after you've isolated a column you can re-assign a new name:
```{r}
names(dataset)[1] <- "GROUP_NUM"
```

[Check out this link](https://www.datanovia.com/en/lessons/rename-data-frame-columns-in-r/#renaming-columns-with-dplyrrename) for info on how to rename multiple columns at once using `names()` or `dplyr::rename()` from the `tidyverse`.

### Reordering your levels

One important consideration that you should have even before you look at your data is what is your **control** (group, condition). Proper experimentation requires a proper control in order to properly isolate the influence of the manipulation. Here the best candidates for our control group might either be "Counting" or "Intentional", depending on how the original problem was approached. If the larger comparison involved "Intentional v. incidental" learning for recall, then the "Intentional" group serves best as your control. If the original question involved levels of processing, then "Counting"" (theoretically the lowest level of incidental processing) is best. Here I am assuming the latter (although I believe theoretically Eysenck originally was interested in the former).

I bring this up, as its typically best to ensure that your control is entered first into the ANOVA model. To check the order of your levels, you may simply:

```{r}
levels(dataset$GROUP_FACTOR)
```

Here we see that "Counting" is first and will therefore be entered first into the model. 

Assuming that we wanted to reorder the sequence, say to have `Intentional` as the control, then we might simply:

```{r re-levling}
dataset$GROUP_FACTOR <- factor(dataset$GROUP_FACTOR, levels = c("Intentional","Counting","Rhyming","Adjective","Imagery"))
levels(dataset$GROUP_FACTOR)
```

However, I liked the original order, so let's change it back:

```{r}
dataset$GROUP_FACTOR <- factor(dataset$GROUP_FACTOR, levels = c("Counting","Rhyming","Adjective","Imagery","Intentional"))
levels(dataset$GROUP_FACTOR)
```

That's better. Why the order is important will be made clear later in this write up. For now, think back to our example of running a t-test using `lm()`. You may recall that the group level that was first entered into the model served as the model intecept where the second group level was expressed in terms of the slope of the line (beta). A similar account will be happening here.

Finally, the data here is presenting in LONG format. This is the desired format for most analysis and plotting in `R`. Check last week's write-up for a discussion on WIDE and LONG format and how to switch from WIDE to LONG.

## Assumptions for ANOVA

### Checking the normality assumption, OPTION 1

To check the distribution of outcomes in ANOVA, you have two options. The first would be to check the distribution of outcomes for EACH group/condition independently. In the case of the example `dataset` we could get info related to the skew and kurtosis of RECALL for each GROUP_FACTOR using `describeBy():

```{r}
psych::describeBy(dataset,group = "GROUP_FACTOR")
```

If we wanted to perform for extensive methods like `hist()`, `qqPlot()`, and `shapiro.test()`, in the past I had you filter be each level (group) and proceed. So for example for `Counting`:

```{r}
countingGroup <- filter(dataset,GROUP_FACTOR=="Counting")

hist(countingGroup$RECALL)
qqPlot(countingGroup$RECALL)
shapiro.test(countingGroup$RECALL)
```

In the past you would have to repeat this for each other level. An alterative to generate an output by each level of a factor is to use the `by()` function. For example to generate a sequence of `qqPlot`s (for the sake of space I'm not going to execute this code here, but try at home)

```{r eval=FALSE}
# by(dependent variable, grouping factor, name of function)
by(dataset$RECALL,INDICES =dataset$GROUP_FACTOR,qqPlot)
```

You can do the same with `hist()` and `shapiro.test()`.

### Checking the normality assumption, OPTION 2

Although `by()` may or may not make life easier for you in this test case, things rapidly become more complicated when attempting to check normality by condition. For example if you're running a 2×3×3 mixed effect ANOVA, you would need to run through 18 conditions. So what to do. A simpler alternative is to run you model and analyze your residuals. [This web link](https://www.theanalysisfactor.com/checking-normality-anova-model/) does a nice and quick job of explaining the logic.

In this case we would run our ANOVA model using `aov`:

```{r}
dataset_aov <- aov(RECALL~GROUP_FACTOR, data=dataset)
```

Congrats, you've just run an ANOVA, but for now we aren't interested in the results from the model. Remember from a view weeks back that many outputs have `attributes` that may be accessed. For example:

```{r}
attributes(dataset_aov)
```

We want those residuals!! From here, we can simply take the residuals and submit them to our standard tests for normality:

```{r}
psych::describe(dataset_aov$residuals)
hist(dataset_aov$residuals)
qqPlot(dataset_aov$residuals)
shapiro.test(dataset_aov$residuals)
```

So, between OPTION 1 and OPTION 2, I'd recommend typically going with OPTION 2. 

### Homogeneity of Variance

Another assumption of ANOVA is the homogeneity of variance between groups. An easy-way to get an eyeball test of this assumption is two perform a box plot of the data. Here I am performing this plot using `ggplot2`:

```{r}
boxplots <- ggplot(data =dataset, aes(x=GROUP_FACTOR,y=RECALL)) +
  geom_boxplot()

show(boxplots)
```

Huge differences in the IQR regions may be a clue that the homogeneity assumption is violated. We can run more specific tests in `R` including the Levene Test and Figner-Killeen (non-parametric, to be used if the data is not normal) Test of Homogeneity of Variances. As is typically the case $p<.05$ indicates a violation of this assumption:

```{r}
# Levene Test of Homogeneity of Variances
car::leveneTest(RECALL~GROUP_FACTOR, data=dataset)

# Figner-Killeen Test of Homogeneity of Variances
fligner.test(RECALL~GROUP_FACTOR, data=dataset)
```

### What to do if the assumptions are violated?

If either assumption is violated, one option that you have is to transform you data. We've talked several times in class about the pros and cons of doing this, and both the Field and Howell texts provide examples for how this is done. Another option is to use a non-parametric test such as the Kruskal-Wallis Test if the data is not normal, or Welch's ANOVA is the variances are not homogenous. That said, one of reasons that ANOVA is so popular is that it has been demonstrated to be robust in the face of violated assumptions (as long as the sample sizes are equal). For example, in _Design and Analysis of Experiments_ (1999, p. 112) Dean & Voss argue that the maximum group variance may be as high as 3× the minimum group variance without any issue. With this in mind, a question (gray area) before us is _how much_ of a violation is there in the data? And if not so much, you may be fine just running an ANOVA regardless.

## Running the ANOVA in R:

There a many, many ways to run an ANOVA in `R`. Throughout the semester we will be highlighting three: `aov()`, `lm()`, and using the `afex` package. For the next two weeks we will concentrate on `aov()` which is the standard method, as well as the `lm()` method that you have used before, just to reinforce that ANOVA and regression are one in the same. In fact, SPOILER ALERT, `aov()` is just a fancy wrapper for `lm()`. 

Like `lm()` from weeks past, `aov()` asks us to enter our dependent and independent variables into the model in the formula format **DV ~ IVs**. In this case, we only have a single IV, `GROUP_FACTOR`. Thus our model is:

```{r}
aov.model <- aov(RECALL~GROUP_FACTOR,data =dataset)
```

From here, an `anova()` of the model gives us our ANOVA table.
```{r}
anova(aov.model)
```

Also note that we can get `residuals` from the `aov()` output as well. In fact take a look at the object's `$class`... see I told you, `lm()`!!!

```{r}
attributes(aov.model)
```

## Reporting your data

First let's grab the descriptive stats:

```{r}
describeBy(dataset$RECALL,dataset$GROUP_FACTOR)
```

### Reporting in the text

Reporting the omnibus ANOVA includes:

- **the F value**,
- **the degress of freedom (between and within)**,
- **the p-value**
- **effect size**: typically with ANOVA we report partial eta squared, although note that Howell advocates for omega squared. Howell gives you the equations to calculate both; and indeed they can be calculated given the info that is conveyed in the `anova(aov)` output. I won't delve too much into the how to do that here, because, well... homework. But it should be included.

For our data our report takes the form: $F(4,45)=9.085,p<.001, \eta_p^2=.45$.

As we typically focus on means with ANOVA, it is typically a good idea to report means and some report of of the distribution of each group (typically either standard deviation or standard error; although 95% CI may be useful in certain scenarios). 

So for example reporting the Rhyming group:

- **M ± SD:** 6.90 ± 2.13
- **SE:** 6.90 ± 0.67

### What the omnibus ANOVA tells you

While it may be useful to report the means, you need to be mindful of what the omnibus ANOVA tells you. Remember, that the the null hypothesis of the omnibus ANOVA is that "there are no differences between observed means". Our significant result tells us that there *are* differences between our means, **but does not tell us what those specific differences are**. So while it may be useful to report general relationships, e.g., "Recall for people in the Intentional group tended to be greater than for the Counting group" you **cannot** say definitively "Recall was significantly greater for the Intentional Group". Typically when you only have tested the omnibus ANOVA, you only speak in generalities (e.g., "Recall tended to increase with level of processing.")

Always be mindful that you don't over interpret your data.

### Plots

For ANOVA plots, there are typically three acceptable plots used to convey, box plots, bar plots, and line plots. As the ANOVA become more complex, we tend towards using line plots (box plots and bar plots may become very busy in complex designs). Unless there are compelling reasons not to we tend to plot the means (although note that box plots usually give you medians). Since we have already produced a box plot above, I'll show examples of bar and line plots. 

To create a **line plot** with points at the means and error lines representing the 95% CI (known as a point range), we can use call that you are familar with and a new geom `pointrange`. Whats nice about point range is that it allows you to create your points and the error bars all in a single line.

```{r}
p <- ggplot2::ggplot(data =dataset,mapping = aes(x = GROUP_FACTOR, y = RECALL))

# create the points
p <- p + stat_summary(fun.data = mean_cl_normal, size=1, color="black", geom="pointrange"); show(p)
```

Now to add the lines to this plot. For this you will need another `stat_summary()` line specifying that the vertices of the lines should be the means, `fun.y = mean` and a parameter that specifies how the lines should be grouped. Since we have a One-way ANOVA, `group=1`. When we built to more complex designs you may elect to `group=FACTOR_NAME`. Something like this is useful to say make some lines dashed and some lines solid according to levels on a factor. More on this in two weeks when we get to factorial ANOVA.

While we're at it let's fix those axes titles. We can use the functions `xlab()` and `ylab()` to do so.

```{r}
# this is from before:
p <- ggplot2::ggplot(data =dataset,mapping = aes(x = GROUP_FACTOR, y = RECALL))
p <- p + stat_summary(fun.data = mean_cl_normal, size=1, color="black", geom="pointrange"); 

# and now adding the lines
p <- p + stat_summary(data=dataset, fun.y = mean, size = 1, color = "black", mapping=aes(group=1), geom="line")

# and fixing the axis titles:
p <- p + xlab("Group")+ylab("Words recalled")

show(p)
```

Finally, for aesthetic reasons yo may elect to expand the y-axis. For example to make the range of y-axis values (0,20):

```{r}
p + expand_limits(y=c(0,20))
```

Now a **bar plot**, with standard error bars. This is similar to how you built your bar plots from last week (t-test)

```{r}
p <- ggplot2::ggplot(data =dataset,mapping = aes(x = GROUP_FACTOR, y = RECALL))
p <- p + stat_summary(fun.y = mean, geom = "bar");
p <- p + stat_summary(fun.data = mean_se, geom = "errorbar", aes(width=.25))
p <- p + xlab("Group")+ylab("Words recalled")

show(p)
```

and fixing the gap below the zero:
```{r}
# fixing gap below zero:
p <- p + scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,17.5))

show(p)
```

Note that when you present a figure, (A) **you need to refer to the figure in the text** and (B) you need to provide a figure caption that gives adequate detail. The caption should go below the figure:

```{r}
show(p)
```

*Figure 1*. Mean words recalled as a function of learning group. Error bars represent standard error.

<br><br>


## DIGGING DEEPER: ANOVA and Regression

Now that we've got practical matters out of the way, I want to take some time to dig a little deeper into the connections between ANOVA this week and our work on correlations and regressions in the past. 

As we've already mentioned (and spent time discussing in class) ANOVA is just an extension of the simple linear model that we covered last week, where ANOVA is used when our predictors are discrete. In fact `aov()` is simply a wrapper for the `lm()` function that we used last week. For example, let's run our model using `lm()` and then pipe it into `anova()`

```{r}
lm.model <- lm(RECALL~GROUP_FACTOR, data=dataset)
anova(lm.model)
```

All `aov()` does is take an `lm` object and produce an ANOVA table from the results. If we were simply to look at the `lm() model` we see that it gives us the info in our ANOVA table at the end of the summary, including the `F-statistic` (9.085), the `degrees of freedom` (4 and 45) and the `p-value` (1.815e-05):
```{r}
lm(RECALL~GROUP_FACTOR, data=dataset) %>% summary()
```

Taking a look at this output, we see that the `lm() model` also gives us a lot of other additional useful info. For example the $R^2$ of the model may be understood as the effect size of the ANOVA. However, when we report it for ANOVA we express it as... dun, dun, dunnnn... *partial eta-squared*!!, or $\eta_p^2$. 

Zooming in on the coefficients:
```{r echo=FALSE}
lm.summary <- lm(RECALL~GROUP_FACTOR, data=dataset) %>% summary() # save above to object
lm.summary$coefficients
```

We see information about the means of each group relative to the `(Intercept)`. *This is why I stressed earlier that it may be useful to rearrange the order of your levels such that `R` enters your control group into the model first*. In this case, the first predictor entered is assigned to the `(Intercept)`. The remaining predictors in the model are then presented relative to the first. Since we entered "Counting" first, the estimate of the  `(Intercept)` represents its mean. The means for each remaining group are the sum of its estimate coefficient and the `(Intercept)`. So for example the mean of the Rhyming group is $(-0.1)+(7.0)=6.9$.

The coefficients section also gives us one other useful bit of information, the *t*-values of the estimate. As we learned two weeks ago, for a simple regression the beta coefficient gives us information about the slope of the regression line and the corresponding *t*-value is a test of the null $beta=0$. So for a simple regression this tells us if our slope is significantly different from 0.

A similar logic applies to the ANOVA model. As I mentioned above, deriving the means of each level of our IV is a matter of summing the (Intercept) and the beta estimate for that level. It should be apparent, then, that the beta estimates here represent the slope of a line that between the intercept and the mean of the level (where the distance between the predictor and coefficient is treated as a unit 1). Therefore, a significant *t*-value for beta tells us that the slope between the two means is significant, or that those two means are significantly different from one another. Keep in mind the only comparisons that are being made here are between the individual levels of our IV and the control (Intercept). So while this output allows us to make a claim about the difference between the means of the Counting (Intercept) group compared to each of the other groups, it **does not** allow us to make a claim about differences between our other level—for example no information about a statistical test of differences between the Rhyming and Imagery groups is conveyed here. However, assuming you are interested in deviations from your control, you can get info here quickly. There is a caveat here, in that our alpha criterion will need to be adjusted to be more conservative than .05. More on this next week.

### Regression method v. means method

By now you may ask: "Well, if ANOVA is simply linear regression then why involve different functions?" You may even note (and be peeved about the fact) that the manner which we mathematically derived the *F* ratio used an entirely different equation than was used this week, even though the both equations yield the same result! Why?!?!?!?!?!

Sadly, tradition and convention are all I can tell you. While in both cases the *F-value* is an expression of amount of variance that your model (`lm` or `aov`) explains given total variance that exists in the data. In the ANOVA case we are concerned with the means of discrete groups or IVs. Since `lm` deals with continuous predictors (or IVs) then it makes little sense to talk about means, other than the grand mean. In both instances, however, the derived F is an expression of the model's fit.

My goal for this section is to demonstrate to you that the regression method we used to derive the the *F* ratio last week is identical to the means method Howell introduced you to this week. First to recap the regression method, our equation from the correlation week's lecture slides was:

$$F=\frac{r^2 / df_{reg}}{(1-r^2)/df_{res}}$$

- where $r^2$ is the model's coefficient of determination
- $df_{reg}$: the model's $df$ (based on # of predictors), and
- $df_{res}$: the model's residual $df$

moreover, 

$$r^2=\frac{SS_Y-SS_{res}}{SS_Y}$$

- where $SS_Y$ is the sum of squared differences between the observed values $Y$ and the grand mean $\bar{Y}$
- $SS_{res}$ is the sum of squared differences between the observed values $Y$ and the model-predicted values $\hat{Y}$ (or distance to regression line).

### Comparing two models

To conceptualize lets take another look at our data again. But first it may help to take a look at [Section 2.6](https://github.uc.edu/pages/tehrandavis/gradStats/measures-of-distribution-central-tendency-plots-oh-my.html#looking-ahead-means-and-better-models) on this site where I talk about means and better models. Go ahead... I'll wait.

OK, now that you're back let's have another look at our example data. In the plot below, the dotted line represents the *grand mean* model, where we are using the mean of *all data* to predict individual scores. This corresponds to our assumption that all samples come from the same population and are ramdomly distributed (aka random error). Is from this _means model_ that $SS_Y$ is calculated. 

```{r}
p <- ggplot(data = dataset,mapping = aes(x = GROUP_NUM, y = RECALL)) +
  geom_point()
p <- p + geom_hline(yintercept = mean(dataset$RECALL), linetype="dashed")
p <- p + geom_smooth(method="lm", level=0.95, se = FALSE)
#p <- p + stat_summary(fun.data = mean_se, size=1, color="red", geom="pointrange")
show(p)
```

The solid line in this plot represents the line of best fit for our *ANOVA model* which, in addition to assuming random error, also assumes that `GROUP` (encoding) is a predictor of variation that we see in the data (aka systematic error).  Its from this error which $SS_{res}$ is calculated.

Thus $r^2$ is simply a statment about the difference in the grand mean model and the experimental model (in this case our ANOVA model with GROUP predicotr) residuals as a percentage of $SS_Y$.


In addition, with ANOVA we are dealing with discrete predictors rather than continuous. Visually, as you can see that the data in the plot are stacked into columns, with the mean of each group located at the center of the columns' density. This also means that rather than a continuous range of predicted values $\hat{Y}$ we have a set number of $\hat{Y}$, one for each group mean that we are considering in the ANOVA. Therefore:

- $SS_Y$ may be thought of in terms of the differences between every score compared to the grand mean.
- $SS_{res}$ may be thought of as the differences between every score within the group and the group mean $\hat{Y}$

To put it more succinctly, with ANOVA we are directly comparing our `GROUP` predictor model's ability to account for variance in the data above what we might expect by chance. Unpacking this a bit, we have a `grand_mean` model and a `GROUP_mean` model. Recall from our discussions in `Chi-square` that we can use a *Likelihood Ratio Test* to directly compare to models to see if the second accounts for significantly more variance than the first. In `R` this is accomplished using.... *dun, dun, dun* `anova()`:

```{r}
grand_mean_model <- lm(RECALL~1, data = dataset) # Here, "1"" indicates let scores simply vary randomly
GROUP_mean_model <- lm(RECALL~1 + GROUP_FACTOR, data = dataset) # add GROUP as a predictor to our original model

# run Likelihood Ratio Test to compare two models
anova(grand_mean_model, GROUP_mean_model, test="LRT")
```

The output above tells you the structure (formula) of each model and that `Model 2` accounts for significantly more variance than `Model 1` (as assessed on the Chi-sq. distribution). For what it's worth, let's run this same comparision of models using a different test, `F`:

```{r}
# run Likelihood Ratio Test to compare two models
anova(grand_mean_model, GROUP_mean_model, test="F")
```

Those numbers look familar? What does that suggest is really going on with ANOVA?

### Means method calculations

Now lets turn to our *F*-ratio equation from Howell (means method):

$$F=\frac{MS_{treat}}{MS_{error}}$$

where 

- $MS_{error}$ = $SS_{res}/df_{res}$
- and $MS_{treat}$ = ($(SS_{Y}-SS_{res})/df_{reg}$;

note that $df_{res}$ may be calculated as $k(n-1)$ where $k$ is the number of IV and $n$ is the number of scores in each group; $df_{reg}$ is $k-1$.


so for our model, `lm.model` we can calucalte the `F-ratio` using the means method as follows:
```{r}
SS_Y <- ((dataset$RECALL-mean(dataset$RECALL))^2) %>% sum()
SS_res <- (lm.model$residuals^2) %>% sum()

k <- 5 # predictors
n <- 10 # number in each group

df_res <- k*(n-1)
df_reg <- k-1
MS_error <- SS_res/df_res
MS_treat <- (SS_Y-SS_res)/df_reg

F.ratio <- (MS_treat/MS_error) %>% print()
```

### Bringing it together: it's all about the residuals, baby

So on one hand we have the *F*-ratio being calculated using $r^2$ and on the other we have it being calculated using the mean square. Underlying both is a calculation of various $SS$ and it is using this fact that we can show that the two methods are equivalent. Drawn out, we have:

$$F=\frac{r^2 / df_{reg}}{(1-r^2)/df_{res}} =\frac{MS_{treat}}{MS_{error}}$$

rewriting our $r^2$ in terms of $SS$ (sums of squares) we get:

$$\frac{\frac{SS_Y-SS_{res}}{SS_Y} / df_{reg}}{(1-\frac{SS_Y-SS_{res}}{SS_Y})/df_{res}} =\frac{MS_{treat}}{MS_{error}}$$

and rewriting our $MS$ side of the equation in terms of $SS$:
$$\frac{\frac{SS_Y-SS_{res}}{SS_Y} / df_{reg}}{(1-\frac{SS_Y-SS_{res}}{SS_Y})/df_{res}} =\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}$$

multiplying the left side of the equation by 1, or $\frac{SS_Y}{SS_Y}$:

$$\frac{(SS_Y-SS_{res}) / df_{reg}}{(SS_Y-(SS_Y-SS_{res}))/df_{res}} =\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}$$

which gives us:

$$\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}=\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}$$
What does this mean for you... well perhaps nothing. Honestly, if you are using your computer for stats a correct calculation is a correct calculation. However, conceptually it may be easier to get and keep yourself in general linear model mode, as it will make incorporating more complex modeling techniques (higher-order ANOVA, ANCOVA, mixed-models, multi-level models, growth curve models) a more intuitive step in the future.

<br><br>


<!--chapter:end:09week.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Analysis of Variance II: Multiple comparisons in One-way ANOVA


In last weeks vignette we covered One-Way ANOVA. ANOVA is useful when we are comparing 3 or more group means such that the null hypothesis is:

$$\mu_1=\mu_2=\mu_3...=\mu_n$$.

In this case, if a single mean is revealed to be significantly different from the others, then the null is rejected. However, rejecting the null only tells us that at least one mean was different from the others; it does not tell us which one or how many. For example with just three means, it could be the case that:

- $\mu_1≠\mu_2=\mu_3$
- $\mu_1=\mu_2≠\mu_3$
- $\mu_1=\mu_3≠\mu_2$
- $\mu_1≠\mu_2≠\mu_3$

Simply getting a significant *F*-value does not tell us this at all. In order to suss out any differences in our groups we are going to need to make direct comparisons between them. 

Enter multiple contrasts. Multiple contrasts are a way of testing the potential inequalities between group means like those above. As always, both Howell (Chapter 12) and Field (Chapter 10, specifically 10.4+) do wonderful jobs of laying out the mathematics and logic of multiple comparisons. As with last week I focus on practical implementation and spend some time focusing a bit on potential landmines and theoretical concerns as I see them.

## Getting started (loading packages and data)

This vignette assumes that you have the following packages installed and loaded in R: 

```{r warning = FALSE, message=FALSE}
# check to make your computer has pacman installed, if not install
if (!require(pacman)){install.packages('pacman')}

# use pacman to check, install, and load necessary packages
pacman::p_load(agricolae,cowplot, tidyverse, multcomp, psych)

```

To start, lets download Siegel's (1975) data set on Morphine Tolerance. This data set can be found on Howell's website. Please check the Howell text for background info:
```{r downloading the dataset}

# grab data from online location:
dataset <- read_table2("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab12-1.dat")

# convert dataset$Group dummycodes to named factor levels:
dataset$Group <- recode_factor(dataset$Group, "1"="MS","2"="MM","3"="SS","4"="SM","5"="McM")

# get descriptive stats for this data by Group
psych::describeBy(dataset$Time,dataset$Group)
```

And a quick peek at this data:
```{r}
ggplot(data = dataset,aes(x=Group,y=Time)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_se, geom = "errorbar", aes(width=.25)) +
  scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,35)) + theme_cowplot()
```

## Running the One-way ANOVA
Now that our data is properly coded we can run our omnibus ANOVA. My own personal preference is to run the ANOVA using `lm()`. This makes like a lot easier when dealing with contrasts, especially if you decide to employ the method that Field suggests in his guide. I'll mention more on this alternative below. That said. recall from last week that using the aov() function gives you the same result. Depending on which you choose, you can use the `summary(lm.model)` or `anova(lm.model)` to switch back and forth to get the info that you desire: 

```{r}
# running the ANOVA using lm:
lm.model <- lm(formula = Time~Group,data = dataset)
# using the summary.aov() function to display as ANOVA table
anova(lm.model)
# summary(lm.model)  # alternative output
```

So we see here that we have: $F(4,35)=27.33,p<.001,\eta_p^2=.75$

Remember again that the **only** thing that the omnibus ANOVA tells us is that there is an inequality in our means. In this respect, the omnibus begs more questions than it answers—which means are different from which. In order to get this answer we need to run direct comparisons between our means. There are two ways of going about this, we can either *(1)* plan beforehand what differences in means are especially relevant for us and focus on those, or *(2)* take a look at all potential differences without any specified predictions. In Case 1, we are performing **planned contrasts**; in Case 2, we use **post hoc** tests. More often than not, you will see researchers analyzing differences in means using post hoc tests—that is they run the ANOVA, find that it is significant, and run a battery of pairwise comparisons. It is sometimes the case that of that battery of comparisons, only a select few are actually theoretically relevant. However, if there is a theory-driven case to be made that you are predicting differences between a few select means in your data, then there is an argument to be made that you should run your planned contrasts independent of your ANOVA. That is, you are technically only permitted to run post-hoc tests if your ANOVA is significant (you can only go looking for differences in means if your ANOVA tells you that they exist), whereas planned contrasts can be run regardless of the outcome of the omnibus ANOVA (indeed, some argue that they obviate the need to run the omnibus ANOVA altogether).

My guess is that most of you have experience with post-hoc tests. They are more commonly performed tend to be touched upon in introductory stats courses. So we will spend a little time on these first before proceeding to a more in depth treatment of planned contrasts.

## Post-hoc tests

**We use a post-hoc test when we want to test for differences in means that we have not explicitly predicted prior to conducting our experiment**. As a result, whenever we perform a post-hoc test, we need to adjust our critical p-values to correct for inflation of Type 1 error. Recall from earlier discussions that the odds of committing a Type 1 error (falsely rejecting the null) is $1-(1-\alpha)^c$ where $\alpha$ is you critical p-value and $c$ is the number of comparisons that are to be performed. Typically we keep this at .05, so when conducting a single test, the likelihood of committing a Type 1 error is: 
$1-(1-.05)^1=1-0.95^1=0.05$

However as we increase the number of comparisons, assuming an $\alpha$ of 0.05:

- 2 comparisons = $1-.95^2=0.0975$
- 3 comparisons = $1-.95^3=0.1426$
- 4 comparisons = $1-.95^4=0.1855$
- 5 comparisons = $1-.95^5=0.2262$

Obviously, we need to control for this. The post-hoc methods that were introduced this week are all similar in that they involve comparing two means (*a la t*-test) but differ in how the error is controlled. For example a Bonferroni-Dunn correction (which is often used as a post-hoc correction, although initially intended for correcting planned comparisons) adjusts for this by partitioning the significance (by diving your original alpha by the number of comparisons). A popular variant of this method, the Holm test, is a multistage test. It proceeds by ordering the obtained *t*-values from smallest to largest. We then evaluate the largest *t* according to the Bonferroni-Dunn correction $\alpha/c$. Each subsequent comparison *t* value, $n$ is evaluated against the correction $\alpha/(c-n)$. Please note I mention the these two methods with post-hoc analyses, although in true they are intended for planned comparisons. However, in instances in which the number of comparisons is relatively small, I've often seen them employed as post-hocs.

So how many comparisons is relatively small? I'd suggest best form is to use the above methods when you have 5 or fewer comparisons, meaning that your critical $\alpha$ is .01. That said, with a post hoc test, you really do not have a choice in the number of comparisons you can make, you need to test for all possible comparisons on the IV. Why? well if not you are simply cherry picking your data. For example it would be poor form to run our ANOVA and plot your data like so:

```{r}
p <- ggplot(data = dataset,aes(x=Group,y=Time)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_se, geom = "errorbar", aes(width=.25)) +
  scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,35)) + 
  theme_cowplot()
  show(p)
```

and then decide that you only want to compare 'McM' to 'MS' because that's where you see the greatest differences. Or that you simply want to take a look at "MM" and "SS" without considering the rest. Since you did not plan for or explicitly predict these differences from the outset, you are simply banking on what I like to say might be a "historical accident", that you simply stumbled into these results. As such, it's deemed as proper for to test *all* contingencies.

In the case above there are $(5!)/(2!)(5-2)!$ = 10 combinations. If we were to run a Bonferroni correction in this case or critical *p* would need to be $.05/10=.005$ which is an extremely conservative value, and thus dramatically inflates the likelihood of Type II error. In cases like this Tukey's HSD is the traditionally preferred method, as it takes into account the characteristics of your data (in particular the standard error of the distribution) when calculating the critical *p* value. As such in cases where many post-hoc, pairwise comparisons are made, Tukey's HSD is less conservative than a Bonferroni adjustment. 

One final method that is becoming more *en vogue* is the Ryan, Einot, Gabriel, Welsch method (REGWQ). Whereas Tukey's method holds the critical *p* constant for all comparisons (at the loss of power) the REGWQ allows for an adjustment for the number of comparisons. It is currently being promoted as the most desirable post-hoc method.

### Bonferonni-Dunn and Holm tests

In R there are several ways in which we can call post hoc corrections. For example we can call the Bonferonni and Holm adjustments using `pairwise.t.test()` function from the `base` package (already installed). The `pairwise.t.test()` method asks you to input:

- `x` = your DV
- `g` = your grouping factor
- `p.adjust.method` = the name of your desired correction in string format

First let's run the pairwise.t.tests with no adjustment (akin to uncorrected *p* values):

```{r}
pairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = "none")
```

You see above that we get a cross-matrix containing the *p* values for each cross pair (row × column). Remember this is something we would never do in a post hoc (no corrections) but I wanted to first run this to illustrate a point. Now let's run the the Bonferroni and Holm corrections:

#### Bonferroni example (pairwise.t.test())
```{r}
pairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = "bonferroni")
```

#### Holm example (pairwise.t.test())
```{r}
pairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = "holm")
```

As you can see in the remaining tables, R actually adjusts the *p* values for you. This is different from (but analogous to) the critical *t*-value method described in Howell's text. What this means is that you may interpret the output against your original (familywise) $\alpha$. So here, any values that are still less than .05 after the corrections are significant.

### Tukey HSD and REGWQ tests

In order to run Tukey's HSD and REGWQ methods we call upon  the `agricolae` package. In this case, we need to input our `lm() model` into the function, as well as identify our "treatment" (in this case our "Group" factor). For example:

#### Tukey HSD example (agricolae)
```{r}
lm.model <- lm(formula = Time~Group,data = dataset) # from above
agricolae::HSD.test(lm.model,trt = "Group",group = T,console = T) 
```

Note that the `group` and `console` arguments pertain to the output. You typically will want to keep console set to `TRUE` as that simply prints the output of your test. The `group` argument controls how the output is presented. Above we set it to TRUE. This results in an output that groups the treatment means into subsets where treatments with the same letter are not significantly different from one another (i.e., *a*s are not significantly different from each other, *b*s are not significantly different from each other, **but** *a*s are different from *b*s). Conversely if you wanted to see each comparison you can set this to `FALSE`:

```{r}
agricolae::HSD.test(lm.model,trt = "Group",group = FALSE,console = TRUE) 
```

Finally, if you do decide to group (`group=TRUE`), you can take the outcome of this function and use it to generate a nice group plot. This is useful for quick visual inspection.

```{r}
agricolae::HSD.test(lm.model,trt = "Group",group = T,console = T) %>% plot()
```

#### REGWQ example (agricolae)
The same applies to REGW, using the `REGW.test()` function (with `group=F`, I'm showing all of the comparisons):
```{r}
agricolae::REGW.test(lm.model,trt = "Group",group = F,console = T) 
```


## The logic of Planned contrasts

If you have reasons to predict differences between particular sets of group means that are theory-driven, then you may perform *a priori* or planned contrasts. Logically, planned contrasts are similar to post-hoc tests in that we are comparing against two means, but there are some differences that make planned contrasts more powerful.

- Since your predictions are made prior to collecting data you, technically do not need to get a significant result on the omnibus ANOVA to run your contrasts. Remember, when doing post-hoc tests, if the omnibus ANOVA fails to reject the null, you are not permitted to run follow-up post hoc tests.

- Since you are making predictions prior to seeing the outcome of your observed data, then you are safe to make a limited number of comparisons without the charge of cherry-picking. For example, if you have predicted-ahead that there would be differences between groups "MS" and "McM", or groups "MM" and "MS" then you are free to run those comparisons and those comparisons-only. This is especially useful, since by limiting the number of comparisons, you can effectively reduce the problem of Type I error inflation while limiting the possibiliy of Type II error, keeping the required corrections relatively minimal. For example, recall that using a Bonferonni correction on this data in the post-hoc case mandates that I use an adjusted $\alpha$ of .005 (.05/10 comparisons), even if I was only really interested in these two comparisons. Here, I am allowed to only perform these two, so my adjusted *p* is .025.

- Depending on the number of comparisons, you may be justified in not performing any *p* correction at all. For example some recommend no need for correction if the number of contrasts is low or when the comparisons are complementary (e.g. orthogonal). See [here](http://jrp.icaap.org/index.php/jrp/article/view/514/417), [here](https://www.graphpad.com/support/faqid/1390/), and [here](http://pirun.ku.ac.th/~faasatp/734462/data/06_contrasts2.pdf)  for discussion of this issue.

- We can easily perform a variety of comparisons using planned contrasts. For example, say we are interested in whether MS is different from the mean of the remaining groups (McM+MM+SM+SS), or that MM+MS is different from McM+SM+SS. We can test this using planned contrasts.

## Performing planned contrasts in R

As outlined in both Howell and Field, planned contrasts begin by creating contrast weights. The idea with contrast weights is that the groups that are being compared should sum to equal -1 and +1 respectively, resulting in a null test against 0 (i.e. the two weighted means are equal). Any groups that are not being included in the comparison should be assigned coefficients of 0. For example assume we are comparing "MS" to "MM" and were not concerned with the remaining groups. Then our contrast weights would be:

- MS = +1
- MM = -1
- McM = SM = SS = 0

How about we want to compare McM + MM against the remaining three groups?

- MS + MM = 1
- McM + SM + SS = -1

From there we distribute the weight equally between the number of groups on each side of the contrast, so

- MS = +1/2; MM = +1/2
- McM = -1/3; SM = -1/3; SS = -1/3

in R we can construct each of these contrasts like so:
```{r}
# checking the order of groups:
levels(dataset$Group)

# building contrasts

# MS v. MM
contrast1 <- c(1,-1,0,0,0) 

# MS + MM v. SS + SM + McM
contrast2 <- c(1/2,1/2,-1/3,-1/3,-1/3)
```

When performing planned contrasts in `R` I recommend using the `multicomp` package rather than the `base` package examples in the Field text. For those that are interested as to why, in my experience the `base` method has a hard time dealing with non-orthogonal contrasts. To perform the contrast requires 3 steps:

- run your omnibus ANOVA `lm()` model and save it as an object
- input your `lm()` model into the `glht()` function, specifying the name of your IV and planned contrasts; save this object
- run a summary() of your object from step 2, including any desired adjustments. 

For example, say I wanted to run `contrast1` from my data, Comparing "MS" to "MM". 

```{r}
lm.model <- lm(formula = Time~Group,data = dataset)
contrast.model <- multcomp::glht(lm.model, linfct = mcp(Group = contrast1))
# where Group is the name of my IV, and contrast1 are my planned contrasts from above
summary(contrast.model, test = adjusted('bonferroni'))
```

The above result tells me:

- `Estimate`: the difference in means between my contrasted groups
- `Std. Error`: a measure of variability
- `t-value`: obtained from the t-test between groups
- `Pr(>|t|)`: the resulting p-value
- You'll also note in the output the specified correction method is mentioned (in this case Bonferroni)

Based upon these results I conclude that "MS" and "MM" are significantly different from one another.

Suppose I wanted to run both simultaneously? My preferred method is to create a contrast matrix by `rbind()` my contrasts. You can then place that matrix object into `glht()`:

```{r}
# create contrast matrix
contrast.matrix <- rbind(contrast1,contrast2)

# run contrasts
lm.model <- lm(formula = Time~Group,data = dataset)
contrast.model <- multcomp::glht(lm.model, linfct = mcp(Group = contrast.matrix))
# where Group is my IV, and contrast.matrix are my planned contrasts
summary(contrast.model, test = adjusted('bonferroni'))
```

You'll note that the reported p.value for `contrast1` has changed from the previous example, due to the Bonferroni correction (in this case we have 2 tests). To get a list and description of accepted adjustment methods type `? adjusted` in your console.

### Making the output easier to read

One thing you may have noticed above is that your contrasts aren't labelled very transparently. Looking at the output you would have to remember what was being contrasted in `contrast1` and `contrast2`. You can save yourself a little headache if you label your contrasts while constructing the matrix. For example, here I'm performing the linear contrasts from the Howell text:
```{r}
contrast1 <- c(0,-1,0,0,1)
contrast2 <- c(-1,0,1,0,0)
contrast3 <- c(0,1,-1,0,0)
contrast4 <- c(-1/3,-1/3,-1/3,1/2,1/2)
```

From here you can modify the previous code to assign names to the rows in your contrast matrix:

```{r}
contrast.matrix <- rbind("MM v. McM" = contrast1,
                    "MS v. SS" = contrast2,
                    "MM v. SS" = contrast3,
                    "MS+MM+SS v. SM+McM" = contrast4
                    )
```

And now running these contrasts (this time using a Holm adjustment):

```{r}
lm.model <- lm(formula = Time~Group,data = dataset)
contrast.model <- multcomp::glht(lm.model, linfct = mcp(Group = contrast.matrix))
# where Group is my IV, and contrast1 are my planned contrasts from above
summary(contrast.model, test = adjusted('holm'))
```
For practice, try re-running this last analysis but using the Bonferroni adjustment instead of Holm. Note how that changes the p-values for some of your contrasts.

### Calculating your effect size

Typically when reporting the effect size off the difference between two means we use Cohen's D. However, calculating Cohen's D in a planned contrast is slighly more involved than the method used for a regular t-test. This is because with a regular t-test you only have 2 means from 2 samples that you have collected. In the case of Planned Contrasts in ANOVA, while you are only comparing two means, those means are nested within a larger group (e.g., comparing MS and MM, we still need to account for the fact that we also collected samples from SS, SM, and McM) or may be derived from multiple samples (e.g., contrasting the mean of MS + MM against the mean of SS + SM + McM). Simply put, in our calculations we need to account for the influence of all of our collected groups. This is done by placing the contrasted difference in the context of the Root Mean Square Error, or the square root of the Mean Square Error of the residuals in our ANOVA model. To do this we need two things:

- a vector containing our contrasts— e.g., `c(-1/2,-1/2,1/3,1/3,1/3)`
- the mean of each group
- the Mean Square Error of the residuals from `anova()`

So to calculate Cohen's d for the contrast of our first two groups (MS + SM) against our last three groups (SS + SM + McM):

```{r}
# contrast vector:
# MS + MM + SS v. SM + McM
contrast_vector <- c(-1/3, -1/3, -1/3, 1/2, 1/2)

# using by() to create a vactor of means
means <- by(data = dataset$Time,INDICES = dataset$Group,FUN = mean)

# running an anova() of the omnibus model
aov.table <- lm.model%>%anova()

# get the sqrt of the `Mean Sq` of residuals
RMSE <- sqrt(aov.table['Residuals','Mean Sq'])

# calculate d from formula see Howell, Ch. 12
d <- sum(contrast_vector*means)/RMSE

print(d)
```

Easy as that. You'll notice above that I pulled out the `Mean Sq` of the `Residuals` from my `aov.table` using the indexing method instead of the names method. In the past we've covered how to call rows and columns by their index numbers. If the rows and columns also have names assigned to them you can use those as well.

To see the associated names, try:

```{r}
rownames(aov.table)
colnames(aov.table)
```

In this case I prefer this method as it always ensures that I get the correct value. In contrast, depending on the number of factors the Residuals may be on the 2nd row (as in this case), 3rd, 4th, or 5th row of the `Mean Sq` column.

## Reporting your results

When reporting your results, we typically report:

- the omnibus ANOVA
- a statement on what multiple comparisons were run and how corrected
- note which comparisons were significant.

Since our comparisons, if corrected, rely on adjusted p.values its kosher to simply state that $p<\alpha$. *If you elect to use the actual p values, then you need to note that they are corrected.*

So for post hocs, something analogous to:

> ...To test our hypothesis, we conducted a One-way ANOVA assessing latency as a function of morphine group. This ANOVA was significant—$F(4,35)=27.33,p<.001,\eta_p^2=.75$. As can be seen in Figure 1, pairwise Tukey HSD tests revealed that SS, MM, and MS groups were not significantly different from one another, but were significantly less than McM and SM (p<.05). 

For planned comparisons, we might say:

> Our prevailing hypothesis predicted that response latency for groups SS, MM, and MS would be significantly less than groups SM and McM. To test this hypothesis we performed a planned contrast (no corrections). Our results revealed  statistically significant difference between these groups, *t*(35) = 9.95, *p* < .001, *d* = 3.21, where the responce latency in MS, MM, and SS was lower than SM and McM.

Note that the *df* in the t-test are the error *df* from the omnibus model (35). 


## A note on using the `multcomp` package v. using the base stats method

You may have noticed as you were moving through the Field text that he recommends a different method for constructing contrasts. The base method that he uses has you apply the contrast matrix directly to the factor, then re-run the ANOVA in `lm()` as opposed to the method I've outline above. Of course, Field's is appropriate and conceptually it may give you a better understanding of what is going on.

In what follows I briefly walk through othe considerations if you use the `stats::contrasts()` method. For most of you, you can simply stop right here and things will be fine. For those interested in an alternative, press on...


There is one crucial factor that you must consider when using the `stats::contrasts()` method—your number of contrasts cannot be larger that the number of IV levels minus 1. If you have too many contrasts (can only happen if using non-orthogonal contrasts) then you have to go through several steps to derive the inverse of the matrix using the Moore-Penrose inversion method, which depending on how constructed may still not work if the determinant of your matrix = 0. In other words, you don't want to do that!


`R` automatically defaults to creating (number of IV levels) minus 1 contrasts. So if you intend to run fewer, then you need to be wary that you can only trust the contrasts that you specified. To see what I'm getting at, let's use the `stats::contrasts()` method (no need to load `stats`, it's already loaded when you start R). To demonstrate the issue at hand let's start with only one contrast. But first let's run a regular ANOVA

```{r}
# creating lm.model for comparison:
lm.model <- lm(Time~Group, data = dataset)
summary(lm.model)
```

And now let's re-run after applying our contrasts to `Group`:
```{r}
# create contrast
contrast1 <- c(0,-1,0,0,1) # MM v McM

#assign that contrast to our IV:
contrasts(dataset$Group) <- contrast1

#re-running lm model after assigning contrasts:
contrast.model <- lm(Time~Group, data = dataset)
summary(contrast.model)
```

When comparing or original `lm.model` to the model run after applying the contrasts we see that the latter gives us a different result—the contrasts that we requested. Actually, as you can see it give us more that we requested. We only requested 1 contrast, it supplies us with 4. To see what has happened here let's take a look at what contrasts we indeed applied to `Group`:

```{r}
# setting contrasts
contrasts(dataset$Group) <- contrast1

# looking at the set contrast matrix
attributes(dataset$Group)
```

Column 1 contains the contrast that we requested. Columns 2-4 contain non-orthogonal contrasts that R has generated. So, when reading the resulting `lm()` output, you should only focus on the first contrast that is reported. 

<br><br><br>


<!--chapter:end:10week.Rmd-->

# Analysis of Varience III: Factorial ANOVA

In this week's vignette we are simply building upon the previous two weeks coverage of One-way ANOVA and multiple comparisons. I'm assuming you've taken a look at all of the assigned material related to these topics. This week we up-the ante by introducing more complex ANOVA models, aka factorial design. As we discussed in lecture, a factorial ANOVA design is required (well, for the purposes of this course) when your experimental design has more than one IV. Our examples this week focus on situations involving two IVs, however, what is said here applies for more complex designs involving 3, 4, 5, or however many IV's you want to consider. Well, maybe not however many... as we we'll see this week and the next, the more IVs you include in your analysis, the more difficult interpreting your results becomes. This is especially true if you have interaction effects running all over the place. But perhaps I'm getting a little bit ahead of myself. Let's just way I wouldn't recommend including more than 3 or 4 IVs in your ANOVA at a single time and for now leave it at that.

Note that this week's vignette assumes you have the following packages:

```{r}
pacman::p_load(afex, # a new way to do ANOVA
               emmeans, # a different way to do contrasts
               multcomp,
               ez, # get info on design of ANOVA, also can run ezANOVA
               Rmisc, # getting summary data
               cowplot, 
               tidyverse,
               psych)
```

## Main effect, main effect, and interactions... oh my!

When we are performing a factorial ANOVA we are performing a series of independent comparisons of means as a function of our IVs (this assumption of independence is one of the reasons that we don't typically concern ourselves with adjusting our p-values in the omnibus factorial ANOVA). For any given number of IVs, or **factors**, we test for a **main effect** of that factor on the data—that is "do means grouped by levels within that factor differ from one another **not taking into consideration the influence of any of the other IVs**. Our tests for interactions **do** consider the possibility that our factors influence one another—that is, "do the differences that are observed in one factor depend on the intersecting level of another?"

For the sake of simplicity, we will start with a 2 × 2 ANOVA and work our way up by extending the data set. Given our naming conventions, saying that we have a 2 × 2 ANOVA indicates that there are 2 IVs and each has 2 levels. A 2 × 3 ANOVA indicates that there are 2 IVs, and that one IV has 2 levels and the other has 3 levels; a 2 × 3 × 4 ANOVA indicates that we have 3 IVs, the first has 2 levels, the second has 3 levels, and the third has 4 levels. 

Our example ANOVA comes from Howell (13.5), testing the effects of smoking on performance in different types of *putatively* (I'm showing my biases here) information processing tasks. There were 3 types of cognitive tasks: the first, a pattern recognition task where participants had to locate a target on a screen; the second, a cognitive task where participants had to read a passage and recall bits of information from that passage later; and the third, participants performed a driving simulation. Three groups of smokers were recruited— those that were actively smoking prior to and during the experiment; those that were smokers, but did not smoke 3 hours prior to the experiment; and finally non-smokers. As this is a between design, each participants only completed one of the cognitive tasks.

## Example: a 2×2 ANOVA

Let's grab the data from Howell's website. Note that for now we are going to ignore the covar column:
```{r}
dataset <- read_delim("https://www.uvm.edu/~dhowell/methods8/DataFiles/Sec13-5.dat", "\t", escape_double = FALSE, trim_ws = TRUE)

dataset$Task <- recode_factor(dataset$Task, "1" = "Pattern Recognition", "2" = "Cognitive", "3" = "Driving Simulation") 

dataset$Smkgrp <- recode_factor(dataset$Smkgrp, "1" = "Nonsmoking", "2" = "Delayed", "3" = "Active")
```

To get a quick view of our data structure we can use two kinds of calls:

`summary()` provides us with info related to each column in the data frame. If a column contains a factor it provides frquency counts of each level. It the column is numeric it provides summary stats:


```{r}
summary(dataset)
```

In addition, I like to use the `ezDesign()` function from the `ez` package to get a feel for counts in each cell. This is useful for identifying conditions that may have missing data.

```{r}
ez::ezDesign(data = dataset, x=Task,y=Smkgrp,row = NULL,col = NULL)
```

This provides me with a graphic representation of cell counts. In this case, every condition (cell) has 15 participants. As you can see right now this is a 3 x 3 ANOVA.

To start, let's imagine that we are only comparing the active smokers to the nonsmokers, and that we are only concerned with the pattern recognition v driving simulation. In this circumstance we are running a 2 (smoking group: active v. passive) × 2 (task: pattern recognition v. driving simulation) ANOVA. We can do a quick subsetting of this data using the `filter()` command. For our sake, let's create a new object with this data, `dataset_2by2`:

```{r}
# subsetting the data. Remember that "!=" means "does not equal"; "&" suggests that both cases must be met, so
dataset_2by2 <- filter(dataset, Smkgrp!="Delayed" & Task!="Cognitive")
```


To get a quick impression of what this dataset looks like, we can use the `summary()` function, or `ezDesign()`:

```{r}
# getting a summary of dataset_2by2:
summary(dataset_2by2)
```


You may notice from the summary above that the groups that were dropped "Delayed" smokers and "Cognitive" task still show up in the `summary`, albeit now with 0 instances (you'll also notice that the remaining groups decreased in number, can you figure out why?). In most cases, `R` notices this an will automatically drop these factors in our subsequent analyses. However, if needed (i.e. it's causing errors), these factors can be dropped by invoking the `droplevels() `function like so:

```{r}
dataset_2by2$Smkgrp <- droplevels(dataset_2by2$Smkgrp)
dataset_2by2$Task <- droplevels(dataset_2by2$Task)
summary(dataset_2by2)
```

And to see the cell counts:
```{r}
ez::ezDesign(data = dataset_2by2, x=Task,y=Smkgrp,row = NULL,col = NULL)
```

## Making sense of plots

Let's go ahead and plot the data using a line plot with 95% CI error bars. Note that these plots (up until the last section) are not APA-complete!!!!

Since, I'm going to examples of several different plots, I'm going to create a general canvas `p` and build from that.
```{r}
p <- ggplot2::ggplot(data = dataset_2by2, mapping=aes(x=Smkgrp,y=score,group=Task))
```


### Interaction plots
Interaction plots take into consideration the influence of each of the IVs on one another—in this case the mean and CI of each smoking group (Active v. Nonsmoking) as a function of Task (Driving Simulation v. Pattern Recognition). For example, a line plot might look like this:

```{r}
line_p <- p + 
  stat_summary(geom="pointrange",fun.data = "mean_cl_normal", position=position_dodge(.5)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(.5), aes(linetype=Task)) +
  theme_cowplot()

show(line_p)
```


A brief inspection of the plot can be quite informative. Let's start with the interaction, in fact: **you should always start with the interaction**. Since this is an "interaction" plot, often a quick visual inspection will allow us to predict whether our subsequent ANOVA will likely yield an interaction effect (it's good practice to plot *before* running your ANOVA). A simple rule of thumb is that if you see the lines converging or intersecting then more than likely an interaction is present (whether its significant is another question). You might think that this rule of thumb is useful if you use a line plot, and well, you'd be right. What about a bar plot or box plot you ask?

```{r}
bar_p <- p + 
    stat_summary(geom="errorbar", width=.3, fun.data = "mean_cl_normal", position = position_dodge(.9)) + 
  stat_summary(geom = "bar",fun.y = "mean",color="black", aes(fill=Task), position=position_dodge(.9)) +
  theme(legend.position="none") +
  scale_fill_manual(values = c("light grey", "white"))

show(bar_p)
```

```{r}
box_mean <- function(x) {
  r <- quantile(x, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
  # replace median with mean
  r[3] <- mean(x)
  names(r) <- c("ymin", "lower", "middle", "upper", "ymax")
  r
}

# Add points outside of whiskers
box_out <- function(x) {
  subset(x, x < quantile(x,probs=0.025) | quantile(x,probs=0.975) < x)
}

box_p <- p +
  stat_summary(fun.data = box_mean, size=.5, color="black", geom="boxplot", position=position_dodge(.9), aes(fill=Task)) +
  theme(legend.position="none") + 
  scale_fill_manual(values = c("light grey", "white"))
```

```{r, fig.width=8}
plot_grid(bar_p,box_p)
```

You should note that in my boxplots what is typically the median line is now represents the means (this was accomplished by the `box_mean()` function that I custom wrote in the chunk above. Note that the grey fills above are the "Driving Simulation" group and the white are the "Pattern Recognition". In both cases just take a look at the means that are grouped together. If the relative difference between grouped means changes as you move from one category on the x axis to the next, you likely have an interaction. Note that this is a general rule of thumb and applies to the line plots as well (the reason that the lines intersect is because of these sorts of changes). In this case, the means on the "Active" grouping are nearly identical, while the means in the "Nonsmoking" grouping are much further apart. So we likely have interaction.

### Plotting main effects

If we wanted we could also create separate plots related to our mean effects. These plots would look something like this:

```{r generate main effects plots, fig.width=8}
Smoke_p <- ggplot2::ggplot(data = dataset_2by2, mapping=aes(x=Smkgrp,y=score, group=1)) + 
  
  stat_summary(geom="pointrange",fun.data = "mean_cl_normal", position=position_dodge(0)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(0)) + 
  coord_cartesian(ylim=c(4,12)) +
  theme_cowplot()

Task_p <- ggplot2::ggplot(data = dataset_2by2, mapping=aes(x=Task,y=score, group=1)) + 
  
  stat_summary(geom="pointrange",fun.data = "mean_cl_normal", position=position_dodge(0)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(0)) + 
  coord_cartesian(ylim=c(4,12)) +
  theme_cowplot()

plot_grid(Smoke_p,Task_p)
```

and would take a look at changes due to each IV without considering the other. Here we might infer that there is a main effect for both of our IVs. That said, the interaction plot is useful as well in assessing main effects as well:

```{r}
show(line_p)
```

Here, to infer whether there might be main effects we can imagine where the means would be if we collapsed our grouped plots (this is exactly what the main effects take a look at). To help with your imagination I'm going to plot our main effect means on the interaction plot. Here the grey-filled triangles represent the the collapsed Smoking  Group means indifferent to task. To get these, just imagine finding the midpoint of the two circles in each level of Smoking group. The slope suggests the possibility a main effect.

```{r}
line_p + 
  stat_summary(aes(x=Smkgrp, y=score, group=1), geom = "point", fun.y = "mean", color="dark grey", size=3, shape=17) +
  stat_summary(aes(x=Smkgrp, y=score, group=1), geom = "line", fun.y = "mean", color="dark grey", size=1)
```

To imagine the collapsed Task means, we can just find the y-values that intersect with the midpoints of each line (note the red line is the mean value of the Driving Simulation group):

```{r}
line_p +
  geom_hline(yintercept = 9.6667, color="red") + 
  geom_hline(yintercept = 6.1333, color="blue")
```

The difference in y-intercepts suggests the possibility of a main effect.

So in summary, I'm guessing from plot I've got 2 main effects and an interaction. Let's test this.


## Running the ANOVA `lm()` method

Now we can run the using the `lm()` method as we have previously done with the One-way ANOVA. The new wrinkle is simply adding our additional IV and interaction terms the formula equation the structural equation:

$$y=IV_1+IV_2+(IV_1*IV_2)$$
where the first and second terms capture our main effects and the third is our interaction. Using our data in `R` this formula becomes:

```{r eval=FALSE}
lm(score~Smkgrp+Task+Smkgrp:Task,data = dataset_2by2) %>% anova()
```
The operator `:` may be understood here as "interaction between". Note that we can write this same equation in shorthand:
```{r eval=FALSE}
lm(score~Smkgrp*Task,data = dataset_2by2) %>% anova()
```
where the `*` operator tells `R` to examine **all** related main effects and interactions. This is usually the best way to code this, as the long-form becomes unweildy as we increase IVs and combinations of interactions. However, note that if you have planned comparisons you may elect to only specify certain main effects and interactions in log-form.

OK, running the above gives us:
```{r echo=FALSE}
lm(score~Smkgrp*Task,data = dataset_2by2) %>% anova()
```

One thing to note here are that the apparent equal F-values in our Task effect and Interaction is due to rounding. This is simply an anomaly, and is not to be expected in most cases. Even here, they are different, but only nearly identical. As a thought experiment, can we think of why they are nearly identical in this case (Hint: can you see what might be causing the interaction)?

## Running the ANOVA in the `afex::aov_ez()` method:

We can also use the `aov_ez` method from the `afex` library. In fact, this might be preferable, as the output gives you most everything you need to report your ANOVA, and in the future this method makes the necessary corrections for you if your data violates certain assumptions needed for ANOVA. While I have been stressing the `lm()` method to re-enforce that ANOVA and linear regression are two sides of the same coin, when performing ANOVA I typically use `afex::aov_ez()` unless I am planning some very complex a priori comparisons in my model (this is rarely the case). With `aov_ez()` we can easily specify the ANOVA model by inputting our parameters into the function. Please note which need to be input as strings (i.e., have quoations around them), and which are input as objects or numbers.

One important note before continuing is that this method requires that you have a column that specifies participant number, here I'll use `PartID`. Since we are running a purely between-design, we need to assign each participant a unique number. If `R` finds instances where `PartID` are the same it will assume that data comes from the same participant (i.e., we have a within-subjects or repeated measures design). For our purposes now, this is easily solved by simply creating a column that runs from 1 to the number of observations in our data. After that we can proceed with the ANOVA:

```{r}
# create a PartID column and number
dataset_2by2$PartID <- 1:nrow(dataset_2by2)

# run the ANOVA
afex::aov_ez(id = "PartID",dv = "score",data = dataset_2by2,between = c("Smkgrp","Task"),within = NULL,covariate = NULL,observed = NULL,fun_aggregate = NULL,type = 3,factorize = TRUE,check_contrasts = TRUE,return = "afex_aov",anova_table = list(es = "pes"))

```
Note that above I specified every argument in the function. This was not necessary (indeed if you use Rstudio to help you fill this in, you'll notice that many of the defaults are NULL). A complete write-up of what is going on here may be found by `?afex::aov_ez`.

For now I want to comment on a few of the choices I made. For `type` I selected Type III sum of squares. This is typically the type that we will choose in ANOVA, however see Field, Ch 11, "Jane Superbrain 11.1" for an excellent overview of the different Types of Sum of Squares and when to use different values. Also you'll notice I made a few adjustments in my `anova_table` output via `list()`. These were to get the table to output partial eta-squared `es="pes"` for my effect size.

## Getting cell means

Up until this point we've used `psych::describeBy()` to generate summary stats. 
This becomes a little more difficult as the designs become more complex. Instead I recommend `summarySE()` from the `Rmisc` package:

```{r}
Rmisc::summarySE(data = dataset_2by2,measurevar = "score",groupvars = c("Task","Smkgrp"))
```

We can also use this function to report the means related to the main effects (irrespective of interaction). 

For example, Smoking effect I can re-write the call above, simply dropping `Task` from `groupvars`:
```{r}
Rmisc::summarySE(data = dataset_2by2,measurevar = "score",groupvars = "Smkgrp")
```

and for the Task effect:
```{r}
Rmisc::summarySE(data = dataset_2by2,measurevar = "score",groupvars = "Task")
```

Note that If you are reporting means related to the main effects, you need to report these marginal means!

## More examples: a 2 × 3 ANOVA

In the example above we focused in the 2 × 2 scenario for ease, however, remember that our original `dataset` was a 2 (Smoking group) by 3 (Task) design.

### Running the ANOVA

I'm going to use `afex::aov_ez()`. Also, I'll need to add a participant identification number column to the original `dataset`

```{r}
# create a PartID column and number
dataset$PartID <- 1:nrow(dataset)

# run the ANOVA
afex::aov_ez(id = "PartID",dv = "score",data = dataset,between = c("Smkgrp","Task"),type = 3, return = "afex_aov",anova_table = list(es = "pes"))
```

### APA plotting

For the plot I'm going to give examples of presenting the data in line plot, bar plot, and box plot form. Which you choose depends on what you want to convey, and how complex the plot becomes. Note that in addition to using `theme_cowplot()` which you are familiar with, I'm also including several arguements in `theme()` which allow for greater customization. These include arguments to change the fontface and size of legend and axes titles, as well as re-positioning the legend. While the former may not be needed for APA (but might be aesthetically useful) not that APA format has strict guidelines pertaining to legend position.

That said, all we are doing is extending the plotting methods that you have been using for the past few weeks. The important addition here is the addition of `group=` in the first line the `ggplot`. For example:

```{r eval=FALSE}
p <- ggplot2::ggplot(data = dataset, mapping=aes(x=Smkgrp,y=score,group=Task))
```

indicates that we are:
- using the `dataset` data set
- putting first IV, `Smkgrp`, on the x-axis
- putting our dv, `score` on the y-axis
- and grouping our data by our other IV, `Task`

This last bit is important as it makes clear that the resulting mean plots should be of the cell means related to `Smkgrp` x `Task` 


#### Line plot with 95% CI bars

```{r}
# setting original parameters
p <- ggplot2::ggplot(data = dataset, mapping=aes(x=Smkgrp,y=score,group=Task))

# making a basic line plot
line_p <- p + 
  # add pointranges, diferent Task by shape:
  stat_summary(geom="pointrange",fun.data = "mean_cl_normal", size=0.75, position=position_dodge(.25), aes(shape=Task)) + 
  # add lines, different Task by linetype (solid v dashed)
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(.25), aes(linetype=Task))

# adding APA elements and other aesthetics
line_p <- line_p + theme_cowplot() + 
  theme(
    axis.title = element_text(size = 16, face = "bold"), # make axes larger and bold
    axis.text = element_text(size = 12), # specify text size
    legend.title = element_text(size = 12, face = "bold"), # make lengend title bold
    legend.position = c(.1,.9) # change the legend position
    ) +
  xlab("Smoking Group") + 
  ylab ("Performance score") +
  # optional... create additional white space around the plot
  theme(plot.margin=unit(c(.25,.25,.25,.25),"in"))

show(line_p)
```

A few notes:

- I elected to dodge the mean points. I find this to be clearer as I don't have to worry about my overlapping error bars (this would be true even if I have the caps on the bars as well). This was accoumplished by adding `position=position_dodge(.25)` to both my calls for `pointrange` and `line`.

- I also differentiated each task group by both line and shape. In reality this is redundant, but I feel that it makes the differences in groups easier to distinguish with a quick glance. Note that I could have also chosen color, but with print APA color should only be chosen when using 2 or three shades of grey, and typically as a last resort.

#### Box plot
Here, let's do the same with box plots:
```{r}
# setting original parameters
p <- ggplot2::ggplot(data = dataset, mapping=aes(x=Smkgrp,y=score,group=Task))

# note that I'm using the functions I've created in the previous boxplot example
box_p <- p +
  # make boxplots, set their width to half of the default
  stat_summary(fun.data = box_mean, color="black", geom="boxplot", position="dodge", aes(fill=Task, width=0.5))

# APA-ify
box_p <- box_p + theme_cowplot() + 
  theme(
    axis.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12, face = "bold"),
    legend.position = c(.1,.9)) +
  # used to specify the exact colors that I want, 
  # two shades of grey and white
  scale_fill_manual(values = c("grey50","grey75","white")) +
  xlab("Smoking Group") + 
  ylab ("Performance score") +
  theme(plot.margin=unit(c(.25,.25,.25,.25),"in"))

show(box_p)
```

A few notes:

- I decreased the widths of my box plots as otherwise the box plot may feel a little cramped. This was accomplished in the `aes()` call when I constructed the base box plot. (I changed width to 0.5)
- I also specified the fill-in color for each boxplot using `scale_fill_manual(values = c("grey50","grey75","white"))`

#### Bar plot

And finally a bar plot. Remember when making a bar plot, we need to correct for the gap below zero on the y-axis:

```{r}
# setting original parameters
p <- ggplot2::ggplot(data = dataset, mapping=aes(x=Smkgrp,y=score,group=Task))

# constructing the bar plot:
bar_p <- p + 
stat_summary(geom="errorbar", width=.3, fun.data = "mean_cl_normal", position = position_dodge(.9)) + 
  stat_summary(geom = "bar",fun.y = "mean",color="black", aes(fill=Task), position=position_dodge(.9))

# adding APA:
bar_p <- bar_p + theme_cowplot() + 
  theme(
    axis.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12, face = "bold"),
    legend.position = c(.1,.9)) +
  scale_fill_manual(values=c("black","grey50","white")) +
  xlab("Smoking Group") + 
  ylab ("Performance score") +
  theme(plot.margin=unit(c(.25,.25,.25,.25),"in")) +
  # fixing y-axis:
  scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,25))

show(bar_p)
```

#### Cell means
```{r}
Rmisc::summarySE(data = dataset,measurevar = "score",groupvars = c("Task","Smkgrp"))
```

#### Main effect means:
```{r}
Rmisc::summarySE(data = dataset,measurevar = "score",groupvars = c("Task"))
Rmisc::summarySE(data = dataset,measurevar = "score",groupvars = c("Smkgrp"))
```

## 2 × 5 ANOVA (Howell 13.1)

For another example, we can return to the Eysenck data from Howell, Ch 13.1:

```{r}
dataset <- read_delim("https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab13-2.dat", "\t", escape_double = FALSE, trim_ws = TRUE)

# label dummy codes:
dataset$Age <-recode_factor(dataset$Age,"1"="Old", "2"="Young")

dataset$Condition <- recode_factor(dataset$Condition, "1"="Counting","2"="Rhyming","3"="Adjective","4"="Imagery","5"="Intentional")

# re-leveling:
dataset$Condition <- factor(dataset$Condition,levels = c("Counting","Rhyming","Adjective","Imagery","Intentional"))

summary(dataset)
```


### Running the ANOVA
```{r}
# create a PartID column and number
dataset$PartID <- 1:nrow(dataset)

# run the ANOVA
afex::aov_ez(id = "PartID",dv = "Recall",data = dataset,between = c("Age","Condition"),type = 3, return = "afex_aov",anova_table = list(es = "pes"))
```


#### Line plot with SE bars

```{r}
# setting original parameters
p <- ggplot2::ggplot(data = dataset, mapping=aes(x=Condition,y=Recall,group=Age))

# making a basic line plot
line_p <- p + stat_summary(geom="pointrange",fun.data = "mean_se", size=0.75, position=position_dodge(.25), aes(shape=Age)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(.25), aes(linetype=Age))

# adding APA elements
line_p <- line_p + theme_cowplot() + theme(
    axis.title = element_text(size = 16, face = "bold", lineheight = .55),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12, face = "bold"),
    legend.position = c(.2,.75)) +
  xlab("Smoking Group") + 
  ylab ("Performance score") +
  theme(plot.margin=unit(c(.25,.25,.25,.25),"in"))

show(line_p)
```

We could elect to create bar plots and box plots here as well. One thing to keep in mind is that these plots become more and more cramped as you consider additional IVs and levels as in higher order designs. Anything above a 3 x 3, and I default to a line plot. At the same time, lineplots can become messy. A good rule of thumb is to place the IV with the greater number of levels on the x-axis and differentiate the other(s) by lines or shapes.

For example, assuming we don't follow this rule of thumb and place `Age` on the x-axis and `Smoking` by line type:

```{r}
# setting original parameters
p <- ggplot2::ggplot(data = dataset, mapping=aes(x=Age,y=Recall,group=Condition))

# making a basic line plot
line_p <- p + stat_summary(geom="pointrange",fun.data = "mean_se", size=0.75, position=position_dodge(.25), aes(shape=Condition)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(.25), aes(linetype=Condition))

# adding APA elements
line_p <- line_p + theme_cowplot() + theme(
    axis.title = element_text(size = 16, face = "bold", lineheight = .55),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12, face = "bold"),
    legend.position = c(0,.75)) +
  xlab("Smoking Group") + 
  ylab ("Performance score") +
  theme(plot.margin=unit(c(.5,.5,.5,.5),"in"))

show(line_p)
```

This becomes more difficult to interpret.

## One last example, 

```{r}
dataset_no_inter <- read_delim("https://raw.githubusercontent.com/tehjespah/tehjespah.github.io/master/teaching/PSYC7014/datasets/two_factor_ANOVA_no_interaction", delim="\t")


dataset_no_inter$Lecture <- recode_factor(dataset_no_inter$Lecture, "1"="Phys","2"="Soc","3"="Hist")

dataset_no_inter$Presentation <- recode_factor(dataset_no_inter$Presentation, "1"="Comp","2"="Stand")

summary(dataset_no_inter)
```

### Running the ANOVA

I'm going to use `afex::aov_ez()`:

```{r}
# create a PartID column and number
dataset_no_inter$PartID <- 1:nrow(dataset_no_inter)

# run the ANOVA
afex::aov_ez(id = "PartID",dv = "Score",data = dataset_no_inter,between = c("Lecture","Presentation"),type = 3, return = "afex_aov",anova_table = list(es = "pes"))
```

### Interaction plot

Here we have a case where there is no interaction. Plotting this:

```{r}
# setting original parameters
p <- ggplot2::ggplot(data = dataset_no_inter, mapping=aes(x=Lecture,y=Score,group=Presentation))

# making a basic line plot
line_p <- p + stat_summary(geom="pointrange",fun.data = "mean_se", size=0.75, position=position_dodge(.25), aes(shape=Presentation)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(.25), aes(linetype=Presentation))

# adding APA elements
line_p <- line_p + theme(
    axis.title = element_text(size = 16, face = "bold", lineheight = .55),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12, face = "bold"),
    legend.position = c(.75,.9)) +
  xlab("Lecture type") + 
  ylab ("Performance score") +
  theme(plot.margin=unit(c(.25,.25,.25,.25),"in"))

show(line_p)
```

## Interaction v. no interaction
Why is the fact that there is no interaction here important? Of all of the examples that we've covered to this point, the last example was the only example that we can trust our main effects at face value. This re-enforces the point I made earlier: **Look at your interaction FIRST!!!** Consider what the interaction means— that your observed main effects are contingent on certain conditions. If you have a significant interaction, you must run **simple effects** analysis to elucidate these contingencies. For example, looking at our very first example:

```{r}
p <- ggplot2::ggplot(data = dataset_2by2, mapping=aes(x=Smkgrp,y=score,group=Task))

interaction_p <- p + stat_summary(geom="pointrange",fun.data = "mean_cl_normal", position=position_dodge(.25)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(.25), aes(linetype=Task)) +
  theme_cowplot()

show(interaction_p)
```

```{r}
# create a PartID column and number
dataset_2by2$PartID <- 1:nrow(dataset_2by2)

# run the ANOVA
afex::aov_ez(id = "PartID",dv = "score",data = dataset_2by2,between = c("Smkgrp","Task"),within = NULL,covariate = NULL,observed = NULL,fun_aggregate = NULL,type = 3,factorize = TRUE,check_contrasts = TRUE,return = "afex_aov",anova_table = list(es = "pes"))

```

The ANOVA reveals a main effect for smoking group. However, looking at the plot, we know that this is not the entire story. That is, the means change from one smoking group to another for the Driving simulation task, but not for the Pattern recognition group. Our main effect for Smoking group **is contingent** on task. This is what I mean by "you can't trust your main effects when you have an interaction"—they don't tell the entire story.

### No interaction? How about some posthocs?

If you don't have an interaction, you may simply proceed to run post-hoc analyses on any significant main effects in the manner you would with a One-way ANOVA. Easy, peasy, right. One thing to note, you need to make the appropriate multiple comparison corrections. The easy way to do this is to perform a Bonferroni correction on the number of post-hoc comparisons that you intend. For example, returning to our data with no interaction:

```{r}
# create a PartID column and number
dataset_no_inter$PartID <- 1:nrow(dataset_no_inter)

# run the ANOVA
afex::aov_ez(id = "PartID",dv = "Score",data = dataset_no_inter,between = c("Lecture","Presentation"),type = 3, return = "afex_aov",anova_table = list(es = "pes"))
```

```{r}
# setting original parameters
p <- ggplot2::ggplot(data = dataset_no_inter, mapping=aes(x=Lecture,y=Score,group=Presentation))

# making a basic line plot
line_p <- p + stat_summary(geom="pointrange",fun.data = "mean_se", size=0.75, position=position_dodge(.25), aes(shape=Presentation)) + 
  stat_summary(geom = "line", fun.y = "mean", position=position_dodge(.25), aes(linetype=Presentation))

# adding APA elements
line_p <- line_p + theme_cowplot() + theme(
    axis.title = element_text(size = 16, face = "bold", lineheight = .55),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12, face = "bold"),
    legend.position = c(.75,.85)) +
  xlab("Lecture type") + 
  ylab ("Performance score") +
  theme(plot.margin=unit(c(.25,.25,.25,.25),"in"))

show(line_p)
```

We need to test for differences in both the `Lecture` and `Presentation` main effects.

- Presentation: This one is easy. We only have two levels of `Presentation`, so the omnibus `Ftest` tells us that our two groups are different. Nothing else to do here other than note which mean (Computer v. Standard) is greater than the other.
- Lecture: We have three levels of lecture, so were are going to need to run a post-hoc analysis. In this case, we may call upon our old standbys, Bonferroni, and Tukey. For example, a `pairwise.t.test()` with a "bonferroni" adjustment reveals:

```{r}
pairwise.t.test(dataset_no_inter$Score,dataset_no_inter$Lecture, p.adjust.method = "bonferroni")
```

You may now be confused that the post hoc for `Lecture` revealed no differences even though we had an overall main effect. Don't worry, these two do not contradict one another. The main effect is examining the overall trend in the data (think of the slope of the regression line) while the post-hoc pairwise comparisons are examining differences in means. In some cases the two are in agreement with one another. In others they don't match up. In this case you may simply say that "while the main effect for Lecture type indicated an overall trend that performance was different by lecture, pairwise comparisons revealed no significant differences between individual lecture types."

That said... remember that the Bonferroni adjustment is the most conservative of the three. Let's see what happends if we run a Tukey HSD. To run a Tukey you need the ANOVA model to be saved to an object. Here I'm saving it to the object `aov.model`

```{r}
aov.model <- afex::aov_ez(id = "PartID",dv = "Score",data = dataset_no_inter,between = c("Lecture","Presentation"),type = 3, return = "afex_aov",anova_table = list(es = "pes"))
```

From here you may call upon the `emmeans()` function to derive your posthocs. By itself, emmeans produces the comparision of differences between cell means. It takes the `afex_aov` model as a first arguments, and the IVs of interest as the second.

```{r}
# input your model into the emmeans,
# interested in Lecture

emmeans(aov.model,specs = "Lecture")
```

`emmeans()` allows another method for making contrasts (planned and posthoc). If you want to perform a Tukey test follow this procedure you can simply pipe the previous (or save to an object and submit) to `pairs()`

```{r}
emmeans(aov.model,specs = "Lecture") %>% pairs()
```

While this method doesn't provide the nice grouping plots from `agricolae` you can, if you choose, get the grouping letters  by adding `CLD()` to your pipe:

```{r}
emmeans(aov.model,specs = "Lecture") %>% pairs() %>% CLD()
```

It appears that our Tukey does reveal differences between means, when performance of those getting Psychical Science Lectures is greater than both History and Social, but Social and History are not difference from one another. 


### what to do if you DO have an interaction

Well, for this week, you stop. Next week we will see how to press on!

## What about planned contrasts?

You need to be careful when running planned contrasts in factorial ANOVA. In genral I would recommend only running planned contrasts on a single main effect, or a planned contrast on the effects of one of your factors at a single level of your other (though you still need to proceed with caution here).

For example, using the data from the last section, I would only run a planned contrast related to the main effect of `Lecture Type`, or a contrast of `Lecture Type` means only in `Computer presentation` conditions (or `Standard presentation`). DO NOT, I repeat DO NOT run contrasts that go across levels of your other factors. Well, truthfully, you can do whatever you want, but you may find that your ability to meaningfully interpret your results in such cases is extremely limited.

We can run planned contrasts using `emmeans()` as well. In this case, we need to specify the contrasts.

First we need to obtain the `emmeans()` of the model including all cells (all factors). Using `aov.model` from the previous example:
```{r}
emmeans(aov.model, specs = c("Lecture", "Presentation"))
```

OK. From here let's build two custom contrasts. First lecture contrast on the main effect. In this case let's assume I want to contrast `Phys` with the combined other two conditions. Using the output above, I identify which rows contain `Phys` and I ensure that the summation of those rows is 1. In this case there are two rows so each gets `0.5`. My remaining conditions must also equal `-1`. In this case there are four, so each is `-0.25`. Following the output above, then my contrast vector is: 

```{r}
main_eff_contrast <- c(.5,-.25,-.25,.5,-.25,-.25) %>% list()
```

You'll note that I piped my contrast into a list. This is required and a quirk of using `emmeans()`. From here I simply call `contrast()` contrast matrix as an argument. So the entire pipe goes from:

```{r}
emmeans(aov.model, specs = c("Lecture", "Presentation")) %>% contrast(.,main_eff_contrast)
```

Assuming that I want to run a similar contrast, but only on lectures done via computer, I would reaquaint myself with the cell means:

```{r}
emmeans(aov.model, specs = c("Lecture", "Presentation"))
```

And do my contrasts like so (see if you can follow the coding logic):

```{r}
# built my contrast
computer_only_contrast <- c(1,-.5,-.5, 0, 0, 0) %>% list()

# run the planned contrast
emmeans(aov.model, specs = c("Lecture", "Presentation")) %>% contrast(.,computer_only_contrast)
```

Assuming I wanted to perfom a set of orthoginal contrasts:

- 1. Phys v. Soc and Hist and
- 2. Soc v Hist

```{r}
# build the contrast matrix
contrast1 <- c(1, -.5, -.5, 0, 0, 0)
contrast2 <- c(0, -1, 1, 0, 0, 0)
contrast_matrix <- list(contrast1,contrast2)

# run the contrasts
emmeans(aov.model, specs = c("Lecture", "Presentation")) %>% contrast(.,contrast_matrix)
```




In both cases, my p-values are unadjusted. I can add an adjustment to the contrast() argument like so:
```{r}
emmeans(aov.model, specs = c("Lecture", "Presentation")) %>% contrast(.,contrast_matrix, adjust = "holm")

# or 
emmeans(aov.model, specs = c("Lecture", "Presentation")) %>% contrast(.,contrast_matrix, adjust = "bonferroni")
```

That's all for this week.

<!--chapter:end:11week.Rmd-->

---
title: "GGPlot Spectacular: Histograms"
author: "Tehran Davis"
date: "9/20/2018"
output: html_document
---
# (APPENDIX) Appendix {-}

# Plotting histograms and probability distributions

Hi all, a few of you have asked for something a little more concrete with respect to producing APA tables and figures in R. Before continuing on, let me say that the "Intro to tidyverse" and "Data Visualization with `ggplot`, pts 1 & 2" provide excellent tutorials on using `ggplot` to contruct a wide variety of figures. If you are having issues with the fundamentals of `ggplot` then I would suggest starting there (with the acknowledgement that Data Visualization with `ggplot`, pt 2 may be a little overkill for this course).

Here I'm going to walk you thru the construction of a histogram plots and lay out the logic of getting those plots in appropriate APA format. For the purposes of keeping things simple I'm going to focus on the types of data that we have encountered in class so far, namely frequency data and data with two means. As own designs and analyses become more complex, I will have sections at the end of each week that build upon what we do here (scatterplots, regression plots, interaction plots, oh my!)

There are a few packages that we will introduce in this walkthrough, including: `devtools`, `cowplot`, and `plotly`. I'll go into further detail what they can do for us as we move along, but for now let's just load in the one we are most familiar with:

```{r}
pacman::p_load(tidyverse)
               
```


For the purposes of this walk-through I will be using the `FacultyIncome.txt` dataset from Week 3's homework. If you want to follow along, you'll need to import that data (I've saved it to the object `FacultyIncome`)

```{r echo=FALSE}
FacultyIncome <- read.csv("~/Box/sharedCourses/PSYC7014/weeklySchedule/week3/homework/FacultyIncome.txt")
```

## The Basics

In general, the basic procedure for constructing a plot goes throu the following steps:

1. link to the data and tell `ggplot` how to structure its output
2. tell `ggplot` what kind of plot to make
3. adjust the axes and labels if necessary
4. adjust the appearance of the plot for APA (or other appropriate format)
5. add a legend if necessary
6. save or export the plot

We won't be doing much in terms of a legend today, but we will address legends as they become necessary (on the other side of the midterm).

### Step 1: Building the canvas

Here you need to be thinking about what form you want the plot to take. Key points:

1. `data =`: what data set you will be pulling from. This needs to be in the form of a `data_frame()`, with `names` in the headers. For most data that we will be working with from here on out. That will be the case, though for constructed / simulation data you may have to do this manually.

If you are unsure of the header names you can see them by:
```{r}
names(FacultyIncome)
```

Note that depending on what guide you follow you may also include `mapping=aes()` here as well. This would include telling `R` eventually what info goes on each axis, how data is grouped, etc. BUT this info can also be relayed in Step 2, and I think that conceptually it make more sense to put it there.

So for Step 1, just tell `ggplot` what data we are using and save our Step 1 to an object called `p`

```{r}
p <- ggplot(data = FacultyIncome)
show(p)
```

As you can see we've created a blank canvas—nothing else to see here.

### Step 2: Tell `ggplot` what kind of plot to make.

Plots can take several forms, or geoms (geometries), most common include:
+ histogram: `geom_histogram`
+ boxplot: `geom_boxplot`
+ scatter: `geom_point`
+ bar: `geom_bar`
+ line: `geom_line`

Here we are creating a histogram, so in Step 2 we add `geom_histogram()` to our original plot `p`. We also need to tell `ggplot` how to go about constructing our histogram. This info would be including in the `mapping = aes()` argument. This tells `ggplot` about the general layout of the plot, including:
  + `x`: what is on the x-axis
  + `y`: what's on the y-axis (usually your dv, although for histograms this ends up being frequency and `x` is your dv)
  + `group`: how should the data be grouped? This will become important for more complex designs.
  
What `aes()` options you choose in large part is determined by what kind of plot you intend to make. For our histogram we want bins of `Income` on the x-axis and frequency `..count..` on the y-axis.


```{r}
# Repeating previous step for clarity:
p <- p <- ggplot(data = FacultyIncome)
# new step:
p <- p + geom_histogram(mapping = aes(x=Income,y=..count..)) # take our original "p", add a geom, save the new plot to "p"
# show the result
show(p)
```

The default `geom_histogram()` produces the above plot. However, we can tweak several arguments in `geom_histogram()` to change the presentation of data, including:

+ `binwidth`: the width of each bin, or...
+ `bins`: the number of bins
+ `color`: the color of the perimeter of each bin/bar (note `color` refers to lines)
+ `fill`: the color of the bin/bar itself (note `fill` refers to spaces)
+ `mapping = aes()`: change the mapping (see below)

For example, toget this in APA format we would like light gray bars with black lines. Rather than the default 30 bins, we elect to use the rule of thumb $\sqrt{N}$.

```{r}
p <- p <- ggplot(data = FacultyIncome) #step 1

# before plotting get the get the number of observations
N <- nrow(FacultyIncome) 

#step 2:
p <- p + geom_histogram(mapping = aes(x=Income,y=..count..), 
                        fill="light gray",
                        color="black", 
                        bins = sqrt(N))
# show the result
show(p)
```

Not sure if increasing the number of bins here actually improves things (it's really a subjective choice), but let's stick with this.

You may elect to convey the information in probability rather than frequency count. To do this you can modify the `aes()` within  `geom_histogram`. For example, modifying the previous chunk:

```{r}
p <- p <- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1

N <- nrow(FacultyIncome) # get the number of observations

p <- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N
                        fill="light gray",
                        color="black", 
                        bins = sqrt(N))
# show the result
show(p)
```

### Step 3: Adjust the axes and labels

The plot at the end of Step 2 is almost there, but there are a few issues that remain. First, our axis labels could be more desciptive than "count/N" and "Income". This is solved by adding the arguments `xlab()` and `ylab()` to our plot. For example:

```{r}
# step 1:
p <- p <- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1

# step 2:
N <- nrow(FacultyIncome) # get the number of observations
p <- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N
                        fill="light gray",
                        color="black", 
                        bins = sqrt(N))
# step 3:
p <- p + xlab("FY2018 Salary") + ylab("Proportion of UC Professors")
show(p)

```

Another issue may be that gap that sits between the x-axis (x=0) and the axis scale. This can be remedied by adding the following to our plot `p`:

```{r}
p <- p + coord_cartesian(xlim=c(10000, 80000), ylim=c(0,.06), expand = FALSE)
```

The `coord_cartesian()` command allows us to zoom in or zoom out of our axes. There are several arguments that this command takes including:

+ `xlim=`: takes a pair of values `c(lower,upper)` for limits of x-axis
+ `ylim=`: same as above, but for y-axis
+ `expand=`: do you want to create additional whitespace between your data and axes?

For example if I wanted to only show Incomees with Proportions of UC Professors between `.05` and `.06` I can just zoom into my plot (with needing to go back and filter my original data)

```{r}
p <- p + coord_cartesian(xlim=c(15000, 80000), ylim=c(0.05,.06), expand = FALSE)
show(p)
```

Or if I instead wanted to only focus on those Profs making between 35K and 65K:

```{r}
p <- p + coord_cartesian(xlim=c(35000, 65000), ylim=c(0,.06), expand = FALSE)
show(p)
```

Not something I would in this case, but just an example. OK, let's get back to our working plot:

```{r}
# step 1:
p <- p <- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1

# step 2:
N <- nrow(FacultyIncome) # get the number of observations
p <- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N
                        fill="light gray",
                        color="black", 
                        bins = sqrt(N))
# step 3:
p <- p + xlab("FY2018 Salary") + 
  ylab("Proportion of UC Professors") +
  coord_cartesian(xlim=c(15000, 80000), ylim=c(0,.06), expand = FALSE)

show(p)
```

One last thing to consider regarding our axes are the location of labels on the scale. For example the y-axis has labels at every .01 and the x-axis at every 20K. What if we want to change the labels on the x-axis? Here we can add `scale_y_continuous` to our plot `p`. We need to tell `scale_y_continuous` what our desired sequence is. You've done sequences before, like the sequence from 0 to 10:

```{r}
0:10
```

However, if we want to say count by 2's the command is a little more involved
```{r}
seq(0,10,2)
```
where `seq(start, stop, by)`. So to start at 20K and stop at 70K going by 10K in our plot we add `breaks= seq(20000, 80000, 10000)`:

```{r}
p <- p <- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1

# step 2:
N <- nrow(FacultyIncome) # get the number of observations
p <- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N
                        fill="light gray",
                        color="black", 
                        bins = sqrt(N))
# step 3:
p <- p + xlab("FY2018 Salary") + 
  ylab("Proportion of UC Professors") +
  coord_cartesian(xlim=c(15000, 80000), ylim=c(0,.06), expand = FALSE) + 
  scale_x_continuous(breaks = seq(20000,70000,10000))

show(p)
```

### Step 4: Adjusting for APA
While the plot at the end of Step 3 is almost there, there are a few issues remaining, Including that grey grid in the background and the lack of axis lines. In the past these changes would need to be done line by line. But fortunately for us we are in the glorious present, and there is a library that does a lot of this cleaning for you. 

Introducting `cowplot`!!!!

```{r}
pacman::p_load(cowplot)
```

OK, we've loaded cowplot. Now what? Simply re-run the previous chunk, but this time adding `theme_cowplot()` to the end. Note that running `? theme_cowplot` shows that it includes arguments for font size, font family (type), and line size. Below I'm just going to ask for size "15" font. I typically leave the font type alone, but if I do change it I may occasionally use `font_family="Times"`:

```{r}
p <- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1

# step 2:
N <- nrow(FacultyIncome) # get the number of observations
p <- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N
                        fill="light gray",
                        color="black", 
                        bins = sqrt(N))
# step 3:
p <- p + xlab("FY2018 Salary") + 
  ylab("Proportion of UC Professors") +
  coord_cartesian(xlim=c(15000, 80000), ylim=c(0,.06), expand = FALSE) + 
  scale_x_continuous(breaks = seq(20000,70000,10000)) + 
  theme_cowplot(font_size = 15)

show(p)
```

Easy-peasy! You'll note that cowplot adjusted your fonts, fixed your axes, and removed that nasty background. Even more, once cowplot is loaded it doesn't need to be called to do this. It just sits in the background and automatically adjusts the format of any plot you make using `ggplot`. If you ever wanted to return to the default plot style you can run the following:

```{r}
theme_set(theme_gray())
```

But for now `cowplot` FTW!!! (other cool `cowplot` things will show up in the advanced section).

### Step 5: Add and adjust the legend
Doesn't make sense in this context so not going to spend a lot of time here, but more on this when it's relevent (see you in a few weeks)

### Step 6: Save the `ggplot`.

Within command-line, plots can be saved using the `ggsave()` function. Note that both `cowplot` and `ggplot` libraries have a `ggsave()` function. They for the most part do the same thing. If you've installed `cowplot`, your computer will just default to that. Lets get some help on `ggsave()` to see exactly what the parameters are:

```{r}
? ggsave()
```

from the result the important arguments are:

> filename = Filename of plot
plot = Plot to save, defaults to last plot displayed.
device = what kind of file, depending on extension.
path = Path to save plot to (defaults to project folder).
scale = Scaling factor.
width	= Width (defaults to the width of current plotting window).
height = Height (defaults to the height of current plotting window).
units	= Units for width and height (in, cm, or mm).
dpi	= DPI to use for raster graphics.

So to save the previous plot to my project folder, `p` as a .pdf to a file names "histogram.pdf" with the dimensions 8-in by 6-in, and a DPI of 300 (300 is usually the min you want for print)
```{r}
ggsave(filename = "histogram.pdf", plot = p,width = 8,height = 6,units = "in",dpi = 300)
```

if I wanted an image file like a .png, I just change the filename extension:
```{r}
ggsave(filename = "histogram.png", plot = p,width = 8,height = 6,units = "in",dpi = 300)
```

Note that you can also copy and save plots that are printed in your Notebook and Plots tab. In the notebook, simply right click on top of the plot for a quick Copy or Download. For plots printed to the Plots tab, click the Export button (this gives you all the options as `ggsave`).

## A note about your BONUS on Week 4.

In the example histogram we generated a frequency plot based on counts from our data. We converted it to probability data by dividing our counts by the total number of observations. If you were to run sufficiently large simulation of draws and follow the steps above) modifying as needed) you should be create something approximating what is asked for Week 4's bonus.

However, you can also produce a plot from the table you created using the `dbinom()` function by modifying the mapping of `y`. Here, I'm going to re-create the `probTable` of possible outcomes from Week 4.5:


```{r}
# 1. range of possibilites
numberHeads <- 0:10
# 2. prob of outcome
probHead <- dbinom(x = numberHeads,size = 10,prob = .65)
# 3. combine to data frame
probTable <- data_frame(numberHeads,probHead)
# 4. Show the data frame (table)
show(probTable)
```

So following my steps, *Step 1*:

```{r}
p <- ggplot(data = probTable)
names(probTable) # I want to see my header names for step 2
```

Step 2: Here, if I try to use `geom_histogram`, the software won't let me because the data that I have is not count data. What can I do? Creat a column plot instead. So substituting `geom_col`:

```{r}
# Step 2
p <- p + geom_col(mapping = aes(x=numberHeads,y=probHead)) 
# show the result
show(p)
```

From here you can follow Steps 3 and 4 to achieve your desired result.

## Advanced stuff

For other cool stuff that can be done with `cowplot()` including placing multiple plots side by side, check out the signette links below written by its author, [Claus O. Wilke](http://wilkelab.org/):

+ [Introduction to cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html)
+ [Changing the axis positions](https://cran.r-project.org/web/packages/cowplot/vignettes/axis_position.html)
+ [Plot annotations](https://cran.r-project.org/web/packages/cowplot/vignettes/plot_annotations.html)
+ [Arranging plots in a grid](https://cran.r-project.org/web/packages/cowplot/vignettes/plot_grid.html)
+ [Shared legends](https://cran.r-project.org/web/packages/cowplot/vignettes/shared_legends.html)

<!--chapter:end:A1_ggplot_hist.Rmd-->

