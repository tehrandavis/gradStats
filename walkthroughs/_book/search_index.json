[
["index.html", "musings of the Professor Weekly musings", " musings of the Professor Tehran J. Davis 2018-12-15 Weekly musings In these (roughly) weekly musings I will supplimenting the course texts and in-class examples with some written walk-thoughs. In previous years students have informed me that they got a lot out of these little jaunts. My hope is that they will be of some use to you. If you want to follow along you can copy and paste the relevant code chunks into your own Rmd file and execute. Note that in what follows code chunks and outputs are in blockquotes with the colored tint. Outputs that are generated by the code have hash marks in front of them. "],
["introduction-to-r-getting-to-know-your-data.html", "Week 1 Introduction to R / getting to know your data 1.1 Installing and loading an R package: 1.2 Loading in data 1.3 Looking ahead…", " Week 1 Introduction to R / getting to know your data This week we aim to suppliment the assigned readings with a few blurbs and examples in R. We will go over how to install packages and load data into the R environment, and will revisit our discussion of simple models with a walkthrough. Let’s go! Two of the first skills that you are going to need to learn are 1) how to install / load your required packages into the R environment and 2) how to import data from a file. More often than not, these will be the first two steps of your analysis (though if you are working on more complex projects you may find it easier to keep track of things if you load import your data and packages as needed). 1.1 Installing and loading an R package: On of the great things about R is its exensibility via packages. For other great things about R (or how to sell R to your peers) see this link on simplystatistics.org. In fact this blog is a great resource for staying in contact with topics and new developments in data analysis. From Peng’s article: “With over 10,000 packages on CRAN alone, there’s pretty much a package to do anything. More importantly, the people contributing those packages and the greater R community have expanded tremendously over time, bringing in new users and pushing R to be useful in more applications. Every year now there are probably hundreds if not thousands of meetups, conferences, seminars, and workshops all around the world, all related to R.” For example, in my own work I often find myself not only using statistical techniques such as growth curve modelling (package: lmer) but also advanced quantification techniques such as cross recurrence quantificantion analysis (package: crqa), MultiFractal Detrended Fluctuation Analysis (package: MFDFA), and Multivariate Sample Entropy Analysis. Ten years ago this involved 100’s of lines of custom programming to carry out these analysis… today, there’s an R package for that—i.e., someone else likely with far greater programming abilities than I has already done it. 1.1.1 Installing a package using the GUI For the point-and-click crowd, the easiest way to install an R package is to locate the Packages tab in your Window. From here you can click install. From there you can type in the package that you wish to install. So for example, let’s install the psych package, which is useful for obtaining descriptive stats from your data (although to be honest I hardly use it). Check out an example from this video on Box. As you see the video, I left the Install dependences box ticked. You should always install with dependencies, this automatically installs other packages that may be required for psych (or whatever package of interest) to work. You may have also noticed when after you clicked OK, R entered this line of code into your Console. install.packages(&quot;psych&quot;) Which leads us to… 1.1.2 Installing packages with install.packages() A basic install of R comes with the base package with has an unbelievable large number of functions and analyses built in. However base R is accompanied by a steep learning curve, especially for those with no programming experience. In this case we will use many of the simple functions in base R, but for more complex analysis and plotting with use the wonderful tidyverse package. tidyverse piggybacks on the basic install, replacing the sometimes bewildering R syntax with more natural language. Here, we’ll use this installation as an example of how to install using command-line. Installing tidyverse (or any package for that manner) couldn’t be easier. At the prompt (or in your notebook) simply type: install.packages(&quot;tidyverse&quot;) Congratulations you have installed over 70 new packages to your R environment! You see, tidyverse is not a single package (as is usually the case), but a collection of packages that function seamlessless with one another abiding a shared ethos of data science practices (who knew there were ethoses in stats! A simple google search of tidyverse might lead you to believe that you have joined a cult with Hadley Wickham as your leader!). The core value is that data should be structured, analyzed, presented, and shared in a manner that is as transparent as possible. In this class, we aren’t going to go the “full Hadley” (all the way down the rabbit hole) but we are going to abide many shared principles (it’s just good science!). Note for the crqa package I simply type install.packages(&quot;crqa&quot;) and for MFDFA install.packages(&quot;MFDFA&quot;). You noticing a pattern here? 1.1.3 An IMPORTANT note on knitting with install.packages: One quirk with knitting from your source is that you will need to specify which CRAN mirror you will download the package from. The Comprehensive R Network (CRAN) is an online repository that houses almost all things R, including packages. There are multiple mirrors set up all over the globe (https://cran.r-project.org/mirrors.html) and you need to tell R which one is your preferred choice. The quirk is that you only need to do this when knitting/compiling. You don’t need to do this for other everday use, but your source won’t compile unless you fix this (see here to read more about the error this produces. Since your homework is going to involve knitting you should probably get in the practice of fixing this. There are two ways to resolve this issue. The first is to simple add the repos argument to your install.packages() command like so: install.packages(package, repos=&quot;http://cran.us.r-project.org&quot;) Using this method you would need add the repos argument and url to every item you wish to install. A better, more efficient way is to provide a default repo at the outset. This can be accomplished by creating a new chunk at the beginning of your Rmd file (right underneath your header), and inserting the following: setRepositories(graphics = getOption(&quot;menu.graphics&quot;), ind = NULL, addURLs = character()) r &lt;- getOption(&quot;repos&quot;) r[&quot;CRAN&quot;] &lt;- &quot;http://cran.cnr.berkeley.edu/&quot; options(repos = r) 1.1.4 Loading packages with library() Once you have installed a package, it remains stored on your hard-drive until you delete it (which you’ll amost never do, packages take up so little disk space it’s really no efficient to install and uninstall unless absolutely necessary). What many new users have difficulty getting used to is that just because a package is installed does not mean that it is loaded. To load tidyverse, we typically type: library(tidyverse) That’s it, easy-peasy. Note that by default, only the base packages are typically loaded when you start a new session. Therefore, a useful practice to get into is to load the necessary packages at the start of every session. Before moving on I’d like to point out two things. First, notice that when installing with install.packages() we included quotations around the package name, but with library() we did not. Knowing when to use quotes and when not to is one of the more frustrating things to grasp for beginners (and even is an annoyance for me from time to time). All I can say is with practice, it becomes automatic. In truth, in the case of library() whether you include the quotes or not doesn’t matter (the default is to not), but for many functions, including install.packages(), the proper quotes do matter (for example: install.packages(tidyverse) will result in an error) Second, while installing tidyverse installs &gt;70 packages on your computer, library(tidyverse) only loads 12 or so primary packages. This isn’t really much of an issue as we will be mostly using this common tidyverse packages, but it’s worth mentioning for future reference. 1.1.5 Loading packages with require() Note that this section is a little more advanced and assumes you have taken a look at the Demos readings assigned for this week in particular those related to assigning variables (Ch 1) and, importantly, logicals (Ch 3). If you want, you can stop here and be perfectly able to install and load R packages. But for those that want to Level-Up, this section and the next outline what I believe is the best way to install and load libraries when creating R-Notebooks for sharing (i.e. turning in your assignments). Of course if you don’t really care about the logic and just want to get the cheat code, you may proceed to the end of the next section There is in fact another way to load a previously installed package, require(): require(tidyverse) require() has the additional benfit of returning a “FALSE” logical output if the requested package hasn’t been installed on your computer (see Demos, Ch 3.1 for more on logicals). This is in contrast to library() which just freaks out and reports an error (errors are bad as they can stop your execution). For example, let’s save the output of the following actions to an object called myVar. For example running: myVar &lt;- library(xkcd) myVar versus myVar &lt;- require(xkcd) myVar In both cases xkcd was not installed on your comp and an error was returned. However, with library(), nothing was saved to myVar, but require() retured a FALSE indicating that the requested package was not installed. Why do I bother to even bring this up? 1.1.6 Installing and loading packages like a ninja The central reason that we are using R is to get in the practice of data transparency and replicability. Ultimately, for every analysis that you perform you should be able to provide me with the appropriate syntax in your notebook file and I should be able to re-run each of your analyses step by step on my own computer. One important consideration is that you may be using packages that I don’t have installed and loaded on my computer and vice versa. To deal with this you would need to include two lines for every package: install.packages(&quot;package&quot;) library(package) If you use a lot of packages, this can become very repetitive (imagine using 10 packages). A much more efficient way of doing things is to take advantage of that FALSE that require() returns. But first we need to install a package called pacman. Before we do this, input the next to lines seperately: show(require(pacman)) ## [1] TRUE show(!require(pacman)) ## [1] FALSE Note that the show function forces R to show the output. Recall from above that require() produces a FALSE if the requested package is NOT installed on your computer. So, assuming that pacman is not installed on your computer the first line will produce a FALSE and the second will produce a TRUE. So what is that second line then? NEGATION. Placing an exclation point in fron of a statement essentially reads as “not true”. For now just understand that using this syntax we can create a line of code that checks to see if pacman is installed on your computer, and if not it installs it: if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) As outlined in Demos, Ch. 3, the above is a conditional if-statement. It literally reads: Check to see if the statement !require(&quot;pacman&quot;) is TRUE (i.e., TRUE = pacman is not installed on your computer). If the above indeed returns a TRUE, then run `install.packages(“pacman”). In theory you could run this line for every package, but that would be tedious as well and is not really a simpler solution. Fortunately pacman contains a function that simplifies this for us, pacman::p_load() function. To get a feel for what this function does run the following line: `?`(pacman::p_load()) The ? brings up an online help module for the named function. In this case the function is p_load() from the pacman package. You see that pacman::p_load() checks to see if a package is installed, if not it attempts to install the package from CRAN and/or any other repository. After that it loads all listed packages. Let’s try a few packages that you haven’t installed but are going to be useful to us later when we do ANOVA. FWIW I usually have a code chunk at the top of every notebook that uses this template, swapping in the various packages that I intend to use. ****** HERE’S THE CHEAT CODE!!! ******* # 1. check to see if pacman is on your computer and if not, let&#39;s install it: if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) # 2. install all other packages that we will be using: pacman::p_load(afex, lmerTest, plyr, car) This bit of code installed and loaded the afex, lmerTest, and plyr packages. To test that everything worked, try: data(obk.long, package = &quot;afex&quot;) afex::aov_ez(&quot;id&quot;, &quot;value&quot;, obk.long, between = c(&quot;treatment&quot;, &quot;gender&quot;), within = c(&quot;phase&quot;, &quot;hour&quot;), observed = &quot;gender&quot;) ## Contrasts set to contr.sum for the following variables: treatment, gender ## Anova Table (Type 3 tests) ## ## Response: value ## Effect df MSE F ges p.value ## 1 treatment 2, 10 22.81 3.94 + .20 .05 ## 2 gender 1, 10 22.81 3.66 + .11 .08 ## 3 treatment:gender 2, 10 22.81 2.86 .18 .10 ## 4 phase 1.60, 15.99 5.02 16.13 *** .15 .0003 ## 5 treatment:phase 3.20, 15.99 5.02 4.85 * .10 .01 ## 6 gender:phase 1.60, 15.99 5.02 0.28 .003 .71 ## 7 treatment:gender:phase 3.20, 15.99 5.02 0.64 .01 .61 ## 8 hour 1.84, 18.41 3.39 16.69 *** .13 &lt;.0001 ## 9 treatment:hour 3.68, 18.41 3.39 0.09 .002 .98 ## 10 gender:hour 1.84, 18.41 3.39 0.45 .004 .63 ## 11 treatment:gender:hour 3.68, 18.41 3.39 0.62 .01 .64 ## 12 phase:hour 3.60, 35.96 2.67 1.18 .02 .33 ## 13 treatment:phase:hour 7.19, 35.96 2.67 0.35 .009 .93 ## 14 gender:phase:hour 3.60, 35.96 2.67 0.93 .01 .45 ## 15 treatment:gender:phase:hour 7.19, 35.96 2.67 0.74 .02 .65 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG Congrats! You’ve just run a 2x2 mxed effects ANOVA! We’ll revisit what exactly is going on here in 8 weeks. 1.2 Loading in data Typically the file types that are used by beginners in R are plain text and delimited. They may have the extension “txt”, “csv”, or “dat” for example. These may become more sophisticated you progress, for example you can load in proprietery types like SPSS and STATA, but for this course we will mostly use plain text files (although later in the course I will show you how to load in Excel files). 1.2.1 Loading in local data using a GUI: As with loading packages, you can also load in a file containing data using the RStudio GUI. See this video Again, this is only preferred if you are not sharing your analysis. If you are sharing your analysis (as in this class) you need to do the command line. Fortunately, RStudio creates the appropiate command-line for you to copy and passte. For example for my computer it’s: LexicalDescisionData &lt;- read.delim(&quot;~/UCedu/Teaching/gradStats/week1/LexicalDescisionData.txt&quot;) I’ve uploaded this file to Box in the “exampleData” folder. Please feel free to download the file and follow along is you wish. Note that if you do elect to load in via the GUI you need to be sure to copy the output to your Rmd source. 1.2.2 importing data from the web You might be saying to yourself, “but Tehran the entire reason you’ve got us learning R is for transparency and openess with our data. How would I be able to share in my code a data file that resides on my hard drive?!?”&quot; Correct, you can’t, but you can upload it to the internet and someone can access it from an online repository. Personally, I like to use Github, but we’ll save that for some advanced stuff later in the semester for those so inclined. For now, several of our in-class examples will come from the Howell textbook. Howell has an online repository on his website that contains data sets for examples in the text: We can load in this data from the web using the read.delim() function from above. Let’s assign it to an object RxData RxData &lt;- read.delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat&quot;, header = T, sep = &quot;&quot;) To see what’s going on with the additional calls in this line, run the following line to get help: `?`(read.delim) A document file should show up in your help tab, containing examples and describing what different arguments are doing. Search for header and sep and try to figure out what’s going on. 1.2.3 Downloading data The example above just pulls data directly from a url, what if you want to download the data file directly onto your computer and load it from there? Well, there’s a package for that… downloader. Let’s install this package and download the data from above: # within the R code sections, hashtags create comments, sections of code that are # not interpreted by the computer, but may serve to inform others (and typically # yourself later in life) about what exactly in the hell is going on here. GET IN # THE PRACTICE OF COMMENTING YOUR CODE. You&#39;ll thank yourself later. Here I&#39;m # using comments to inform you step-by-step what each line is doing: # install and load &#39;downloader&#39; package, this assumes you have &#39;pacman&#39; installed # and loaded *see section 1.1.6 above: pacman::p_load(downloader) # get the url of the file you want to download and assign it to an object # (&#39;dataURL&#39;): dataURL &lt;- &quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat&quot; # decide on what name you want to give the file. In this case I&#39;m extracting # using the basename from the web url: Tab2-1.dat filename &lt;- basename(dataURL) # download the file to your current R-project folder: download(url = dataURL, filename) Keep in mind that objects are just placeholders. So if I was so inclined I could have accomplished all of the above with just one line: download(url = &quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat&quot;, destfile = &quot;Tab2-1.dat&quot;) # destfile is a parameter for naming what you download. From here I could just import the Tab2-1.dat file from my computer using the GUI method above. 1.3 Looking ahead… This is probably a good place to stop for now. In the meantime try running the following 4 commands (assuming that you have imported RxData) and think about what they are returning: class(RxData) names(RxData) head(RxData) summary(RxData) "],
["measures-of-distribution-central-tendency-plots-oh-my.html", "Week 2 Measures of distribution, central tendency, PLOTS… oh my! 2.1 Getting measures of central tendency 2.2 Plotting the distribution with a histogram 2.3 Plotting your data in a boxplot: 2.4 Assessing the data for normality 2.5 Subsetting your data 2.6 Looking ahead: Means and better models", " Week 2 Measures of distribution, central tendency, PLOTS… oh my! We covered a lot of ground this week talking about measures of central tendency and introducing ourselves to distributions. When you get a chance be sure to check out some of the in-class workshopping code and examples as they are posted on Box. These will help you remember what we did in the workshop and you can compare them to your source and output. This week I want to expand upon a few things that were touched upon in the workshop and in lecture. This one might be a little lengthly, but it should serve to supplement your assigned readings and help you a little with the home work. Before jumping in, let’s install and load the necessary packages in R. This week we will make use of psych and several of the core packages of the tidyverse. I’ll also be using data.table which provides another way of importing data (in addition to the examples from last week). Using the cheat code from last week’s walkthrough # 1. check to see if pacman is on your computer and if not, let&#39;s install it: if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;, repos = &quot;http://cran.us.r-project.org&quot;) # 2. install all other packages that we will be using: pacman::p_load(psych, tidyverse, data.table) 2.1 Getting measures of central tendency In this week’s workshop, we touched upon using psych::describe() to generate summary stats of data in a data frame, as well as generating a simple histogram using the hist() function. Our starting point was data from an example provided in the Howell textbook. # importing data, here I&#39;m using data.table. I prefer data table as it produces # truncated output. exampleData &lt;- data.table::fread(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat&quot;) Invoking class(exampleData) tells us that exampleData is a data frame. Invoking names(exampleData) provides us with the header names: class(exampleData) ## [1] &quot;data.table&quot; &quot;data.frame&quot; names(exampleData) ## [1] &quot;Trial&quot; &quot;RxTime&quot; &quot;NStim&quot; &quot;YesNo&quot; From here we discussed how to call upon a specific column in a data frame using the $ operator: # column of Trial(s): exampleData$Trial ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 ## [103] 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 ## [120] 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 ## [137] 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 ## [154] 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 ## [171] 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 ## [188] 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 ## [205] 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 ## [222] 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 ## [239] 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 ## [256] 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 ## [273] 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 ## [290] 290 291 292 293 294 295 296 297 298 299 300 # column of RxTime (reaction time) exampleData$RxTime ## [1] 40 41 47 38 40 37 38 47 45 61 54 67 49 43 52 39 46 ## [18] 47 45 43 39 49 50 44 53 46 64 51 40 41 44 48 50 42 ## [35] 90 51 55 60 47 45 41 42 72 36 43 94 45 51 46 52 52 ## [52] 45 74 56 53 59 43 46 51 40 48 47 57 54 44 56 47 62 ## [69] 44 53 48 104 50 58 52 57 66 49 59 56 71 76 54 71 44 ## [86] 67 45 79 46 57 58 47 73 67 46 57 52 61 72 104 73 83 ## [103] 55 59 51 65 61 64 63 86 42 65 62 62 51 62 72 55 58 ## [120] 46 67 56 52 46 62 51 51 61 60 75 53 59 56 50 43 58 ## [137] 67 52 56 80 53 72 62 59 47 62 53 52 46 60 73 47 63 ## [154] 63 56 66 72 58 60 69 74 51 49 69 51 60 52 72 58 74 ## [171] 59 63 60 66 59 61 50 67 63 61 80 63 60 64 64 57 59 ## [188] 58 59 60 62 63 67 78 61 52 51 56 95 54 39 65 53 46 ## [205] 78 60 71 58 87 77 62 94 81 46 49 62 55 59 88 56 77 ## [222] 67 79 54 83 75 67 60 65 62 62 62 60 58 67 48 51 67 ## [239] 98 64 57 67 55 55 66 60 57 54 78 69 66 53 61 74 76 ## [256] 69 82 56 66 63 69 76 71 65 67 67 55 65 58 64 65 81 ## [273] 69 69 63 68 70 80 68 125 63 74 61 85 59 61 74 76 62 ## [290] 83 58 72 65 61 95 58 64 66 66 72 and how to get some important summary statistics on the entire data frame using psych::describe: psych::describe(exampleData) ## vars n mean sd median trimmed mad min max range skew ## Trial 1 300 150.50 86.75 150.5 150.5 111.19 1 300 299 0.00 ## RxTime 2 300 60.26 13.01 59.5 59.3 11.12 36 125 89 0.97 ## NStim 3 300 3.00 1.64 3.0 3.0 2.97 1 5 4 0.00 ## YesNo 4 300 1.50 0.50 1.5 1.5 0.74 1 2 1 0.00 ## kurtosis se ## Trial -1.21 5.01 ## RxTime 2.12 0.75 ## NStim -1.51 0.09 ## YesNo -2.01 0.03 You may also get summary statistics for individual groups. In this case the NStim column represents experimental conditions (num. of stimuli = 1, num. of stimuli = 3, num. of stimuli = 5). To take a look at the summary stat by condition we used the psych::describeBY() function: psych::describeBy(exampleData, group = exampleData$NStim) ## ## Descriptive statistics by group ## group: 1 ## vars n mean sd median trimmed mad min max range skew ## Trial 1 100 50.50 29.01 50.5 50.50 37.06 1 100 99 0.0 ## RxTime 2 100 53.27 13.36 50.0 51.27 8.90 36 104 68 1.7 ## NStim 3 100 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 0.0 ## kurtosis se ## Trial -1.24 2.90 ## RxTime 3.34 1.34 ## NStim NaN 0.00 ## YesNo -2.02 0.05 ## -------------------------------------------------------- ## group: 3 ## vars n mean sd median trimmed mad min max range skew ## Trial 1 100 150.50 29.01 150.5 150.50 37.06 101 200 99 0.00 ## RxTime 2 100 60.65 9.41 60.0 60.01 7.41 42 95 53 0.78 ## NStim 3 100 3.00 0.00 3.0 3.00 0.00 3 3 0 NaN ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 0.00 ## kurtosis se ## Trial -1.24 2.90 ## RxTime 1.13 0.94 ## NStim NaN 0.00 ## YesNo -2.02 0.05 ## -------------------------------------------------------- ## group: 5 ## vars n mean sd median trimmed mad min max range skew ## Trial 1 100 250.50 29.01 250.5 250.50 37.06 201 300 99 0.00 ## RxTime 2 100 66.86 12.28 65.0 65.91 9.64 39 125 86 1.29 ## NStim 3 100 5.00 0.00 5.0 5.00 0.00 5 5 0 NaN ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 0.00 ## kurtosis se ## Trial -1.24 2.90 ## RxTime 4.03 1.23 ## NStim NaN 0.00 ## YesNo -2.02 0.05 We also noted that some summary stat information can be obtained using the summary() function. The basic install comes with the summary() function that, when applied to a data frame provides a minimal number of summary stats. NOTE: the summary() function is a jack-of-all trades and when used in other instances may provide drastically different outputs (e.g., it may summarize a linear model). For this data set: summary(exampleData) ## Trial RxTime NStim YesNo ## Min. : 1.00 Min. : 36.00 Min. :1 Min. :1.0 ## 1st Qu.: 75.75 1st Qu.: 51.00 1st Qu.:1 1st Qu.:1.0 ## Median :150.50 Median : 59.50 Median :3 Median :1.5 ## Mean :150.50 Mean : 60.26 Mean :3 Mean :1.5 ## 3rd Qu.:225.25 3rd Qu.: 67.00 3rd Qu.:5 3rd Qu.:2.0 ## Max. :300.00 Max. :125.00 Max. :5 Max. :2.0 In general I prefer psych::describe() to summary() when getting descriptive stats, but introduce summary() here to use it provide an example further in this week’s chapter. 2.2 Plotting the distribution with a histogram 2.2.1 Using hist() In class we used the hist() call on a vector of scores to produce a quick and dirty histogram. For example to produce a histogram of RxTime from our exampleData dataset we can: hist(exampleData$RxTime) # R auto-selects number of bins hist(exampleData$RxTime, breaks = 5) # create 5 bins The second line above forces R to create 5 bins. If you recall from Howell, one rule of thumb for the number of bins (there are many) is to use the square root of the number of observations (N). To do that here we can first use the length() function to get the number of observations (i.e., the length of the exampleData$RxTime vector of scores): length(exampleData$RxTime) # length of vector ## [1] 300 ASIDE: Note that length works on vectors, but not on entire data frames (multiple columns of data). Vectors are 1 dinmensional… thing of a line of data. Data sets (or data frames) are 2D (colmuns and rows of data). Since each column contains our variables, the number of obervations corresponds to the number of rows in the data set. So, to get the “length” you just need to count the number of rows: nrow(exampleData) # number of rows in the data set (each row corresponds to an observation) ## [1] 300 Returning to the ‘optimal’ number of bins, be can now put together the following: N &lt;- length(exampleData$RxTime) hist(exampleData$RxTime, breaks = sqrt(N)) # create square root of N bins 2.2.2 Plotting a histogram with ggplot() This section assumes you have completed this week’s DataCamp assignment that includes a wonderful introduction to plotting with ggplot(). If you have yet to do so, please go there first. Now let’s try fitting a normal curve to our histogram. For this we are going to move into more complex ways of plotting data using ggplot2 from the tidyverse. You’ll be introduced to the tidyverse in this week’s DataCamp homework assignments. I suggest looking there and at the Demos Ch. 10 &amp; 11 for another walkthrough. The Field text, Ch. 4 also covers ggplot extensively. Fisrt let’s use ggplot() to create a histogram. To get a feel for each step, run through this code line by line in RStudio: # load in tidyverse if you haven&#39;t already pacman::p_load(tidyverse) # intiial step i identify data and grouping parameters: histPlot1 &lt;- ggplot2::ggplot(data = exampleData, aes(x=RxTime)) histPlot1 # note that this produces a blank slate but the parameters are locked in. # take histPlot 1 (blank plot) and paint a layer on top of it # tell ggplot what to do (this is where you actually build the graphics): histPlot2 &lt;- histPlot1 + geom_histogram(binwidth = 5, color = &quot;orange&quot;, # what color is the outline of bars fill = &quot;green&quot;) # what color to fill the bars with histPlot2 I intentially made a hideous looking plot above to show you what the additional arguments do. Also note that with ggplot() you modify the width of the bin binwidth rather than the number of bins, or breaks as in hist(). Okay, now to add a curve representing the normal distribution. One important thing to note is that the histogram that is ultimately produced here has probability density (think like % of scores) on the y-axis instead of frequency (# of scores). # take histPlot 1 (blank plot) and paint a layer on top of it tell ggplot what to # do (this is where you actually build the graphics): histPlot3 &lt;- histPlot1 + geom_histogram(binwidth = 5, color = &quot;black&quot;, fill = &quot;white&quot;, aes(y = ..density..)) # turns plot into density plot histPlot3 notice that histPlot3 is a density plot rather frequency plot from above # add a normal curve to density plot: histPlot4 &lt;- histPlot3 + stat_function(fun = dnorm, # generate theoretical norm data color = &quot;red&quot;, # color the line red args=list(mean = mean(exampleData$RxTime), # build around mean sd = sd(exampleData$RxTime))) # &amp; standard devation histPlot4 Howell also mentions a better alternative to the normal curve, a kernel density plot. To fit a kernel density plot to our histogram we can invoke: Howell also mentions a better alternative to the normal curve, a kernel density plot. To fit a kernel density plot to our histogram (histPlot3) we can invoke: # hisPlot 3 was the base histogram without curve histPlot3 + geom_density() and let’s make the x-axis label a little more transparent with xlab() histPlot3 + geom_density() + xlab(&quot;Reaction time (ms)&quot;) Any guesses on how to change the y-axis label to “Percent of scores”? 2.3 Plotting your data in a boxplot: Let’s recreate the boxplot from slide 7 of the lecture (Figure 2.15 - Howell). Before we proceed, I would like to draw you attention to a characteristic of the NStim column in our exampleData dataset. psych::describe(exampleData) ## vars n mean sd median trimmed mad min max range skew ## Trial 1 300 150.50 86.75 150.5 150.5 111.19 1 300 299 0.00 ## RxTime 2 300 60.26 13.01 59.5 59.3 11.12 36 125 89 0.97 ## NStim 3 300 3.00 1.64 3.0 3.0 2.97 1 5 4 0.00 ## YesNo 4 300 1.50 0.50 1.5 1.5 0.74 1 2 1 0.00 ## kurtosis se ## Trial -1.21 5.01 ## RxTime 2.12 0.75 ## NStim -1.51 0.09 ## YesNo -2.01 0.03 2.3.1 turning numeric data into categorical As you can see we get a mean, sd, median, etc for NStim, BUT SHOULD WE? The number of stimuli is not numerical properly… but instead represent experimental categories. The same can be said for the YesNo data. We need to tell R that these data are categories, or factors and not continuous numbers. To do so we can call on the factor() function as below. # convert a numeric to a categorical (i.e., make a factor) factor(exampleData$NStim) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [71] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 ## [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [141] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [176] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 ## [211] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 ## [246] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 ## [281] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 ## Levels: 1 3 5 When you run the line above you’ll note at the end of the output it mentions “Levels: 1 3 5”. This is telling you that in this vector, there are three levels, or conditions in this factor- number of stimuli = 1; number of stimuli = 3; number of stimuli = 5. To connect this factorized vector to your original data set, you can either create a new column in the data set or overwrite the original NStim column that you are replacing. For beginners I would recommend adding a new column. # 1. create a new vector attached to the original data frame To do this you can # create a blank column by invoking [dataset$newcolumn name] and then fill it # with the factorized data: exampleData$categoricalNStim &lt;- factor(exampleData$NStim) psych::describe(exampleData) ## vars n mean sd median trimmed mad min max ## Trial 1 300 150.50 86.75 150.5 150.5 111.19 1 300 ## RxTime 2 300 60.26 13.01 59.5 59.3 11.12 36 125 ## NStim 3 300 3.00 1.64 3.0 3.0 2.97 1 5 ## YesNo 4 300 1.50 0.50 1.5 1.5 0.74 1 2 ## categoricalNStim* 5 300 2.00 0.82 2.0 2.0 1.48 1 3 ## range skew kurtosis se ## Trial 299 0.00 -1.21 5.01 ## RxTime 89 0.97 2.12 0.75 ## NStim 4 0.00 -1.51 0.09 ## YesNo 1 0.00 -2.01 0.03 ## categoricalNStim* 2 0.00 -1.51 0.05 The asterisk next to categoricalNStim lets you know it’s categorical. Aternatively you can use summary(): summary(exampleData) ## Trial RxTime NStim YesNo ## Min. : 1.00 Min. : 36.00 Min. :1 Min. :1.0 ## 1st Qu.: 75.75 1st Qu.: 51.00 1st Qu.:1 1st Qu.:1.0 ## Median :150.50 Median : 59.50 Median :3 Median :1.5 ## Mean :150.50 Mean : 60.26 Mean :3 Mean :1.5 ## 3rd Qu.:225.25 3rd Qu.: 67.00 3rd Qu.:5 3rd Qu.:2.0 ## Max. :300.00 Max. :125.00 Max. :5 Max. :2.0 ## categoricalNStim ## 1:100 ## 3:100 ## 5:100 ## ## ## # also note that summary() now tells you the number of obsevations rather than # trying to compute descriptive stats # 3. or you can overwrite the original column Be careful doing this. Once you # overwrite you can&#39;t go back exampleData$NStim &lt;- factor(exampleData$NStim) psych::describe(exampleData) # star lets you know it&#39;s categorical ## vars n mean sd median trimmed mad min max ## Trial 1 300 150.50 86.75 150.5 150.5 111.19 1 300 ## RxTime 2 300 60.26 13.01 59.5 59.3 11.12 36 125 ## NStim* 3 300 2.00 0.82 2.0 2.0 1.48 1 3 ## YesNo 4 300 1.50 0.50 1.5 1.5 0.74 1 2 ## categoricalNStim* 5 300 2.00 0.82 2.0 2.0 1.48 1 3 ## range skew kurtosis se ## Trial 299 0.00 -1.21 5.01 ## RxTime 89 0.97 2.12 0.75 ## NStim* 2 0.00 -1.51 0.05 ## YesNo 1 0.00 -2.01 0.03 ## categoricalNStim* 2 0.00 -1.51 0.05 summary(exampleData) ## Trial RxTime NStim YesNo categoricalNStim ## Min. : 1.00 Min. : 36.00 1:100 Min. :1.0 1:100 ## 1st Qu.: 75.75 1st Qu.: 51.00 3:100 1st Qu.:1.0 3:100 ## Median :150.50 Median : 59.50 5:100 Median :1.5 5:100 ## Mean :150.50 Mean : 60.26 Mean :1.5 ## 3rd Qu.:225.25 3rd Qu.: 67.00 3rd Qu.:2.0 ## Max. :300.00 Max. :125.00 Max. :2.0 2.3.2 boxplotting using ggplot() To create a boxplot of the entire data set (regardless of NStim) we can: boxPlot &lt;- ggplot2::ggplot(data = exampleData, aes(y = RxTime)) boxPlot1 &lt;- boxPlot + geom_boxplot() boxPlot1 Note that the x-axis in this plot is meaningless. Now let’s take a look at the data that will be boxplotted as a function of NStim groups. Here I want to note that psych::describeBy has an additional argument to display the quantiles: psych::describeBy(exampleData, group = exampleData$NStim, quant = c(0.25, 0.75)) ## ## Descriptive statistics by group ## group: 1 ## vars n mean sd median trimmed mad min max range ## Trial 1 100 50.50 29.01 50.5 50.50 37.06 1 100 99 ## RxTime 2 100 53.27 13.36 50.0 51.27 8.90 36 104 68 ## NStim* 3 100 1.00 0.00 1.0 1.00 0.00 1 1 0 ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 ## categoricalNStim* 5 100 1.00 0.00 1.0 1.00 0.00 1 1 0 ## skew kurtosis se Q0.25 Q0.75 ## Trial 0.0 -1.24 2.90 25.75 75.25 ## RxTime 1.7 3.34 1.34 45.00 57.25 ## NStim* NaN NaN 0.00 1.00 1.00 ## YesNo 0.0 -2.02 0.05 1.00 2.00 ## categoricalNStim* NaN NaN 0.00 1.00 1.00 ## -------------------------------------------------------- ## group: 3 ## vars n mean sd median trimmed mad min max range ## Trial 1 100 150.50 29.01 150.5 150.50 37.06 101 200 99 ## RxTime 2 100 60.65 9.41 60.0 60.01 7.41 42 95 53 ## NStim* 3 100 2.00 0.00 2.0 2.00 0.00 2 2 0 ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 ## categoricalNStim* 5 100 2.00 0.00 2.0 2.00 0.00 2 2 0 ## skew kurtosis se Q0.25 Q0.75 ## Trial 0.00 -1.24 2.90 125.75 175.25 ## RxTime 0.78 1.13 0.94 53.75 64.25 ## NStim* NaN NaN 0.00 2.00 2.00 ## YesNo 0.00 -2.02 0.05 1.00 2.00 ## categoricalNStim* NaN NaN 0.00 2.00 2.00 ## -------------------------------------------------------- ## group: 5 ## vars n mean sd median trimmed mad min max range ## Trial 1 100 250.50 29.01 250.5 250.50 37.06 201 300 99 ## RxTime 2 100 66.86 12.28 65.0 65.91 9.64 39 125 86 ## NStim* 3 100 3.00 0.00 3.0 3.00 0.00 3 3 0 ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 ## categoricalNStim* 5 100 3.00 0.00 3.0 3.00 0.00 3 3 0 ## skew kurtosis se Q0.25 Q0.75 ## Trial 0.00 -1.24 2.90 225.75 275.25 ## RxTime 1.29 4.03 1.23 59.75 72.50 ## NStim* NaN NaN 0.00 3.00 3.00 ## YesNo 0.00 -2.02 0.05 1.00 2.00 ## categoricalNStim* NaN NaN 0.00 3.00 3.00 To plot this we need to tell R to put different distributions at different points along the x-axis boxPlot &lt;- ggplot2::ggplot(data = exampleData, aes(y = RxTime, x = categoricalNStim)) boxPlot1 &lt;- boxPlot + geom_boxplot() boxPlot1 Before leaving I want to show what would’ve happended in the latter case if we didn’t correct NStim and left it as a numeric variable rather than converting to a categorical: # re-importing original data exampleData &lt;- data.table::fread(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab2-1.dat&quot;) # creating the boxplot using exact code from chunk above: boxPlot &lt;- ggplot2::ggplot(data = exampleData, aes(y = RxTime, x = NStim)) boxPlot1 &lt;- boxPlot + geom_boxplot() boxPlot1 ## Warning: Continuous x aesthetic -- did you forget aes(group=...)? 2.4 Assessing the data for normality As mentioned in class, one of the key underlying assumptions of our inferential stat is that the data being analyzed is normally distributed. For this example I’ll be using some data that I had intended to work from in this week’s workshop. This data is located on the Box drive in weeklySchedule/week2/workshop/InClass_1.csv. To proceed, you may download it from BOX drive to your computer and import, or you can access it directly from the shared folder… InClass_1 &lt;- read.delim(&quot;https://uc.box.com/shared/static/3u49i5zrtpz5pvtmv566eizobd01oqby.csv&quot;, sep = &quot;,&quot;) and now plotting this data related to BehaviorX with a normal curve overlayed as in slide #19: # load in tidyverse if you haven&#39;t already pacman::p_load(tidyverse) # identify data and grouping parameters: histPlot &lt;- ggplot2::ggplot(data = InClass_1, aes(x = BehaviorX)) + geom_histogram(binwidth = 5, color = &quot;black&quot;, fill = &quot;white&quot;, aes(y = ..density..)) + stat_function(fun = dnorm, color = &quot;red&quot;, args = list(mean = mean(InClass_1$BehaviorX), sd = sd(InClass_1$BehaviorX))) histPlot As you can see the distribution is approximately normal. We can get a further assessment of skew and kurtosis using psych::describe(): psych::describe(InClass_1) ## vars n mean sd median trimmed mad min max range ## BehaviorX 1 500 74.76 18.81 74.61 74.62 18.17 20.48 132.65 112.17 ## BehaviorY 2 500 37.31 24.17 34.53 35.68 27.15 0.79 110.02 109.23 ## skew kurtosis se ## BehaviorX 0.09 -0.11 0.84 ## BehaviorY 0.53 -0.36 1.08 As conveyed here, our values for skew and kurtosis fall within ideal ranges. 2.4.1 Q-Q Plots Another method of assessing normalitly is by using a quantile-quantile, or Q-Q plot. A Q-Q plot is the plot of the sample quantiles from your data against a theoretical normal distribution. Q-Q plots typically use a straight line y=x as an assessment device where any deviation away from the straight line indicates deviation from normality. To assess our data we can invoke both of following functions on our data: qqnorm(InClass_1$BehaviorX) qqline(InClass_1$BehaviorX) Looking pretty good! Check out InClass_1$BehaviorY on your own. 2.4.2 Generating a theoretical normal distribution. We can also generate a random normal distribution using the rnorm() function. By filling in the appropriate arguments we can create a set of data of a desired length with a desired mean and standarad deviation. numSamples &lt;- 20 set.seed(1) # we&#39;ll come back to this next class randomGenerated &lt;- rnorm(n = numSamples,mean = 45,sd = 5.0) ranGen_df &lt;- data_frame(randomGenerated) # coerce to a data frame for ggplot # creating a histogram: # note I did this all in one step: normalPlot &lt;- ggplot2::ggplot(data = ranGen_df, aes(x=randomGenerated)) + geom_histogram(binwidth = 5, color = &quot;black&quot;, fill = &quot;white&quot;, aes(y=..density..)) + stat_function(fun = dnorm, # generate theoretical norm data color = &quot;red&quot;, # color the line red args=list(mean = mean(ranGen_df$randomGenerated), # build around mean sd = sd(ranGen_df$randomGenerated))) # &amp; standard devation normalPlot see what happens as you increase the number of samples from 20 to 30 to 50, 100, 1000, 10000. What does this suggest about ideal sample size? 2.5 Subsetting your data Your homework this week asks you to subset your data by condition and create a plot for each resulting condition (or group). For example in our exampleData there are 3 conditions by NStim: 1, 3, and 5. Using psych::describeBy() we can get summary data for each: psych::describeBy(exampleData, group = exampleData$NStim) ## ## Descriptive statistics by group ## group: 1 ## vars n mean sd median trimmed mad min max range skew ## Trial 1 100 50.50 29.01 50.5 50.50 37.06 1 100 99 0.0 ## RxTime 2 100 53.27 13.36 50.0 51.27 8.90 36 104 68 1.7 ## NStim 3 100 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 0.0 ## kurtosis se ## Trial -1.24 2.90 ## RxTime 3.34 1.34 ## NStim NaN 0.00 ## YesNo -2.02 0.05 ## -------------------------------------------------------- ## group: 3 ## vars n mean sd median trimmed mad min max range skew ## Trial 1 100 150.50 29.01 150.5 150.50 37.06 101 200 99 0.00 ## RxTime 2 100 60.65 9.41 60.0 60.01 7.41 42 95 53 0.78 ## NStim 3 100 3.00 0.00 3.0 3.00 0.00 3 3 0 NaN ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 0.00 ## kurtosis se ## Trial -1.24 2.90 ## RxTime 1.13 0.94 ## NStim NaN 0.00 ## YesNo -2.02 0.05 ## -------------------------------------------------------- ## group: 5 ## vars n mean sd median trimmed mad min max range skew ## Trial 1 100 250.50 29.01 250.5 250.50 37.06 201 300 99 0.00 ## RxTime 2 100 66.86 12.28 65.0 65.91 9.64 39 125 86 1.29 ## NStim 3 100 5.00 0.00 5.0 5.00 0.00 5 5 0 NaN ## YesNo 4 100 1.50 0.50 1.5 1.50 0.74 1 2 1 0.00 ## kurtosis se ## Trial -1.24 2.90 ## RxTime 4.03 1.23 ## NStim NaN 0.00 ## YesNo -2.02 0.05 but we cannot use this to generate a plot. In order to do this we need to split the original data file into three parts, one for each NStim condition. DataCamp introduces you to the dplyr::filter() function this week, which extracts data from a larger set given some criteria. For example to pull our NStim == 1 data from the exampleData data set we can: dplyr::filter(exampleData, exampleData$NStim == 1) ## Trial RxTime NStim YesNo ## 1 1 40 1 1 ## 2 2 41 1 1 ## 3 3 47 1 1 ## 4 4 38 1 1 ## 5 5 40 1 1 ## 6 6 37 1 1 ## 7 7 38 1 1 ## 8 8 47 1 1 ## 9 9 45 1 1 ## 10 10 61 1 1 ## 11 11 54 1 1 ## 12 12 67 1 1 ## 13 13 49 1 1 ## 14 14 43 1 1 ## 15 15 52 1 1 ## 16 16 39 1 1 ## 17 17 46 1 1 ## 18 18 47 1 1 ## 19 19 45 1 1 ## 20 20 43 1 1 ## 21 21 39 1 1 ## 22 22 49 1 1 ## 23 23 50 1 1 ## 24 24 44 1 1 ## 25 25 53 1 1 ## 26 26 46 1 1 ## 27 27 64 1 1 ## 28 28 51 1 1 ## 29 29 40 1 1 ## 30 30 41 1 1 ## 31 31 44 1 1 ## 32 32 48 1 1 ## 33 33 50 1 1 ## 34 34 42 1 1 ## 35 35 90 1 1 ## 36 36 51 1 1 ## 37 37 55 1 1 ## 38 38 60 1 1 ## 39 39 47 1 1 ## 40 40 45 1 1 ## 41 41 41 1 1 ## 42 42 42 1 1 ## 43 43 72 1 1 ## 44 44 36 1 1 ## 45 45 43 1 1 ## 46 46 94 1 1 ## 47 47 45 1 1 ## 48 48 51 1 1 ## 49 49 46 1 1 ## 50 50 52 1 1 ## 51 51 52 1 2 ## 52 52 45 1 2 ## 53 53 74 1 2 ## 54 54 56 1 2 ## 55 55 53 1 2 ## 56 56 59 1 2 ## 57 57 43 1 2 ## 58 58 46 1 2 ## 59 59 51 1 2 ## 60 60 40 1 2 ## 61 61 48 1 2 ## 62 62 47 1 2 ## 63 63 57 1 2 ## 64 64 54 1 2 ## 65 65 44 1 2 ## 66 66 56 1 2 ## 67 67 47 1 2 ## 68 68 62 1 2 ## 69 69 44 1 2 ## 70 70 53 1 2 ## 71 71 48 1 2 ## 72 72 104 1 2 ## 73 73 50 1 2 ## 74 74 58 1 2 ## 75 75 52 1 2 ## 76 76 57 1 2 ## 77 77 66 1 2 ## 78 78 49 1 2 ## 79 79 59 1 2 ## 80 80 56 1 2 ## 81 81 71 1 2 ## 82 82 76 1 2 ## 83 83 54 1 2 ## 84 84 71 1 2 ## 85 85 44 1 2 ## 86 86 67 1 2 ## 87 87 45 1 2 ## 88 88 79 1 2 ## 89 89 46 1 2 ## 90 90 57 1 2 ## 91 91 58 1 2 ## 92 92 47 1 2 ## 93 93 73 1 2 ## 94 94 67 1 2 ## 95 95 46 1 2 ## 96 96 57 1 2 ## 97 97 52 1 2 ## 98 98 61 1 2 ## 99 99 72 1 2 ## 100 100 104 1 2 To create plots related to this data you can save the above result to a new object and then create your plots as described above. For example, to get RxTime for NStim == 1: NStim1_data &lt;- dplyr::filter(exampleData,exampleData$NStim==1) # you can use hist() here but I&#39;m going to do ggplot() NStim1_plot &lt;- ggplot2::ggplot(data = NStim1_data, aes(x=RxTime)) + geom_histogram(binwidth = 5, color = &quot;black&quot;, fill = &quot;white&quot;, aes(y=..density..)) + stat_function(fun = dnorm, # generate theoretical norm data color = &quot;red&quot;, # color the line red args=list(mean = mean(NStim1_data$RxTime), # build around mean sd = sd(NStim1_data$RxTime))) # &amp; standard devation NStim1_plot Go back and do this with NStim==2 and NStim==3 for practice. Be sure to create new objects for each subset (e.g. NStim2_data) and plot (`NStim2_plot) so that you don’t overwrite what you’ve done before. This is a good place to stop for this week. However if you want to continue on, in the next section I’m going to cash out all of that rambling about “the importance of means” that I’ve been doing 2.6 Looking ahead: Means and better models Hold this one in your back pocket for now. This is the 2nd week in a row that I’ve stressed the importance of the mean. Last week, I said that the mean is the best guess for any individual score if you know nothing else other than the data is normally distributed. BUT it’s a very limited predictor. Here we will use R to extend our in-class example of the mean as the best model if you’ve got nothing else going for you. We’ll also use this to get some more practice with variable assignment and working with vectors. Revisting our in class example from Week 1: tehran &lt;- 38 nabiha &lt;- 28 james &lt;- 24 chris &lt;- 22 margaret &lt;- 27 emily &lt;- 22 sarah &lt;- 24 nate &lt;- 26 tyra &lt;- 22 julia &lt;- 24 angela &lt;- 28 allie &lt;- 24 daniel &lt;- 29 sierra &lt;- 24 Everyone’s age (numerical) is assigned to an object (their name). From here there are several ways to get a mean age for the class. You could just simply follow the formula, the sum of the individual ages divided by the number of people in the class: sum(tehran, nabiha, james, chris, margaret, emily, sarah, nate, tyra, julia, angela, allie, daniel, sierra)/14 ## [1] 25.85714 Alternatively, you could use the mean() function. However, mean requires that all of these number be bound together into a vector (see Demos online text, 1.6). This is simply accomplished by using the concationation function,c(): # first we concatonate our data: ages &lt;- c(tehran, nabiha, james, chris, margaret, emily, sarah, nate, tyra, julia, angela, allie, daniel, sierra) # now we get the mean: mean(ages) ## [1] 25.85714 Of the two options, the second is much more preferable. Once you get your individual data vectored (grouped) into assigned object you can perform functions on that object very easily, like: # doubling everyone&#39;s age: ages * 2 ## [1] 76 56 48 44 54 44 48 52 44 48 56 48 58 48 # getting the log of everyone&#39;s age: log(ages) ## [1] 3.637586 3.332205 3.178054 3.091042 3.295837 3.091042 3.178054 ## [8] 3.258097 3.091042 3.178054 3.332205 3.178054 3.367296 3.178054 # getting the square root: sqrt(ages) ## [1] 6.164414 5.291503 4.898979 4.690416 5.196152 4.690416 4.898979 ## [8] 5.099020 4.690416 4.898979 5.291503 4.898979 5.385165 4.898979 # cubing: ages^3 ## [1] 54872 21952 13824 10648 19683 10648 13824 17576 10648 13824 21952 ## [12] 13824 24389 13824 or perhaps most important for our present concerns, getting the mean, median, standard deviation, minimum value, maximum value, skew and kurtosis of our ages: psych::describe(ages) # gets both min and max ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 14 25.86 4.2 24 25.17 2.97 22 38 16 1.58 2.17 1.12 2.6.1 mean as highest probability: Okay, now we are going to demonstrate that if you know nothing else about your data, the mean is the best guess in terms of probability. We know that if your data are normally In this case we can plot our age data to a density histogram. I’m also going to create a vertical line on the plot indicating the mean age: classAges_df &lt;- data_frame(ages) # coerce to a data frame for ggplot # creating a histogram: # note I did this all in one step: classAgesPlot &lt;- ggplot2::ggplot(data = classAges_df, aes(x=ages)) + geom_histogram(binwidth = 1, color = &quot;black&quot;, fill = &quot;white&quot;, aes(y=..density..)) + stat_function(fun = dnorm, # generate theoretical norm data color = &quot;red&quot;, # color the line red args=list(mean = mean(classAges_df$ages), # build around mean sd = sd(classAges_df$ages))) + geom_vline(xintercept=mean(classAges_df$ages), color=&quot;red&quot;, linetype=&quot;dashed&quot;) classAgesPlot So not too far off from the actual best guess, mode = 24, which represents over 30% of our scores, but still pretty close. The next two sections involve a little bit of complex coding that will be rehearsed in next week’s workshop. For now I have two goals, to 1. demonstrate the mean produces the least amount of summed error and 2. by virtue of that minimized the sum-of-squared differences, or SS. The latter is arguably more important as SS is used to calculate variability and is most diectly used in assessing inferential models. 2.6.2 mean as fulcrum point This week we also decribed the mean as the balance point for our data, that is that the sum of all deviation scores from the mean = 0: \\(\\sum{X-\\bar{X}} = 0\\). That is the mean values produces the least amount of summed error, 0. Again, imagine that you are encountering someone in class and are asked to guess their age (and WIN! WIN! WIN!). The only info that you have is the range of our ages: range(ages) ## [1] 22 38 In what follows, I’m going to iterate through each possible age (guess) within our range and get the sum of difference scores, or sumDiff. sumDiff is obtained by subtracting each age from a given value (see Howell 2.8). Typically this value is the mean. For example say I guessed 27: # 1. guessed age: guess &lt;- 27 # 2. resulting difference scores, squared diffScores &lt;- (ages - guess) # 3. the sum of squares: sumDiff &lt;- sum(diffScores) # 4. show the sumDiff (example) show(sumDiff) ## [1] -16 Now, I’m going to iterate through every possible age in the range, and save the resulting sumDiffs to a vector, sumDiffguess: # get ages, repeat of previous chunk: ages &lt;- c(tehran, nabiha, james, chris, margaret, emily, sarah, nate, tyra, julia, angela, allie, daniel, sierra) # create a vector of all possible integers within our range: rangeAges &lt;- min(ages):max(ages) # create an empty vector to save our resulting sum of differences sumDiffguess &lt;- vector() guessedAge &lt;- vector() # here &#39;i&#39; indexes which value from the range to pull, for example when i=1, we # are pulling the first number in the sequence for (i in 1:length(rangeAges)) { guess &lt;- rangeAges[i] diffScores &lt;- (ages - guess) sumDiff &lt;- sum(diffScores) # save results to sumDiffguess guessedAge[i] &lt;- guess sumDiffguess[i] &lt;- sumDiff } diffAges_df &lt;- data_frame(guessedAge, sumDiffguess) #combine the two vectors to single data frame show(diffAges_df) ## # A tibble: 17 x 2 ## guessedAge sumDiffguess ## &lt;int&gt; &lt;dbl&gt; ## 1 22 54 ## 2 23 40 ## 3 24 26 ## 4 25 12 ## 5 26 -2 ## 6 27 -16 ## 7 28 -30 ## 8 29 -44 ## 9 30 -58 ## 10 31 -72 ## 11 32 -86 ## 12 33 -100 ## 13 34 -114 ## 14 35 -128 ## 15 36 -142 ## 16 37 -156 ## 17 38 -170 Now let’s plot the resulting sum of difference scores as a function of guessed age. The vertical line represents mean age. As you can see this is where our function crosses 0 on the y-axis. # note I did this all in one step: sumDiffPlot &lt;- ggplot2::ggplot(data = diffAges_df, aes(x=guessedAge, y=sumDiffguess)) + geom_point() + geom_line() + geom_vline(xintercept=mean(ages), # using actual ages from color=&quot;red&quot;, linetype=&quot;dashed&quot;) sumDiffPlot 2.6.3 mean as minimizing random error Of course, in our statistics the sum of squared-differences (or SS) \\(\\sum{(X-\\bar{X})^2}\\) is our primary measure rather than the sum of difference in the last example. SS is used to calculate varience \\(\\frac{\\sum{(X-\\bar{X})^2}}{N-1}\\)and in turn standard deviation, which is just the square root of the variance. Both measures we use to describe variability in a distibution of data. When we know nothing about our data we assume that the scores, and therefore the resulting difference or error from the mean is randomly distributed. Hence the term random error. First, let’s calculate the resulting SS for each possible guess (similar to above): # create a vector of all possible integers within our range: rangeAges &lt;- min(ages):max(ages) # create an empty vector to save our resulting sum of differences SSguess &lt;- vector() guessedAge &lt;- vector() # here &#39;i&#39; indexes which value from the range to pull, for example when i=1, we # are pulling the first number in the sequence for (i in 1:length(rangeAges)) { guess &lt;- rangeAges[i] diffScores &lt;- (ages - guess) SSDiff &lt;- sum(diffScores^2) # save SS to SSguess guessedAge[i] &lt;- guess SSguess[i] &lt;- SSDiff } SS_Ages_df &lt;- data_frame(guessedAge, SSguess) #combine the two vectors to single data frame show(SS_Ages_df) ## # A tibble: 17 x 2 ## guessedAge SSguess ## &lt;int&gt; &lt;dbl&gt; ## 1 22 438 ## 2 23 344 ## 3 24 278 ## 4 25 240 ## 5 26 230 ## 6 27 248 ## 7 28 294 ## 8 29 368 ## 9 30 470 ## 10 31 600 ## 11 32 758 ## 12 33 944 ## 13 34 1158 ## 14 35 1400 ## 15 36 1670 ## 16 37 1968 ## 17 38 2294 Now, let’s see what happens when we plot the resulting SS as a function of guessed age: SS_Plot &lt;- ggplot2::ggplot(data = SS_Ages_df, aes(x=guessedAge, y=SSguess)) + geom_point() + geom_smooth() + geom_vline(xintercept=mean(ages), # using actual ages from color=&quot;red&quot;, linetype=&quot;dashed&quot;) SS_Plot ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; As you can see we get parabola with a minima at our mean. 2.6.4 but we can do better than that The mean is good as a stab in the dark, but as you can see our SS is still pretty high. If we have other useful information about the nature of our age data, then maybe we can use that to make better preditions. These extra bits of info are our predictors. For example, let’s combine our age data with info on how many years everyone has been out of undergraduate. For the purposes of this example, I’m just going to make this data up: tehran &lt;- 13 nabiha &lt;- 5 james &lt;- 2 chris &lt;- 1 margaret &lt;- 3 emily &lt;- 1 sarah &lt;- 4 nate &lt;- 2 tyra &lt;- 1 julia &lt;- 3 angela &lt;- 4 allie &lt;- 4 daniel &lt;- 5 sierra &lt;- 3 # create a vector of years since UG: yearsUG &lt;- c(tehran, nabiha, james, chris, margaret, emily, sarah, nate, tyra, julia, angela, allie, daniel, sierra) Lets combine ages to yearsUG into a single data_frame and create a scatterplot: model_df &lt;- data_frame(ages, yearsUG) # plot age as a function of yearsUG model_plot &lt;- ggplot2::ggplot(model_df, aes(x = yearsUG, y = ages)) + geom_point() model_plot Keeping in mind that our focus this semester will be in linear models. As a simplification, this means that we are looking for an equation of the line that best fits our data (minimizes the resulting SS). The equation for the line that represents the mean would be: \\[Y = mean\\ age + 0 * yearsUG\\] This means years since undergrad does not figure into the mean model (it gets multiplied by zero. This results in a horizontal line with mean age as the y-intercept. model_plot &lt;- model_plot + geom_hline(yintercept = mean(model_df$ages), color = &quot;blue&quot;) model_plot Imagine the difference scores as a straight vertical lines connecting each data point to the means model line. Now compare the means model (blue line) to a model including yearsUG as a predictor (red): model_plot &lt;- model_plot + stat_smooth(color = &quot;red&quot;, method = &quot;lm&quot;, se = F) model_plot As you may have intuited the lm in the above call stands for “linear model”. Congrats, you just plotted a linear regression!!!! The se adds an error ribbon around the regression line. In this case I used F or FALSE to turn it off. What’s important here is that from a visual inspection the yearsUG model provides a better fit. Why, because it’s reduced the resulting difference scores (of SS). The sum of the distances from each point to this line with yearsUG as a predictor (red line) is less than the means model (blue line). This is what we mean by “accounting for varience”. The yearsUG model accounts for more varience than the means model—why because it reduces the SS. A critical question that we strive to answer with our statistical tests (like the F-test) is whether the model using additional predictors accounts for more variance above a critical threshold that it may be deemed significant, or unlikely due to chance. As I mentioned, this is something that will become critically important in a few weeks. For now I just wanted to make this connection to your understanding of the role of the mean while it was still fresh. "],
["sampling-distributions-and-building-the-logic-of-nhst.html", "Week 3 Sampling distributions and building the logic of NHST 3.1 The sampling distribution 3.2 Standard error of the mean 3.3 The difference distribution 3.4 The Null distribution 3.5 Probabilty of observed differences 3.6 A note about sample size", " Week 3 Sampling distributions and building the logic of NHST So this week is typically one of the more confusing weeks for those that do not have much background in stats and programming because I’m asking you to do two conceptually demanding things. From the stats side: I’m asking you to imagine that the sample data that you collect is not sui generis (I always wonder if I’m using that term correctly), but part of a larger theoretical distribution of possible (but materially non-existent) sample data sets. The number of these sets is indefinitely many… which is a way of saying really large without invoking notions of infinity. On the programming side: I’m asking you to build simulations of these theoretical sampling distributions by using a for() loop; programming R to proceed in a recursive, iterative loop for a very larger number of instances. In the class lecture we laid out the logic of the sampling distibution (of means), the difference distribution (of means) and the null distribution (of means). In the workshop we made it as far as simulating the sampling distribution and using it to obtain important measures (e.g., standard error) and walk through and example for() loop that can be used to generate the distribution of means. Here I’m going to extend that work to show how to get both the difference distribution and the null distribution. This inolve minor tweaks of what was done in class. Please follow along by entering the commands into your own console. First let’s load in the appropriate packages for this week: pacman::p_load(psych, tidyverse, multicon) 3.1 The sampling distribution To begin let’s start off with a control population. For the sake of example, let’s imagine that this population represents the obtained IQ scores of students in Tehran J. Davis High School (apparently I’m feeling myself) that were taught using a standard curriculum. The 5000 students (it’s a really big school) in this population have a mean IQ of 103 with a standard deviation of 14. Note that this mean and sd are FACTS… they are true values that exist independent of my measure. Also note that in real work, these truths about the population are rarely known. We can create a simulated population using the follow chunk of code; note that anytime you are generating “random” distibutions or “random” sampling in R you’ll need to input the set.seed(1) command where I do to ensure that you get the same numbers I do: # population of 5000 with an approximate mean = 103 and sd = 14 set.seed(1) pop_basic &lt;- rnorm(n = 5000, mean = 103, sd = 14) When we take a look at what the above simulation generates, we see that the obtained values are sightly different than requested (it is random after all). BUT AGAIN, for the puposes of example these are still the facts of the population. mean(pop_basic) ## [1] 102.9554 multicon::popsd(pop_basic) ## [1] 14.37227 Wait… what’s that multicon::popsd() mess?!?!? Remember that calculating the standard devation of a population is different than calculating for a sample, where the sample SD requires an adjustment for the degrees of freedom. Simply, the population SD uses \\(N\\) in the denominator, where the sample SD uses \\(N-1\\). The sd() fuction calculates sample SD. To get population SD, we can use the function popsd() from the multicon package. From this population, the Princpal of TJD-HS tells us that we can only administer our IQ test to 250 students. This represents our sample: mySample &lt;- sample(x = pop_basic, size = 250) psych::describe(mySample) ## vars n mean sd median trimmed mad min max range skew ## X1 1 250 102.85 14.56 102.18 102.52 15.87 70.36 143.07 72.71 0.17 ## kurtosis se ## X1 -0.46 0.92 hist(mySample) If we run the above code again, we see that we get different measures for mySample: mySample &lt;- sample(x = pop_basic, size = 250) psych::describe(mySample) ## vars n mean sd median trimmed mad min max range skew ## X1 1 250 102.29 14.59 102.64 102.27 14.65 69.19 143.07 73.88 0.01 ## kurtosis se ## X1 -0.44 0.92 hist(mySample) and in neither case is the observed sample mean identical to the true population mean. These differences highlight the notion of Variability due to chance-the fact that statistics (e.g., means, SDs) obtained form samples naturally vary from one sample to the next due to the particular observations that are randomly included in the sample. The term sampling error is used to refer to variability due to chance, in that, the numerical value of a sample statistic will probably deviate (be in error) from the parameter it is estimating. In class we mentioned that we can build a theoretical distribution of sampling error, a sampling distribution of means. Similar to what you accomplished in the workshop, this can be done with a for() loop that does the following: pull a sample of a given size from the population get the mean of that sample save that sample mean to a vector repeat 1-3 How many times do we repeat? Theoretically an infinite number of times. Because we can’t wait for infinity, lets just use a really big number like 100,000 (or 1e+05) Buiding the sampling distribution (using pop_basic from above): # create an empty vector to store the sample means sampling_distribution &lt;- vector() # use set.seed to ensure our numbers match: set.seed(1) # start our loop at 1 and stop it at 100,000 for (i in 1:1e+05) { # 1. get a sample of 250 students from pop_basic: mySample &lt;- sample(x = pop_basic, size = 250) # 2. get the mean of the obtained sample: mySample_mean &lt;- mean(mySample) # 3. save that mean to our vector, sampling_distribution in the i-th location. # (Think of this like telling R what line of the paper to write on so that you # don&#39;t overwrite your data) sampling_distribution[i] &lt;- mySample_mean # 4. everything is repeated in the brackets until i=100000 } Lets take a look at our sampling distribution: hist(sampling_distribution, main = &quot;Theoretical sampling distribution&quot;) and the grand mean of the sampling distribution (the mean of means if you will): mean(sampling_distribution) ## [1] 102.9521 Again not quite the TRUE mean of the population, but pretty darn close. 3.2 Standard error of the mean We also noted that the standard error (SE) of the mean is simply the standard deviation of the sample distribution. That is, how much does the mean vary due to chance provided repeated sampling. In this case it can be found by: sd(sampling_distribution) ## [1] 0.8837669 However, deriving the SE in this way assumes that we are free to sample and re-sample the population a large number of time (in this case 100 thousand) when in reality we may have neither the time, resources or inclination to do so. Instead, we use the following equation to provide an estimate of SE provided our sample: \\[SE=\\frac{SD_{sample}}{\\sqrt{N_{sample}}}\\] So in this case, for a given mySample from the population pop_basic: set.seed(1) mySample &lt;- sample(x = pop_basic, size = 250) mySample_se &lt;- sd(mySample)/sqrt(250) show(mySample_se) ## [1] 0.8958658 Compared to the theoretically derived SE above, the estimate is not too far off (0.895 v. 0.884). In class, we went a step further and estimated SE for each individual sample in our distribution and compared the mean estimated-SE to the stardard deviation of the entire distribution. Note how this changes our steps slightly: # create an empty vector to store the sample means create another to store sample # SEs sampling_distribution &lt;- vector() sample_ses &lt;- vector() # use set.seed to ensure our numbers match: set.seed(1) # start our loop at 1 and stop it at 100,000 for (i in 1:1e+05) { # 1. get a sample of 250 students from pop_basic: mySample &lt;- sample(x = pop_basic, size = 250) # 2. get the mean of the obtained sample; also get the estimated SE: mySample_mean &lt;- mean(mySample) mySample_se &lt;- sd(mySample)/sqrt(250) # 3. save that mean and se the respective vectors: sampling_distribution[i] &lt;- mySample_mean sample_ses[i] &lt;- mySample_se # 4. everything is repeated in the brackets until i=100000 } and now the SE calculated from the distibution compared to the mean estimated SE: sd(sampling_distribution) ## [1] 0.8837669 mean(sample_ses) ## [1] 0.908183 Again, not identical, but pretty close. 3.3 The difference distribution Imagine for the sake of example, we are interested in IQ. A central debate (as I understand the literature) at present is whether IQ is inherently static or can be improved with education. Indeed the implication of this can be quite controversial (IQ testing is like the third rail of understanding intelligence… making statements about intelligence is like the third rail of psychology… but I digress). In addition to our population of students taking the basic cirriculum, we also have an equal number of students taking an advanced cirriculum. Assuming that we have already controlled for other factors (e.g., baseline IQs from both groups the same before treatment) we would like to address the following question using data obtained from each group: “Does the enhanced cirriculum result in higher IQ?” First let’s create our populations: set.seed(1) pop_basic &lt;- rnorm(n = 5000, mean = 103, sd = 14) pop_advanced &lt;- rnorm(n = 5000, mean = 108, sd = 13) Again, you’ll note that in this example we’re omnipitent and know the truth of the matter (the true measures)… in the real world no one is all-seeing and knowing. You’ll also notice that I kept the SD for each population roughly equivalent. This homogeniety of variences (i.e., SDs should be approximately equal) is another assumption of our non-parametric tests (like ANOVA). Here I’ve made it so, but if in true the variences are not equal then we may have to adjust how we proceed with our analyses. More on that in a few weeks! And now to create our difference distribution using the same programming logic as above: # create an empty vector to store the difference in means difference_distribution &lt;- vector() # use set.seed to ensure our numbers match: set.seed(1) # start our loop at 1 and stop it at 100,000 for (i in 1:1e+05) { # 1. get a sample of 250 students from pop_basic, then get a sample from # pop_advanced: mySample_basic &lt;- sample(x = pop_basic, size = 250) mySample_advanced &lt;- sample(x = pop_advanced, size = 250) # 2. get the means of the obtained samplse; mySample_basic_mean &lt;- mean(mySample_basic) mySample_advanced_mean &lt;- mean(mySample_advanced) # 3. save differences between means to our vector, difference_distribution in the # i-th location. difference_distribution[i] &lt;- mySample_advanced_mean - mySample_basic_mean # 4. everything is repeated in the brackets until i=100000 } And now to take a look at the resulting difference distribution: mean(difference_distribution) ## [1] 4.921686 hist(difference_distribution, main = &quot;Theoretical difference distribution&quot;) As anticipated (because we know all) we end up with a difference distribution with a mean difference of about 5. What we are asked to consider is whether this difference (or any observed difference, really) is enough to say that the likelihood of it occuring by chance is sufficiently small (i.e. is \\(p &lt; .05\\)). For this we need to create a null distribution. 3.4 The Null distribution The logic of constructing a null distribution rests on the claims made by the null hypothesis, that the means of the two (or more) populations in question are identical: \\[\\mu_1=\\mu_2\\] In a larger sense, if the null hypothesis is indeed true it suggests that the two populations may indeed be one single population. Here, we know that the null hypothesis is NOT true (again, all seeing and knowing). mean(pop_advanced) ## [1] 107.8715 mean(pop_basic) ## [1] 102.9554 # are the 2 means exactly equal? mean(pop_advanced) == mean(pop_basic) ## [1] FALSE In the context of our everday experiments, we do not know whether the null is indeed true or not. Based upon the samples that we take we make an inference as to whether we have sufficient evidence to reject the null. We do so if the probability of our observed data, given the null is true, is sufficiently low. In other words, here we are asking: “IF the null is true, THEN what is the likelihood that we would get our observed differences in means. If the likelihood is low, then we may have reason to believe the null hypothesis to be suspect. Returning to the logic of the null distribution, it assumes that our experimental group, pop_advanced, and our control group, pop_basic are from the same population. Thus, any samples obtained betwixt should be equal, and the differences in sample means should approximate 0 (never exactly 0 due to sampling error). However, by virtue of our experimental design we are implicitly assuming that, in fact the two are different from one another. This implicit assumption means that the null distribution cannot be created by creating a difference distribution between the two groups. So how to go about the business of creating a null distribution, then? We create a difference distribution that guarentees that all data is from the same population. In this case we are creating a difference distribution where we take repeated samples from the same population and compare their differences. Based on this distribution, we can make claims regarding the probability of observed differences between scores assuming they are from the same population. Again sampling error guarentees that these differences are not exactly zero. Typically the population we choose is our control population (group). In this case the logical control is pop_basic. Modifying the for() loop that we used for the difference distribution, we create a null distribution like so: # create an empty vector to store the difference in means null_distribution &lt;- vector() # use set.seed to ensure our numbers match: set.seed(1) # start our loop at 1 and stop it at 100,000 for (i in 1:1e+05) { # 1. get a sample of 250 students from pop_basic, then get another 250 students # from pop_basic: mySample_basic_1 &lt;- sample(x = pop_basic, size = 250) mySample_basic_2 &lt;- sample(x = pop_basic, size = 250) # 2. get the means of the obtained samples; mySample_basic_1_mean &lt;- mean(mySample_basic_1) mySample_basic_2_mean &lt;- mean(mySample_basic_2) # 3. save differences betwwen means to our vector, difference_distribution in the # i-th location. null_distribution[i] &lt;- mySample_basic_1_mean - mySample_basic_2_mean # 4. everything is repeated in the brackets until i=100000 } And now to take a look at the resulting null distribution: mean(null_distribution) ## [1] -5.269745e-05 hist(null_distribution, main = &quot;Theoretical null distribution&quot;) In class we made mention that due to the assuption of equal means and equal variences that the null distribution should approximate the form of the standard distribution were \\(\\mu=0\\) and \\(\\sigma = 1\\). This fact allows us to say something about the probability of an observed difference as a probability of obtaining a Z-score that far removed from 0. 3.5 Probabilty of observed differences Continuting on from the previous section, we have our null distribuition of IQ scores, appropriately named null_distribution. This is the theoretical distribution of differences assuming the two populations are the same. Simply put, this is the distribution of differences we might expect if our advanced cirriculum did not have any impact. You’ll note that even assuming that the two groups are identical still yeilds some pretty large differences by chance: max(null_distribution) ## [1] 5.751995 min(null_distribution) ## [1] -5.118378 but that the probability of those extremes is very, very low (but still exists which is why we never prove the alternavtive hypothesis). Let’s say I take a sample from pop_basic and pop_advanced and find that my means differ by 3. Given the null distribution, what is the probability that I will observe a difference this large? To derive this probability, we take advantage of what we know about the standard distribution, where we have a known probability of obtaining any given Z-score. The process of answering the above is as follows: 1. transform the observed difference to a Z-score using the mean and SD of the null distribution; 2. calculate the probability of that extreme of a score. Here is step 1: # 1. convert the observed difference to a z-score: Zscore &lt;- (3 - mean(null_distribution))/sd(null_distribution) show(Zscore) ## [1] 2.390864 I want to pause here to note that the obtained Z-score is positive. As a result the score is on the right side of the distribution. Our probability function, pnorm() returns the probability of an obtained score or lower (the cummulative probability). However, we want the probability of the observed score or more extreme. In this case more extreme means greater than. Conversely if the observed Z score is less than 0, then more extreme means less than. I bring this up as this determines what value we use to calculate the desired probability. In the latter case, less than we can simple take the output of pnorm() and call it a day. However, in cases like ours, when \\(Z&gt;0\\) we need to use 1-pnorm(). OK, on to the calculation: Zscore &lt;- (3 - mean(null_distribution))/sd(null_distribution) 1 - pnorm(Zscore) ## [1] 0.008404386 Based on a criteria of rejecting the null if \\(p&lt;.05\\) we would reject here. BUT NOTE: not necessarily because it’s less than .05. Remember that if I have a two-tailed test, my criteria requires that I split my \\(\\alpha\\). So in truth I need to obtain a value less than .025 in either direction. 3.6 A note about sample size So I got a pretty low p-value, but you might say it was expected given that my sample size was 250 for each group. Let’s see what happens if I drop the sample size down to 10 students per group, creating a new null distribution, null_distribution_10: # create an empty vector to store the difference in means null_distribution_10 &lt;- vector() # use set.seed to ensure our numbers match: set.seed(1) # start our loop at 1 and stop it at 100,000 for (i in 1:1e+05) { # 1. get a sample of 250 students from pop_basic, then get another 250: mySample_basic_1 &lt;- sample(x = pop_basic, size = 10) mySample_basic_2 &lt;- sample(x = pop_basic, size = 10) # 2. get the means of the obtained samplse; mySample_basic_1_mean &lt;- mean(mySample_basic_1) mySample_basic_2_mean &lt;- mean(mySample_basic_2) # 3. save differences betwwen means to our vector, difference_distribution in the # i-th location. null_distribution_10[i] &lt;- mySample_basic_1_mean - mySample_basic_2_mean # 4. everything is repeated in the brackets until i=100000 } What does this distribution look like? hist(null_distribution_10, main = &quot;Theoretical null distribution&quot;) mean(null_distribution_10) ## [1] -0.01377723 And let’s take a look at the probability of an observed difference of 3 in this case. Not that I’m using the pnorm() shortcut so I don’t need to calculate my Z by hand: 1 - pnorm(q = 3, mean = mean(null_distribution_10), sd = sd(null_distribution_10)) ## [1] 0.3187986 As we can see, size matters. Given that we are omnipotent in this case, we know the truth of the matter is that the two populations are in fact different, and that the mean of pop_basic is not equal to the mean of pop_advanced. Thusm what we have here is a failure to reject the null hypothesis when it is indeed false. Hmm, what kind of error is that again? This last example is going to figure prominently in our discussions of power and what power is, properly defined. For now, I think this is a good place to stop this week. For additional practice, walk through and attempt to recreate (in your own notebook) the week3_inclass.Rmd located on BOX in this weeks workshop folder. Be sure that you understand the logic of your steps, and how I’ve answered the questions presented in the file. This should be good prep for your homework. "],
["probability-and-the-binomial-distribution.html", "Week 4 Probability and the Binomial Distribution 4.1 A few words on where we are at so far… 4.2 Okay on to this week’s example analyses: 4.3 Classic example: The coin flip 4.4 Changing the parameters 4.5 Catching a cheat 4.6 One last thing… Permutations, combinations, and building functions", " Week 4 Probability and the Binomial Distribution 4.1 A few words on where we are at so far… Last week we introduced sampling distributions and importantly the Null sampling distribution. Most importantly we talked about how the null sampling distribution allows us to make assessments on our observed data relative to the Null hypothesis. To remind ourselves here, the null hypothesis assumes that data collected from two or more samples comes from the same population. As such, the descriptive statistics of each of those samples should be nearly identical within a certain amount of tolerance. A key point in our understanding, is that the null distribution (of mean differences) should approximate the standard distribution, with a mean \\(\\approx\\) 0, and a SD \\(\\approx\\) 1. As we pointed out a few lectures back, we can do wonderful things with this distribution, including assessing the probability of an obtained difference value assuming the null hypothesis is true (see last week’s write-up). This week, we unpack further notions of probability and how they may be used to allow us to make assessments on important aspects of our data… and further how that may allow us to make decisions about our unterlying statistical (and further, theoretical) assumptions. For the next few weeks, these two critical assumptions about the nature of our observed data will be paramount: that the data (its distribution) is randomly obtained from a normal population. that our measures of the data (including both the tools we use and the resulting desciptions of the data) are independent of one another. We’ve spent some time at length with Assumption #1. What do I mean in Assumption #2? Here’s an example that I use in my Perception course: Say I step on a scale and it says that I’m 180 lbs. I then walk over to a stadiometer and get my height measured and it tells me I’m 71 inches tall. Right after this I return to the original scale. If the measurement of my weight and height were independent of one another, then the scale should again read 180 lbs. However, if in some stange reality the measurement of my height changes the measurement of my weight, then all bets are off—the second time on the scale it could say I’m 250lbs, 85lbs, who knows. One of the lessons of quantum mechanics (see Stern-Gerlach experiments) is that we do live in that strange reality—at least at the quantum level. Most of you have probably encountered the uncertainty principle which is the physical manifestation of this issue. More pertainent for us consider the example in class: We have a theory that adults with children are more likely to wear seatbelts than adults without children. Our null hypotheis is that adults with children are no more likely to wear seatbelts than adults without children. Here, everyone is being measured in two ways, 1. the seatbelt measurement (do you wear a seatbelt, yes or no?) and 2. the parenthood measurement (do you have a child, yes or no?). If the two measures are independent of one another, then the seatbelt measurement should not have an effect on the parenthood measurement and vice versa. If they do, then the measures are not independent of one another. Of course, in this case, it doesn’t mean that the being measured for seatbelt-wearing may instantaneously turn you into a parent (Yikes), but it does mean that being identified as a parent may increase the probability that you are a seatbelt wearer and vice versa. Hopefully from this example, you now see the relationship between the null and research (alternative) hypotheses—the research hypothesis is structured in such a way that it assumes a violation of the null, in this case a violation of the independence assuption. In many cases we will be testing whether the evidence supports such a violation. So returning to why probability is so important, the probabilities related to the null distribution have been worked out by women and men far smarter than I. THESE PROBABILITIES ARE KNOWN and tell us the likelihood that our null assumptions have not been violated given our present set of data. 4.2 Okay on to this week’s example analyses: For the following examples, we will be using functions that come pre-installed in R (base library) as well as the tidyverse. Let’s go ahead and install the tidyverse. pacman::p_load(tidyverse) 4.3 Classic example: The coin flip Let’s make a wager. I’ll let you flip a single coin 10 times, and if you get 6 or more heads I’ll give you $250. If not, you have to give me $250. Should you take this bet? To answer this question you’ll want to assess the likelihood of getting an observed outcome, in this case \\(Heads \\geq 6\\), provided a sample size (10). We assume that the likelhood of getting Heads on any single flip is .5, (assuming that the coin is far and not a cheat). We also assume that the coin flips are independent of one another, that is any single flip does not influence or change the likehood of an outcome on subsequent flips. In light of our discussion in the opening section, I hope you see that depending on the outcome, it may cause us to re-evaluation these assumptions. For now, let’s assume that both are indeed true and that I’m not a cheat. 4.3.1 How many heads would we get if we flipped a fair coin 10 times? In class we noted that with a known sample space and a known sample size we can calculate the expected outcome. In this case our sample space is {Heads, Tails} and our sample size is 10 (flips). The probability of observing a Heads in our sample space is .5. Therefore: \\[expected\\ Heads = p(Heads) \\times N(flips)\\\\ expected\\ Heads = .5 \\times 10 = 5\\] Of course, reality does not often abide our expectations, and given what we know about sampling error you’re more likely to not get exactly 5 flips than you are. So, what we’re really interested in is what is the likelihood of our expectation, and fortunately for us, over the long-run independent/random outcomes can be predicted probabilistically. There are several ways to address this in R. 4.3.1.1 Simulation method The first method involves running a large number of simulations and assessing the probability based upon the outcomes. To run a single simulation, you can use the rbinom() function. Below I am running a simulation of 1 coin (n), with a sample size of ten flips (size), and a known (or really assumed) probability of .5 (prob). # set our seeds to get the same numbers: set.seed(1) # n = number of coins size = number of flips prob(ability) rbinom(n = 1, size = 10, prob = 0.5) ## [1] 4 In this simulation you got 4 flips and I’m off to cop a new pair of retro Jordans with your money… or I suppose if I’m being sensible, you’ve paid for this week’s daycare. In any event, you lost. But we know that not much can be learned about the likelihood of an outcome with a sample size of 1. Let’s try running this scenerio using a simulation of 100K coins. To run this simulation, you could: run the above 100K times, write down the number (yea, sure bud) build a for() loop (like you did last week) run it in a single line by modifying the rbinom() call: set.seed(1) numberHeads &lt;- rbinom(n = 1e+05, size = 10, prob = 0.5) Let’s plot this using ggplot(): # must convert vector to a data frame numberHeads_df &lt;- data_frame(numberHeads) p &lt;- ggplot2::ggplot(data = numberHeads_df, aes(x = numberHeads)) + geom_histogram(binwidth = 1, boundary = -0.5, fill = &quot;red&quot;, col = &quot;grey&quot;) + scale_x_continuous(limits = c(0, 10), breaks = 1:10) show(p) As you can see in the plot above, we get exaclty 5 heads a little less than 25K out of 100K, or about 25% of the time. That’s the eyeball test, How might we go about obtaining the exact probabilities from this simulation? We can take advantage of logical statements. Logicals produce outputs that are either TRUE or FALSE. More, and this is terribly useful for us, R also reads those TRUE or FALSE outcomes as 1 or 0 where TRUE=1 and FALSE=0. Take the following vector for example: ## [1] TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE R reads this as a numeric: ## [1] 1 1 0 0 0 1 0 0 1 0 Hopefully, its apparent now that you can get info about the number of TRUEs by performing mathematical operations on this vector. For example, above you see that there are 4 TRUEs out of 10 possible. That means that we get TRUE 40% of the time. We can obtain this probability in R using the mean() function. How? Remember that the mean is the sum of scores divided by the number of scores. In this case the sum is 4; the number of scores is 10; and 4/10 is .40. Returning to our simulation of 100K samples, numberHeads, we can do the following to get the probability of 5 (see Demos, Ch. 3 on logicals for explanation of operators like ==): mean(numberHeads == 5) ## [1] 0.24477 So the probability of getting exactly 5 heads is 0.245. Returning to our wager, based on our simulation what’s the probability of getting 6 or more heads? mean(numberHeads &gt;= 6) ## [1] 0.37743 About 38%. You probably shouldn’t take my bet. 4.3.1.2 Non-simulation method Simulations are useful for helping us visualize our scenerios and use pseudo real data to test our underlying assumptions. But most times you just want the answer… in fact often that will suffice because as I mentioned, in most normal circumstances the probabilities have already been worked out. How can we get the results above without simulating 100K coins? By using two functions in R that belong to the same family as rbinom(): dbinom() and pbinom(). dbinom() returns the exact probabilty of an outcome, provided a generated probility density function. It takes in arguments related to the numnber of successful outcomes (x), the sample size (size) and the independent probability of a sucessfull outcome (prob). So the probability of 5 Heads (successes) on 10 flips with a fair coin would be entered: dbinom(x = 5, size = 10, prob = 0.5) ## [1] 0.2460938 We see that this value is pretty close to our simulated outcome, confirming that the simulation was indeed correct, if not entirely necessary. We can also use this function to build a table for the individual probabilities of all possible outcomes. To see how, first lets consider the space of possible outcomes in this scenerio. At one extreme, I could flip a coin 10 times and get no Heads. At the other I could get all Heads. So the set of possible outomes can be express as a sequence from 0 to 10. Recall that you can create such a sequence using the : operator: 0:10 ## [1] 0 1 2 3 4 5 6 7 8 9 10 With this in mind you can modify the previous code like so: dbinom(x = 0:10, size = 10, prob = 0.5) ## [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250 ## [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250 ## [11] 0.0009765625 The output gives be the resulting probabilities in order from 0 Heads to 10 Heads. Let’s create a table to make this easier to digest. First I’m going to create a vector of numberHeads to show all possbilites from 0 to 10. Second, I will run dbinom() as above to test each possibility, saving that to a vector probHead. Finally I will combine the two into a data frame called probTable: # 1. range of possibilites numberHeads &lt;- 0:10 # 2. prob of outcome probHead &lt;- dbinom(x = numberHeads, size = 10, prob = 0.5) # 3. combine to data frame probTable &lt;- data_frame(numberHeads, probHead) # 4. Show the data frame (table) show(probTable) ## # A tibble: 11 x 2 ## numberHeads probHead ## &lt;int&gt; &lt;dbl&gt; ## 1 0 0.000977 ## 2 1 0.00977 ## 3 2 0.0439 ## 4 3 0.117 ## 5 4 0.205 ## 6 5 0.246 ## 7 6 0.205 ## 8 7 0.117 ## 9 8 0.0439 ## 10 9 0.00977 ## 11 10 0.000977 Contrats you’ve just created a Binomial Distribution Probability Table for this scenerio. Note that you’re asked to do the same in this week’s homework! (Disregard the single line for() method mentioned in the homework. It dawned me that this would be a way to do it without for loops!) Returning to our wager of 6 or more heads, we would use pbinom(). pbinom() returns probabilities according to the cumulative density function, where the output is the likelihood of obtaining that score or less. Note that pbinom() takes similar arguments to dbinom() but asks for q instead of x. For our purposes q and x are the same thing… number of outcomes. So for example the probability of obtaining 5 or less Heads provided our scenerio may be calculated: pbinom(q = 5, size = 10, prob = 0.5) ## [1] 0.6230469 Given what we know about the Compliment Law, we can compute the probability of 6 or more Heads as: 1 - pbinom(q = 5, size = 10, prob = 0.5) ## [1] 0.3769531 which again matches with our simulation. 4.4 Changing the parameters So obviously if the coin is a fair coin, your shouldn’t take the bet. Let’s imagine that we kept the same wager, but to entice you to bet, I tell you that this coin lands on Heads 65% of the time? Should you take the bet? To test this new sceneria all you need to do is change the probability parameter. Let’s just skip the simulations and just assess using dbinom() and pbinom(). 4.4.1 Constructing a table of outcome probabilities: As before we’ll use dbinom() to create a table, simply modifying the prob argument to .65: # 1. range of possibilites numberHeads &lt;- 0:10 # 2. prob of outcome probHead &lt;- dbinom(x = numberHeads, size = 10, prob = 0.65) # 3. combine to data frame probTable &lt;- data_frame(numberHeads, probHead) # 4. Show the data frame (table) show(probTable) ## # A tibble: 11 x 2 ## numberHeads probHead ## &lt;int&gt; &lt;dbl&gt; ## 1 0 0.0000276 ## 2 1 0.000512 ## 3 2 0.00428 ## 4 3 0.0212 ## 5 4 0.0689 ## 6 5 0.154 ## 7 6 0.238 ## 8 7 0.252 ## 9 8 0.176 ## 10 9 0.0725 ## 11 10 0.0135 How does this table compare to the one above? 4.4.2 Assessing cummulative probability And now, to get the probability of 6 or greater: 1 - pbinom(q = 6, size = 10, prob = 0.65) ## [1] 0.513827 Ahhh… the odds are ever so slightly in your favor. What if we changed the bet: you get 60 Heads out of 100 flips? 1 - pbinom(q = 60, size = 100, prob = 0.65) ## [1] 0.827585 Yea, you should really take that bet! Fortunately for me, I wasn’t born yesterday. How about 12 out of 20? 1 - pbinom(q = 12, size = 20, prob = 0.65) ## [1] 0.6010266 Nope, I’m not taking that bet either. Does 3 out of 5 interest you? 1 - pbinom(q = 3, size = 5, prob = 0.65) ## [1] 0.428415 4.5 Catching a cheat OK. Last scenerio. Let’s imagine that I am not a total sucker, and we reach a compromise on “30 or more Heads out of 50 flips”. You run to your computer and calculate your odds and like your chances (maybe I am a sucker)! 1 - pbinom(q = 30, size = 50, prob = 0.65) ## [1] 0.7264363 You flip 50 times but only get 27 heads. Astounded, because those odds really were in your favor, you label me a liar and a theif. Are you justified in doing so? This scenerio essentially captures our discussion at the outset, how far of a deviation warrants us being skeptical that our original assumptions were true. In this case the original assumptions were that 1. each coin flip is independent and 2. the independent probability of getting a Heads is 0.65. We typically set our threshold at \\(p&lt;.05\\), which remember for a two tailed test means that we are on the lookout of extreme values with a \\(p&lt;.025\\). So essentially we are asking if the probability of obtaining exeacty 27 Heads given this scenerio is less than 2.5%: dbinom(x = 27, size = 50, prob = 0.65) ## [1] 0.03132011 You Madam/Sir have besmirched my honor! OK, well is 27 isn’t enough, then how low (or high) do you need to go to pass the critical threshold? To answer this we need to construct a table of probabilities: # 1. range of possibilites numberHeads &lt;- 0:50 # 2. prob of outcome probHead &lt;- dbinom(x = numberHeads, size = 50, prob = 0.65) # 3. combine to data frame probTable &lt;- data_frame(numberHeads, probHead) # 4. Show the data frame (table) show(probTable) ## # A tibble: 51 x 2 ## numberHeads probHead ## &lt;int&gt; &lt;dbl&gt; ## 1 0 1.60e-23 ## 2 1 1.48e-21 ## 3 2 6.75e-20 ## 4 3 2.01e-18 ## 5 4 4.38e-17 ## 6 5 7.48e-16 ## 7 6 1.04e-14 ## 8 7 1.22e-13 ## 9 8 1.21e-12 ## 10 9 1.05e-11 ## # ... with 41 more rows One could visually inspect the entire table noting which outcomes have a corresponding probability less than .025. But the whole point of learning R is to let it do the heavy lift for you. In this case you can ask R: “Hey, R. Which outcomes in have a probability less than .025” or more exactly: “Which rows in my data frame have a probHead less than .025” This is accomplished using the which() function: which(probTable$probHead &lt; 0.025) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 40 41 42 43 44 45 46 47 48 49 50 51 This gives us indices of all of the rows that meet this criteria. BUT KEEP IN MIND, our list of possible outcomes started at 0 not 1. But there’s no such thing in R as row 0 (now Python on the other hand…). That means in the output above that 1 actually refers to 0 Heads, 2 to 1 Head and so on. You could stop here and just know that you need to subtract the above output by 1 to get the correct result. Or you could address this in R in the following ways: First, the quick/dirty/limited/bad way. Keep in mind this only works because we know that we can apply a subtraction rule: which(probTable$probHead &lt; 0.025) - 1 ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## [24] 23 24 25 26 39 40 41 42 43 44 45 46 47 48 49 50 The correct below way will work for cases in which adjusting the index is unknown. Recall from Demos Chapter 2 that you can get a specific value from a vector or data frame by using its index using the [] operator such that [row,column]. Think of this like how you would get a certain cell in Excel. For example the get the 17th row of our data frame probTable: probTable[17, ] ## # A tibble: 1 x 2 ## numberHeads probHead ## &lt;int&gt; &lt;dbl&gt; ## 1 16 0.00000157 Note that if you want all rows or columns, you leave that index blank (as I did above where I wanted both columns). Keeping in mind that which(probTable$probHead&lt;.025) gave us a vector of indecies, we can rewrite the previous chunk like so to get the important rowns in a data frame structure: crticalValues &lt;- which(probTable$probHead &lt; 0.025) probTable[crticalValues, ] ## # A tibble: 39 x 2 ## numberHeads probHead ## &lt;int&gt; &lt;dbl&gt; ## 1 0 1.60e-23 ## 2 1 1.48e-21 ## 3 2 6.75e-20 ## 4 3 2.01e-18 ## 5 4 4.38e-17 ## 6 5 7.48e-16 ## 7 6 1.04e-14 ## 8 7 1.22e-13 ## 9 8 1.21e-12 ## 10 9 1.05e-11 ## # ... with 29 more rows Again this would ask us to look through the whole table, but we can apply the same logic to just show us the corresponding numberHeads as a vector (Remember that you’ll need to specify the data frame when refering to the columns): crticalValues &lt;- which(probTable$probHead &lt; 0.025) probTable$numberHeads[crticalValues] ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## [24] 23 24 25 26 39 40 41 42 43 44 45 46 47 48 49 50 Note that since vectors don’t properly have rows or columns, you just use a single value in the [ ]. So, if you got 1 less head, then you can call me a cheat. Conversly if you managed to get 39 heads or above, I might have reason to believe that you are somehow gaming me. 4.6 One last thing… Permutations, combinations, and building functions In class we noted the equations for calculating permutations and combinations. For example, permutations: \\[P^N_r=\\frac{N!}{(N-r)!}\\] So assuming that I have for colored balls (green, white, red, blue) how many possible permutations assuming I only draw two? Here we can use the factorial() function: N &lt;- 4 # number total r &lt;- 2 # number draw factorial(N)/factorial(N - r) ## [1] 12 Before leaving you, I want to note one more thing that you is possible in R… constructing your own functions. This might be especially useful when you have a operation that you are performing over and over again that has to built in function. Functions take the general format: myfunction &lt;- function(input parameters seperated by commas){ do something to the input parameters return(output) } For example, with permutations, I can create a function, let’s call it pFunc that takes N and r as input parameters and performs the operation for me: pFunc &lt;- function(N, r) { factorial(N)/factorial(N - r) } After you run these lines of code, pFunc is saved to your environment. From here on out we can simply type: # four balls, draw 2 pFunc(N = 4, r = 2) ## [1] 12 # four balls draw 4 pFunc(N = 4, r = 4) ## [1] 24 # 100 balls, draw 8 pFunc(N = 25, r = 8) ## [1] 43609104000 Given what we’ve just done, try to construct a function for Combinations, given that: \\[C^N_r=\\frac{N!}{r!(N-r)!}\\] OK… signing out for today. "],
["chi-square-and-associated-methods.html", "Week 5 Chi-square and associated methods 5.1 Goodness of fit test 5.2 Contingency Table Analysis in R 5.3 Effect Sizes and Odds Ratios 5.4 Fisher’s exact test 5.5 Cohen’s kappa", " Week 5 Chi-square and associated methods This week we will be working through how to conduct and report Chi-square, \\(\\chi^2\\), analyses in R. As mentioned in class we can use \\(\\chi^2\\) in several ways: to assess goodness of fit (assuming all is fair), to test for independence, to compare models (see you next semester), and a derivative of the Chi-square logic to test for inter-rater reliability. In what follows we will walk through each of these scenarios, focusing on how to conduct the analyses in R. For each analysis we will also provide examples of how to construct frequency tables depending on what information is available to you. In some cases you are already provided with pre-tallied frequencies, in other cases you may be given raw data and need to create the tallied frequencies yourself. I realize in class I walked you through several methods of how get the appropriate results. Here I’m just going to focus on the most simple and efficient way to do this. Here are the packages we’ll be using: pacman::p_load(tidyverse, kableExtra, # creating html tables for this page epitools, # calculating odds ratios lme4, # for the comparing models example vcd, # Cohen&#39;s kappa gmodels # makes obsolete a lot of our workshop ) Also, if you are using a newer version of R, you’ll need to install zoo. vcd depends on zoo to work. If prompted in your Console with the question: “Do you want to install from sources the package which needs compilation?”, type in “no”: pacman::p_load(zoo) # this is a package that needs to be installed, but needs a response from you to do so. 5.1 Goodness of fit test The Goodness of fit test examines how “close“ observed values are to those which would be expected if things were ‘fair’ or equal. Measures of goodness of fit test to see if the discrepancy between observed and expected values is a significant discrepancy using the following formula: \\[\\chi^2 = \\sum{\\frac{(O-E)^2}{E}}\\] Where \\(O\\) and \\(E\\) refer to the observed frequency and expected frequency respectively. 5.1.1 What to do if you are given a table with tallies In class I asked you to consider the data from your Howell text, Table 6.1, where the data are taken from a study testing therapeutic touch as a “real” phenomena. Therapeutic touch postulates that one may heal a patient by manipulating their “human energy field”. If this is true, and if patients are indeed sensitive to therapeutic touch, then we would expect that patients should be able to report the effect (position of the healers hand) greater significantly greater than chance. The resulting frequencies and expected were: Correct Incorrect Total Observed 123 157 280 Expected 140 140 280 In this case, the tallies are already done for you. You just need to enter them into R as a frequency table. 5.1.1.1 Creating the frequency table. The first important step is to build a matrix object that contains the table. This can be accomplished using the matrix() function. In this case we need to create a 2×2 matrix to place our four values in. The critical thing to remember when entering your values is that you need to go in order either down columns or across rows. By default R prefers you to go down columns, so we’ll just do that here. For simplicity’s sake let’s just create an object to store these values. Since we are doing down columns, the entry order should be 123,140,157,140,280,280. table_values &lt;- c(123, 140, 157, 140, 280, 280) We can then create matrix using the matrix() function with the following arguments: data: the values you are putting into the matrix, in this case table_values nrow or ncol: the number of rows or columns in the matrix; you only need to enter one or the other byrow: TRUE or FALSE, will the values be entered into the matrix going across by rows (TRUE) or down by columns (FALSE). The function default is FALSE, meaning data is entered by column. So to get our numbers into a matrix named table_6.1 we: table_6.1 &lt;- matrix(data = table_values, nrow = 2) # no need to enter byrow= as I&#39;m just using the default setting show(table_6.1) ## [,1] [,2] [,3] ## [1,] 123 157 280 ## [2,] 140 140 280 Before moving on, we need to convert this matrix to a table object. This can be accomplished in one fell swoop by adding as.table() to the previous step in a pipe, %&gt;%. Piping was mentioned in the Intro to Tidyverse DataCamp assignment. Basically it says take the previous output and now perform this additional function. Steps that would normally take multiple lines can be done in a single line: table_6.1 &lt;- matrix(data = table_values, nrow = 2) %&gt;% as.table() # the pipe operator %&gt;% is from tidyverse. See Demos show(table_6.1) ## A B C ## A 123 157 280 ## B 140 140 280 Almost there, not we just need to give our table meaningful names, ABC is not going to cut it. This can be accomplished using rownames() and colnames() and assigning a vector of the appropriate names: rownames(table_6.1) &lt;- c(&quot;Observed&quot;, &quot;Expected&quot;) colnames(table_6.1) &lt;- c(&quot;Correct&quot;, &quot;Incorrect&quot;, &quot;Total&quot;) show(table_6.1) ## Correct Incorrect Total ## Observed 123 157 280 ## Expected 140 140 280 And that’s it. Your table is now in R and ready for the next step. 5.1.1.2 Running the Chi-square For a simple Goodness of fit test I suggest using the chisq.test() function. Remember from class that this has several important arguments: x: a vector (1 dimensional) or matrix (2-dimensional: rows, columns) of the observed data p: a vector (1 dimensional) or matrix (2-dimensional: rows, columns) of the expected data expressed as raw numbers, \\(E\\) or as probabilities \\(E/N\\). rescale.p: if you use the raw expected values then this needs to be set to TRUE correct: perform a Yates correction when calculating the \\(\\chi^2\\) So now that we have constructed our table_6.1, I would suggest calculating the \\(\\chi^2\\) as follows: create a matrix of the observed data create a matrix of the expected data submit these matrices to chisq.test(): correction or no correction: I typically prefer to perform the correction, BUT ultimately is unnecessary here as correction only applies to 2 by 2 tables. rescale or no rescale: I typically prefer to enter my raw expecteds and then set rescale.p=TRUE So If I were running the Goodness of fit test based upon the data above I might: # matrix of observed: observed &lt;- matrix(data = c(123, 157), nrow = 1) # matrix of expected: expected &lt;- matrix(data = c(140, 140), nrow = 1) # run the chi-square: chisq.test(x = observed, p = expected, rescale.p = T) ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 4.1286, df = 1, p-value = 0.04216 That said, remember you only need could just list the observeds: chisq.test(x = observed) ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 4.1286, df = 1, p-value = 0.04216 5.1.1.3 Using your built R table You may have noticed that I elected to create my observed and expected objects by entering the data by hand. “Then why on earth did you have us make that damn table?”, you ask? Well I could have also got these values by indexing the table (see Demos, chapter 2). For example: table_6.1[1, 1:2] ## Correct Incorrect ## 123 157 gives me my observeds and table_6.1[2, 1:2] ## Correct Incorrect ## 140 140 gives me my expecteds. Knowing this I could just run the above chisq.test() as: chisq.test(x = table_6.1[1, 1:2]) ## ## Chi-squared test for given probabilities ## ## data: table_6.1[1, 1:2] ## X-squared = 4.1286, df = 1, p-value = 0.04216 5.1.2 Assuming all things are NOT equal Let’s assume that we are dealing with a Goodness of fit that involves three or more categories. For example, taken from Howell Table 6.3 a game of Rock, Paper, Scissors. R-ninjas Below is the code that I use to create the tables you see on this page. Note that the kable() function is what allows pretty web tables: table6.3 &lt;- matrix(c(30, 21, 24, 25, 25, 25), ncol = 3, byrow = TRUE) # I&#39;m a byrow = T person rownames(table6.3) &lt;- c(&quot;Observed&quot;, &quot;Expected&quot;) colnames(table6.3) &lt;- c(&quot;Rock&quot;, &quot;Paper&quot;, &quot;Scissors&quot;) table6.3 &lt;- as.table(table6.3) show(table6.3) # the semicolon here seperates the lines ## Rock Paper Scissors ## Observed 30 21 24 ## Expected 25 25 25 kable(table6.3, format = &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Rock Paper Scissors Observed 30 21 24 Expected 25 25 25 Assuming equal likelihood of a person selecting Rock, Paper, or Scissors I can just run this test as I did in the previous section. Note that in the code below I’m not even bothering with the expecteds because they are assumed equal: observed &lt;- matrix(data = c(30, 21, 24), nrow = 1) chisq.test(x = observed) ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 1.68, df = 2, p-value = 0.4317 BUT WHAT IF MY EXPECTEDS AREN’T EQUAL??? That is, what if I know that people are twice as likely to choose Rock than Paper or Scissors. In this case the respective probabilities would be Rock = .5, Paper = .25, Scissors = .25). I can simply adjust p= in my chisq.test() function. expected_probabilities &lt;- matrix(data = c(0.5, 0.25, 0.25), nrow = 1) chisq.test(x = observed, p = expected_probabilities) ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 3.24, df = 2, p-value = 0.1979 As you can see, \\(\\chi^2\\) has been adjusted to reflect these new probabilities. 5.1.3 Real data is likely NOT pre-tallied In the above examples, the observeds and expecteds were already tallied for you. In the real-world however, you’ll need to do the tallying yourself (or get your RA to do it). Let’s imagine this scenario running a goodness of fit test on a game of Paper, Rock, Scissors, Lizard Spock. This data can be found using this link: prslOutcomes &lt;- prslOutcomes &lt;- read.delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsData/master/prslOutcomes.txt&quot;) Note that you want to be sure that the data object is stored as a data.frame or data_frame and if not convert it to one. Importing a data set from a file typically defaults to a data.frame but here is how to check and convert if necessary: class(prslOutcomes) # check class of object to ensure data.frame ## [1] &quot;data.frame&quot; prslOutcomes_df &lt;- data.frame(prslOutcomes) # convert if necessary, not necessary here but an example And let’s take a look at the data: prslOutcomes ## Outcomes ## 1 Rock ## 2 Paper ## 3 Scissors ## 4 Scissors ## 5 Paper ## 6 Rock ## 7 Scissors ## 8 Lizard Spock ## 9 Paper ## 10 Lizard Spock ## 11 Lizard Spock ## 12 Lizard Spock ## 13 Rock ## 14 Scissors ## 15 Lizard Spock ## 16 Lizard Spock ## 17 Rock ## 18 Rock ## 19 Paper ## 20 Lizard Spock ## 21 Rock ## 22 Lizard Spock ## 23 Rock ## 24 Rock ## 25 Paper ## 26 Rock ## 27 Scissors ## 28 Rock ## 29 Paper ## 30 Rock ## 31 Scissors ## 32 Rock ## 33 Lizard Spock ## 34 Scissors ## 35 Rock ## 36 Rock ## 37 Lizard Spock ## 38 Lizard Spock ## 39 Lizard Spock ## 40 Lizard Spock ## 41 Lizard Spock ## 42 Lizard Spock ## 43 Paper ## 44 Lizard Spock ## 45 Lizard Spock ## 46 Rock ## 47 Scissors ## 48 Scissors ## 49 Scissors ## 50 Lizard Spock ## 51 Rock ## 52 Lizard Spock ## 53 Paper ## 54 Paper ## 55 Scissors ## 56 Lizard Spock ## 57 Scissors ## 58 Lizard Spock ## 59 Paper ## 60 Lizard Spock ## 61 Scissors ## 62 Lizard Spock ## 63 Paper ## 64 Lizard Spock ## 65 Rock ## 66 Rock ## 67 Lizard Spock ## 68 Paper ## 69 Paper ## 70 Paper ## 71 Scissors ## 72 Scissors ## 73 Scissors ## 74 Paper ## 75 Scissors ## 76 Rock ## 77 Rock ## 78 Paper ## 79 Lizard Spock ## 80 Lizard Spock ## 81 Paper ## 82 Paper ## 83 Scissors ## 84 Lizard Spock ## 85 Scissors ## 86 Paper ## 87 Lizard Spock ## 88 Rock ## 89 Rock ## 90 Scissors ## 91 Scissors ## 92 Scissors ## 93 Lizard Spock ## 94 Scissors ## 95 Paper ## 96 Scissors ## 97 Rock ## 98 Rock ## 99 Scissors ## 100 Paper ## 101 Paper ## 102 Lizard Spock ## 103 Rock ## 104 Scissors ## 105 Lizard Spock ## 106 Scissors ## 107 Paper ## 108 Rock ## 109 Lizard Spock ## 110 Rock ## 111 Lizard Spock ## 112 Paper ## 113 Lizard Spock ## 114 Rock ## 115 Lizard Spock ## 116 Lizard Spock ## 117 Rock ## 118 Scissors ## 119 Lizard Spock ## 120 Rock ## 121 Scissors ## 122 Paper ## 123 Rock ## 124 Rock ## 125 Lizard Spock ## 126 Scissors ## 127 Paper ## 128 Scissors ## 129 Rock ## 130 Paper ## 131 Scissors ## 132 Lizard Spock ## 133 Rock ## 134 Rock ## 135 Paper ## 136 Paper ## 137 Rock ## 138 Scissors ## 139 Rock ## 140 Lizard Spock ## 141 Scissors ## 142 Lizard Spock ## 143 Rock ## 144 Paper ## 145 Lizard Spock ## 146 Rock ## 147 Rock ## 148 Scissors ## 149 Paper ## 150 Paper ## 151 Lizard Spock ## 152 Rock ## 153 Scissors ## 154 Scissors ## 155 Lizard Spock ## 156 Paper ## 157 Paper ## 158 Paper ## 159 Scissors ## 160 Scissors In the data set above, each line represents a single play out of 160 total. When dealing with real data you’ll need to get the actual frequency counts that go into the data. There are several ways to get these counts. 5.1.4 getting counts the in-class way (not recommended) We mentioned in class that you can use the plyr::count() function to get tallied data. One of the frustrating things about R is that with so many packages being made, many authors us the same names for their functions. In the case of count() note that the following give you different results: count(prslOutcomes) ## Outcomes freq ## 1 Lizard Spock 45 ## 2 Paper 35 ## 3 Rock 41 ## 4 Scissors 39 dplyr::count(prslOutcomes) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 160 plyr::count(prslOutcomes) ## Outcomes freq ## 1 Lizard Spock 45 ## 2 Paper 35 ## 3 Rock 41 ## 4 Scissors 39 Only plyr::count() gives us the counts per category. Let’s save this to an object called counts: counts &lt;- plyr::count(prslOutcomes) show(counts) ## Outcomes freq ## 1 Lizard Spock 45 ## 2 Paper 35 ## 3 Rock 41 ## 4 Scissors 39 We can simply grab the frequency data from the freq column that plyr::count() has created: chisq.test(x = counts$freq) ## ## Chi-squared test for given probabilities ## ## data: counts$freq ## X-squared = 1.3, df = 3, p-value = 0.7291 5.1.5 getting counts the table() way (my new recommendation) An alternative to the above would be to create a table from your raw data. Given something I just discovered after class yesterday, I think this may be the better way, if for nothing other than being consistent. In this case we create a table from the data frame of raw data using the table() function: table(prslOutcomes) ## prslOutcomes ## Lizard Spock Paper Rock Scissors ## 45 35 41 39 You can simply plug this table into the chisq.test() function: chisq.test(x = table(prslOutcomes)) ## ## Chi-squared test for given probabilities ## ## data: table(prslOutcomes) ## X-squared = 1.3, df = 3, p-value = 0.7291 And if we had prior knowledge that people tend to choose Lizard Spock 50% of the time, Rock 30%, and the remaining two 10% each? would you modify the previous call? 5.2 Contingency Table Analysis in R We use contingency tables to assess the relative independence of 2 categorical variables. Consider the example taken from the Howell text, Table 6.4: Yes No Total Nonwhite 33 251 284 White 33 508 541 Total 66 759 825 The above table assesses whether the frequency of being handed the death penalty (Yes or No) is contingent on race (White or Non-White). In class I walked you through several ways of running the \\(\\chi^2\\) of independence test assuming you are given a table that has both observeds and marginal totals or just observeds. Ultimately this was done to show you how to calculate your expecteds in either scenario and perform the requisite steps flowing from there. BUT BUMP ALL DAT… THAT’S RIGHT I SAID BUMP IT! One of the cool things about R (for me at least) is discovering more efficient ways of doing things, and after class I got the thinking “there’s got to be a simpler way of dealing with Contingency Tables than I’m used to”. And in fact there is. Ladies and Gents, I give you the gmodels package: pacman::p_load(gmodels) Assuming we have the above table, we can use the gmodels:CrossTable() function. But first we need to recreate the table including only our observed values and save it to an object; here I’m saving it to table6.4_observed: table6.4_observed &lt;- matrix(c(33, 251, 33, 508), ncol = 2, byrow = T) colnames(table6.4_observed) &lt;- c(&quot;Yes&quot;, &quot;No&quot;) rownames(table6.4_observed) &lt;- c(&quot;Nonwhite&quot;, &quot;White&quot;) show(table6.4_observed) ## Yes No ## Nonwhite 33 251 ## White 33 508 From here we just throw this table into gmodels::CrossTable. Run ?gmodels::CrossTable to get a feel for what arguments this function takes and what other things it can do. For now, I’m especially concerned with: expected: If TRUE, chisq will be set to TRUE and expected cell counts from the Chi-Square will be included chisq: If TRUE, the results of a chi-square test will be included So if we run this with the argument expected=TRUE then it calculates and displays our expecteds AND runs the \\(\\chi^2\\): gmodels::CrossTable(table6.4_observed, expected = T) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Expected N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 825 ## ## ## | ## | Yes | No | Row Total | ## -------------|-----------|-----------|-----------| ## Nonwhite | 33 | 251 | 284 | ## | 22.720 | 261.280 | | ## | 4.651 | 0.404 | | ## | 0.116 | 0.884 | 0.344 | ## | 0.500 | 0.331 | | ## | 0.040 | 0.304 | | ## -------------|-----------|-----------|-----------| ## White | 33 | 508 | 541 | ## | 43.280 | 497.720 | | ## | 2.442 | 0.212 | | ## | 0.061 | 0.939 | 0.656 | ## | 0.500 | 0.669 | | ## | 0.040 | 0.616 | | ## -------------|-----------|-----------|-----------| ## Column Total | 66 | 759 | 825 | ## | 0.080 | 0.920 | | ## -------------|-----------|-----------|-----------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 7.709865 d.f. = 1 p = 0.005491987 ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ------------------------------------------------------------ ## Chi^2 = 6.978117 d.f. = 1 p = 0.008251239 ## ## Voila!! The key at the top of the output gives you the info that’s in each line of each cell, where: line 1 is observed N line 2 is expected N (it calculates them for you!!) line 3 is the \\(\\chi^2\\) contribution of that cell (remember the sum of the cells gives you \\(\\chi^2\\)) line 4 is the proportion of that cell to the row total line 5 is the proportion of that cell to the column total line 6 is the proportion of that cell to the grand total and then BAM!!! The end of the output gives you your \\(\\chi^2\\) test both with and without the Yates correction. 5.2.1 Okay, but what about with REAL DATA: OK. Let’s take a look at our old professor income data. In this case, instead of actual income values, let’s just take a look at Men v. Women faculty and make a judgment on whether or not they make greater than 50K per year. A raw data set may be found at the address below: FacultyIncome_chi &lt;- read.delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsData/master/FacultyIncome_chi.txt&quot;) As in the REAL DATA example above, we can simply use the table() function as we did above to get tallies from this data frame: table(FacultyIncome_chi) ## greater50 ## Gender No Yes ## Man 808 692 ## Woman 774 726 and throw this table into the gmodels::CrossTable function: gmodels::CrossTable(table(FacultyIncome_chi), expected = T) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Expected N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 3000 ## ## ## | greater50 ## Gender | No | Yes | Row Total | ## -------------|-----------|-----------|-----------| ## Man | 808 | 692 | 1500 | ## | 791.000 | 709.000 | | ## | 0.365 | 0.408 | | ## | 0.539 | 0.461 | 0.500 | ## | 0.511 | 0.488 | | ## | 0.269 | 0.231 | | ## -------------|-----------|-----------|-----------| ## Woman | 774 | 726 | 1500 | ## | 791.000 | 709.000 | | ## | 0.365 | 0.408 | | ## | 0.516 | 0.484 | 0.500 | ## | 0.489 | 0.512 | | ## | 0.258 | 0.242 | | ## -------------|-----------|-----------|-----------| ## Column Total | 1582 | 1418 | 3000 | ## | 0.527 | 0.473 | | ## -------------|-----------|-----------|-----------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 1.545953 d.f. = 1 p = 0.2137338 ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ------------------------------------------------------------ ## Chi^2 = 1.456352 d.f. = 1 p = 0.2275114 ## ## And we’re done. No need to get count() or use the xtabs() function or specify a formula. Life is good! 5.3 Effect Sizes and Odds Ratios Extending this to Table 6.5 from the Howell text, assessing the relationship between childhood sexual abuse (0 incidents, 1,2,3) and abuse as adult (yes,no): No Yes 0 512 54 1 227 37 2 59 15 3+ 18 12 And now a Chi-square: table6.5 &lt;- matrix(c(512, 54, 227, 37, 59, 15, 18, 12), ncol = 2, byrow = T) rownames(table6.5) = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3+&quot;) colnames(table6.5) = c(&quot;No&quot;, &quot;Yes&quot;) show(table6.5) ## No Yes ## 0 512 54 ## 1 227 37 ## 2 59 15 ## 3+ 18 12 gmodels::CrossTable(table6.5, expected = T) ## Warning in chisq.test(t, correct = FALSE, ...): Chi-squared approximation ## may be incorrect ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Expected N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 934 ## ## ## | ## | No | Yes | Row Total | ## -------------|-----------|-----------|-----------| ## 0 | 512 | 54 | 566 | ## | 494.493 | 71.507 | | ## | 0.620 | 4.286 | | ## | 0.905 | 0.095 | 0.606 | ## | 0.627 | 0.458 | | ## | 0.548 | 0.058 | | ## -------------|-----------|-----------|-----------| ## 1 | 227 | 37 | 264 | ## | 230.647 | 33.353 | | ## | 0.058 | 0.399 | | ## | 0.860 | 0.140 | 0.283 | ## | 0.278 | 0.314 | | ## | 0.243 | 0.040 | | ## -------------|-----------|-----------|-----------| ## 2 | 59 | 15 | 74 | ## | 64.651 | 9.349 | | ## | 0.494 | 3.416 | | ## | 0.797 | 0.203 | 0.079 | ## | 0.072 | 0.127 | | ## | 0.063 | 0.016 | | ## -------------|-----------|-----------|-----------| ## 3+ | 18 | 12 | 30 | ## | 26.210 | 3.790 | | ## | 2.572 | 17.783 | | ## | 0.600 | 0.400 | 0.032 | ## | 0.022 | 0.102 | | ## | 0.019 | 0.013 | | ## -------------|-----------|-----------|-----------| ## Column Total | 816 | 118 | 934 | ## | 0.874 | 0.126 | | ## -------------|-----------|-----------|-----------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 29.62726 d.f. = 3 p = 1.653054e-06 ## ## ## As I mentioned in class, we are not only interested in independence in itself, but also the the relationship between the frequency of our observed outcomes. This allow us to further interpret our data. For example we might be interested in not only whether instances of sexual abuse as a child affect the likelihood of experiencing abuse as an adult, but also how much more likely one is to be abused as an adult with increased childhood incidence. For this we can use odds ratios, which essentially act as a measure of effect size, or magnitude of the effect, for frequency data. Odd ratios may be calculated as by establishing a ratio of the binomial outcomes (in this case Yes v No) for each group (Incidence: 0,1,2,3+). To do this “by hand” you can simply take the appropriate columns and perform division to get the odds for each group. In this case you can just divide the values in column 2 of table6.5 by column 1 (because I’m interested in the relationship of number of incidents, Yes column). In themselves, the odds tell us the likelihood of occurrence: odds &lt;- table6.5[, 2]/table6.5[, 1] show(odds) ## 0 1 2 3+ ## 0.1054688 0.1629956 0.2542373 0.6666667 We then calculate each odds ratio by dividing each of the odds by a “control”. In this case the first group, 0 incidents is the logical control. odds_ratios &lt;- odds/odds[1] show(odds_ratios) ## 0 1 2 3+ ## 1.000000 1.545440 2.410546 6.320988 One important bit. Since we are dividing all odds by our control group, the odds ratio for the control will always be 1. Another way to think of this is that the odds ratio for our null condition will always be 1. Deviations away from 1 are deviations away from the null hypothesis. This makes since… if incidence did not have an effect we would expect the odds of adult abuse to be the same for all groups. Rather than doing this by hand, however, you can simply use epitools::oddsratio(): epitools::oddsratio(table6.5) ## Warning in chisq.test(xx, correct = correction): Chi-squared approximation ## may be incorrect ## $data ## No Yes Total ## 0 512 54 566 ## 1 227 37 264 ## 2 59 15 74 ## 3+ 18 12 30 ## Total 816 118 934 ## ## $measure ## NA ## odds ratio with 95% C.I. estimate lower upper ## 0 1.000000 NA NA ## 1 1.546412 0.9822983 2.411754 ## 2 2.420734 1.2459127 4.478786 ## 3+ 6.309182 2.8025291 13.779335 ## ## $p.value ## NA ## two-sided midp.exact fisher.exact chi.square ## 0 NA NA NA ## 1 5.956497e-02 5.726661e-02 5.466205e-02 ## 2 1.025698e-02 8.883009e-03 5.130875e-03 ## 3+ 2.566199e-05 2.246154e-05 2.208505e-07 ## ## $correction ## [1] FALSE ## ## attr(,&quot;method&quot;) ## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot; Note that by default epitools::oddsratio() assumes that your table is constructed with your rows as groups and your columns as outcomes (YorN, Success or Fail, etc.). It also assumes that your first row is your control, and the second column is your column of interest. It this is not the case you need to reformat your table to fit. 5.4 Fisher’s exact test Fisher’s exact test is a statistical significance test used in the analysis of 2×2 contingency tables. Although in practice it is employed when sample sizes are small (expected &lt; 5), it is valid for all sample sizes. It involves: 1. determining all possible tables that could be formed using same marginal totals observed and 2. calculating the probability of the observed data from whether it falls in the extreme region of all possible tables (i.e., &lt; .05 probability). The gmodels::CrossTable() function is nice as it will also run a Fisher’s exact test for you as well. Returning to our examples from the previous section, Table 6.4 from the text: Yes No Nonwhite 33 251 White 33 508 Remember we only need our observed data: table6.4_observed &lt;- matrix(c(33, 251, 33, 508), ncol = 2, byrow = T) colnames(table6.4_observed) &lt;- c(&quot;Yes&quot;, &quot;No&quot;) rownames(table6.4_observed) &lt;- c(&quot;Nonwhite&quot;, &quot;White&quot;) show(table6.4_observed) ## Yes No ## Nonwhite 33 251 ## White 33 508 And now to run the Fischer test by adding an additional argument to gmodels::CrossTable(): gmodels::CrossTable(table6.4_observed, fisher = T) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 825 ## ## ## | ## | Yes | No | Row Total | ## -------------|-----------|-----------|-----------| ## Nonwhite | 33 | 251 | 284 | ## | 4.651 | 0.404 | | ## | 0.116 | 0.884 | 0.344 | ## | 0.500 | 0.331 | | ## | 0.040 | 0.304 | | ## -------------|-----------|-----------|-----------| ## White | 33 | 508 | 541 | ## | 2.442 | 0.212 | | ## | 0.061 | 0.939 | 0.656 | ## | 0.500 | 0.669 | | ## | 0.040 | 0.616 | | ## -------------|-----------|-----------|-----------| ## Column Total | 66 | 759 | 825 | ## | 0.080 | 0.920 | | ## -------------|-----------|-----------|-----------| ## ## ## Fisher&#39;s Exact Test for Count Data ## ------------------------------------------------------------ ## Sample estimate odds ratio: 2.022094 ## ## Alternative hypothesis: true odds ratio is not equal to 1 ## p = 0.00680891 ## 95% confidence interval: 1.179576 3.467215 ## ## Alternative hypothesis: true odds ratio is less than 1 ## p = 0.9978205 ## 95% confidence interval: 0 3.194368 ## ## Alternative hypothesis: true odds ratio is greater than 1 ## p = 0.004780401 ## 95% confidence interval: 1.280213 Inf ## ## ## Note that this provides a general Fischer test, as well as directional tests (greater than, less than). How might we interpret this output? The test uses the odds ratio obtained from the 2×2 data, in this case 2.02, and compares it to the ratio that would be obtained assuming independence, which is always 1 (see previous section on Odds Ratios. You may also notice the 95% confidence intervals. If the interval for any of the tests contains 1 then it must be that \\(p&gt;.05\\). 5.5 Cohen’s kappa Cohen’s kappa is a measure of inter-judge agreement and used when we want to assess the reliability of judges or raters, or inter-rater reliability. For example. Two judges interview 30 adolescents and rate them as exhibiting (1) no behavior problems, (2) internalizing behaviors, (3) externalizing behaviors: NoProblem Internalizing Externalizing Total NoProblem 15 2 3 20 Internalizing 1 3 2 6 Externalizing 0 1 3 4 Total 16 6 8 30 The frequency of Judge 1’s reports move down columns, Judge 2’s reports move across rows. What is being conveyed on this table are the intersection of those reports. For example moving across the top row: there were 15 adolescents that both judges agreed had “No Problem”“; there were 2 that Judge 1 viewed as”Internalizing&quot; and Judge 2 viewed as “No Problem”; there were 3 that Judge 1 viewed as “Externalizing” and Judge 2 said “No Problem”, etc. In class we mentioned that we can get a measure of agreement based upon the observed agreements (along the diagonal in this table) and their expecteds (the amount of agreement we might expect purely due to chance). Based upon that, when can calculate from by hand: N &lt;- 30 KappaAgree &lt;- c(15, 3, 3) kappaExpec &lt;- c((16 * 20)/30, (6 * 6)/30, (8 * 4)/30) kappa &lt;- (sum(KappaAgree) - sum(kappaExpec))/(N - sum(kappaExpec)) show(kappa) ## [1] 0.4726562 Alternatively assuming we have saved this table as an object in R we can use the Kappa function from the vcd package: Constructing the table, note that I’m using piping to build my table in a single line (vector ➔ matrix ➔ table): Also note that I an using the table structure describe above (Rater 1 along columns, Rater down rows) kappaExample_table &lt;- c(15, 2, 3, 1, 3, 2, 0, 1, 3) %&gt;% matrix(., ncol = 3, byrow = T) rownames(kappaExample_table) &lt;- c(&quot;NoProblem&quot;, &quot;Internalizing&quot;, &quot;Externalizing&quot;) colnames(kappaExample_table) &lt;- c(&quot;NoProblem&quot;, &quot;Internalizing&quot;, &quot;Externalizing&quot;) show(kappaExample_table) ## NoProblem Internalizing Externalizing ## NoProblem 15 2 3 ## Internalizing 1 3 2 ## Externalizing 0 1 3 And now Cohen’s Kappa: vcd::Kappa(kappaExample_table) ## value ASE z Pr(&gt;|z|) ## Unweighted 0.4727 0.1304 3.624 0.0002898 ## Weighted 0.5109 0.1322 3.863 0.0001119 Note that psych also has a cohen.kappa() function, but the data needs to be structured differently. That’s it for now. Be sure to check back later. ``` "],
["correlation-and-regression.html", "Week 6 Correlation and Regression 6.1 The Relationship b/tw Stress and Health 6.2 Plotting the data 6.3 Covariance and Correlation 6.4 Sigfnificance testing \\(r\\) 6.5 Fitting the data to a model 6.6 Linear models in R 6.7 Write up and presentaion 6.8 Advanced plotting", " Week 6 Correlation and Regression In this week’s class we covered correlation and regression. In what follows we will revisit the ideas from this week’s lecture making explicit callbacks to topics from slides and using an example from the text. The examples from this page involve R. For a tutorial on how to perform similar analyses in SPSS, please check the appropriate video on the course Box Drive. Please note that this vignette assumes that you have the following packages installed and loaded in R: pacman::p_load(car, # new qqPlot function cowplot, # apa plotting tidyverse, # tidyverse goodness psych, lm.beta # getting standard coefficient ) 6.1 The Relationship b/tw Stress and Health From Howell (Section 9.2): Wagner, Compas, and Howell (1988) investigated the relationship between stress and mental health in first-year college students. Using a scale they developed to measure the frequency, perceived importance, and desirability of recent life events, they created a measure of negative events weighted by the reported frequency and the respondent’s subjective estimate of the impact of each event. This served as their measure of the subject’s perceived social and environmental stress. They also asked students to complete the Hopkins Symptom Checklist, assessing the presence or absence of 57 psychological symptoms. This data (and most data mentioned in the text) can be accessed directly from Howell’s companion website. # Data from Table 9.2 Howell example &lt;- read_table(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab9-2.dat&quot;) ## Parsed with column specification: ## cols( ## ID = col_integer(), ## Stress = col_integer(), ## Symptoms = col_integer(), ## lnSymptoms = col_double() ## ) psych::describe(example) ## vars n mean sd median trimmed mad min max range ## ID 1 107 54.00 31.03 54.00 54.00 40.03 1.00 107.00 106.00 ## Stress 2 107 21.29 12.49 20.00 20.49 11.86 1.00 58.00 57.00 ## Symptoms 3 107 90.33 18.81 88.00 88.87 17.79 58.00 147.00 89.00 ## lnSymptoms 4 107 4.48 0.20 4.48 4.48 0.19 4.06 4.99 0.93 ## skew kurtosis se ## ID 0.00 -1.23 3.00 ## Stress 0.62 -0.19 1.21 ## Symptoms 0.74 0.45 1.82 ## lnSymptoms 0.21 -0.28 0.02 6.1.1 Testing our assumptions Here we are interested in the relationship between perceived Stress (as measured on a scale factors the number and impact of negative life events) and the presence of psychological Symptoms. As with any other parametric analysis, we first need to see if our data adheres to the assumptions of normality and homogeniety of varience. We can use the tools we’ve been employing for the last few weeks test these assumptions: First Stress: hist(example$Stress, breaks = 10) car::qqPlot(example$Stress) ## [1] 72 55 shapiro.test(example$Stress) # see Field (2014), Sec 5.6.1 ## ## Shapiro-Wilk normality test ## ## data: example$Stress ## W = 0.96009, p-value = 0.002709 Eyeballing the plots, our Stress measures look slightly skewed, but not egregiously so. This is confirmed by our obtained skew and kurtosis values. The final function shapiro.test() performs a Shapiro-Wilkes test (\\(W\\)) of normality by comparing our observed distribution against a theoretical normal. Here the null hypothesis is that the \\(observed == theoretical\\), where \\(p &lt; .05\\) indicates that the observed distribution is not normal. Our obtained p-value confirms suggests the possibility that Stress measures deviate from normal. However, as noted in the Field text (5.6.1) we need to be careful using the Shapiro-Wilkes test on large samples. Ultimately you need to make a judgment on whether or not all of the evidence available leads you to the conclusion of non-normality. In this case I would trust the normality of this data due to what I see in the Q-Q plot (very few large deviations from the normal line). Now Symptoms hist(example$Symptoms, breaks = 10) psych::describe(example$Symptoms) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 107 90.33 18.81 88 88.87 17.79 58 147 89 0.74 0.45 ## se ## X1 1.82 car::qqPlot(example$Symptoms) ## [1] 24 19 shapiro.test(example$Symptoms) # see Field (2014), Sec 5.6.1 ## ## Shapiro-Wilk normality test ## ## data: example$Symptoms ## W = 0.95918, p-value = 0.00232 Symptoms does not pass either the eyeball or the Shapiro-Wilkes tests. These data are positively skewed. One way of dealing with non-normal data is by performing a logarithmic transformation (see Field, 5.8). This has already been performed for this data, lnSymptoms is a natural logarithmic transform of Symptoms. With your own data, this can be accomplished quite simply in R by using the log(): Question 1. Perform a natural log transform of Symptoms and save it to a new column in your data frame TransformedData. Assess TransformedData using a qqPlot: example$TransformedData &lt;- log(example$Symptoms) car::qqPlot(example$TransformedData) ## [1] 24 19 6.2 Plotting the data One of the first things that you should do is plot your data. Plotting gives you a sense of what is going on with your data. In fact, YOU SHOULD NEVER TAKE A TEST RESULT AT FACE VALUE WITHOUT FIRST LOOKING AT YOUR DATA!! Beware of Anscombe’s quartet! Let’s plot using ggplot: Question 2: create a ggplot scatterplot p &lt;- ggplot(example, aes(x = example$Stress, y = example$lnSymptoms)) + geom_point() + theme_cowplot() + xlab(&quot;Stress&quot;) + ylab(&quot;lnSymptoms&quot;) show(p) 6.3 Covariance and Correlation Both the Howell and Field texts offer excellent overviews of covariance and correlation so I won’t go into too much depth here. Briefly, let’s make a few connections to ideas that we’ve already encountered. 6.3.1 Covariance Recall that variance may be calculated as: \\(s_{x}^{2}=\\frac{\\sum \\left ( x_{i}-\\bar{X} \\right )^{2}}{n-1}\\) Where the numerator is the sum of squared differences from each score to the sample mean, and the denominator is our degrees of freedom. Variance tells us to what degree scores in a particular sample variable deviate from its mean. With this in mind, covarience is a statement about the degree to which two sampled variables deviate from their respective means. Consider we have sampled two measures, X &amp; Y from a population. When addressing the degree to which X and Y co-vary, we are asking the question: “To what degree and in what direction does Y move away from its mean as X moves from its mean?” As such, the formula for covariance is simply an extension of the formula for variance that we already know: \\[s_{xy}=\\frac{\\sum \\left ( x_{i}-\\bar{X} \\right )\\left ( y_{i}-\\bar{Y} \\right )}{n-1}\\] So, in order to calculate the covariance we need the calculate the sum of the cross-product of the sum of squared differences of our two variables (X = Stress; Y = lnSymptoms) and divide that number by of degrees of freedom. We could go about the business of calculating this “by hand”: # N = number of rows in data frame N &lt;- nrow(example) # mean Stress: meanX &lt;- mean(example$Stress) # and same for lnSymptoms: meanY &lt;- mean(example$lnSymptoms) # plug these values into our equation: # sum of cross product numerator &lt;- sum((example$Stress - meanX) * (example$lnSymptoms - meanY)) # degrees of freedom denominator &lt;- (N - 1) # covariance: covXY &lt;- (numerator/denominator) %&gt;% print() ## [1] 1.336434 The covariance of Stress and lnSymptoms is 1.336. Question 3: Alternatively, covariance may be calculated quickly in R using the cov() function: cov(example$Stress, example$lnSymptoms) ## [1] 1.336434 6.3.2 Correlation It may be tempting to calculate our covariance and stop there, but covariance is a limited measure. What I mean by this is that covarience indexes the degree of relationship between two specific variables, but doesn’t allow for more general comparison across situations. For a quick example, lets multiply both Stress and lnSymptoms by two. In R, (assuming all columns are numerics) we can accomplish this by multiplying the entire data frame by 2: example.by2 &lt;- example * 2 head(example.by2) ## ID Stress Symptoms lnSymptoms TransformedData ## 1 2 60 198 9.190240 9.190240 ## 2 4 54 188 9.086590 9.086590 ## 3 6 18 160 8.764054 8.764053 ## 4 8 40 140 8.496990 8.496990 ## 5 10 6 200 9.210340 9.210340 ## 6 12 30 218 9.382696 9.382696 We would like to think that multiplying every value by a constant should have no effect on our general interpretation of the relationships in our data as the overall relationship in our data remain the same. To help make this apparent, let’s imagine that I weigh 200 lbs (yea… imagine) and my wife weighs 100 lbs. So I weigh twice as much as my wife. Over the next year we both go on a binge fest and both double our respective weights—I’m now 400 lbs and my wife is 200 lbs. I still weigh twice as much as my wife. Coincidentally the fact that relationships remain unchanged in spite of these sorts of mathematical transformations is why we could perform the natural logarithmic transform earlier and not feel too guilty. Well what happens when I get the covariance of my example.by2 data?: cov(example.by2$Stress, example.by2$lnSymptoms) ## [1] 5.345735 The covariance changes!! This is a problem, and is why, in order to usefully convey this data we report the correlation. The correlation is a standardized covariance. The unit of measurement we’ll use for standardization is the standard deviation. We can standardize the covariance in one of two ways: 1. Standardize our variables (into z-scores) and then calculate the covariance: \\[r_{xy}=\\frac{\\sum z_{x}z_{y}}{n-1}\\] zX &lt;- scale(example$Stress) zY &lt;- scale(example$lnSymptoms) corXY &lt;- (sum(zX * zY)/(N - 1)) %&gt;% print() ## [1] 0.5286565 or 2. Calculate the covariance and standardize it (by dividing by the product of the standard deviation): \\[r_{xy}=\\frac{\\sum \\left ( x_{i}-\\bar{X} \\right )\\left ( y_{i}-\\bar{Y} \\right )}{(n-1)s_{x}s_{y}}\\] # get SD of X and Y: sdX &lt;- sd(example$Stress) sdY &lt;- sd(example$lnSymptoms) # using the covXY calculated above: covXY/(sdX * sdY) ## [1] 0.5286565 Again, in R we don’t need to do this by hand, there are in fact several functions. A more comprehensive look can be found in Field 6.5.3. The simplest function is cor(), with outputs the correlation as a single value. However, I prefer to use cor.test() as it tends to provide the most immediately useful data. You can input your data into this function in two ways: cor.test(example$Stress, example$lnSymptoms) or cor.test(~Stress + lnSymptoms, data = example) ## ## Pearson&#39;s product-moment correlation ## ## data: Stress and lnSymptoms ## t = 6.3818, df = 105, p-value = 4.827e-09 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3765970 0.6529758 ## sample estimates: ## cor ## 0.5286565 I prefer the latter as it uses formula notation which is the sort of notation we will use for regression and later t.tests, and ANOVA. Finally, if you don’t want all the additional gobble-dee-gook from cor.test you can simply attach $estimate to the end of either the function or object like so: cor.test(~Stress + lnSymptoms, data = example)$estimate ## cor ## 0.5286565 How did I know that I could do this you ask? To get a quick list of things that you can pull from the attributes, try the attributes() function. Question 4 save the cor.test() output of the correlation between Stress and lnSymptoms as an object corOut. Submit corOut to the attributes() function. Grab the p.value and estimate: corOut &lt;- cor.test(~Stress + lnSymptoms, data = example) attributes(corOut) ## $names ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;estimate&quot; &quot;null.value&quot; ## [6] &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; &quot;conf.int&quot; ## ## $class ## [1] &quot;htest&quot; corOut$p.value ## [1] 4.827486e-09 corOut$estimate ## cor ## 0.5286565 Our correlation is expressed in terms of the Pearson’s product-moment correlation coefficient (or \\(r\\), for short). Here \\(r\\) = .529. Coincidentally, returning to our problematic transformed data example.by2, we find that the obtained \\(r\\) is identical: cor.test(~Stress + lnSymptoms, data = example.by2) ## ## Pearson&#39;s product-moment correlation ## ## data: Stress and lnSymptoms ## t = 6.3818, df = 105, p-value = 4.827e-09 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3765970 0.6529758 ## sample estimates: ## cor ## 0.5286565 Note that when our sample size is small we may need to adjust \\(r\\). This is because the sampling distribution of \\(r\\) is not normally distributed. The formula for this adjustment is: \\(r_{adj}=\\sqrt{1-\\frac{\\sum \\left (1-r^2 \\right )\\left ( n-1 \\right )}{n-2}}\\) For now, we can calculate this by hand, but later we will see that it will be provided by another function (or at least its squared value will). # Pearson&#39;s r: rXY &lt;- cor.test(~Stress + lnSymptoms, data = example)$estimate # adjusted r: rXYadj &lt;- sqrt(1 - ((1 - rXY^2) * (N - 1)/(N - 2))) %&gt;% print() ## cor ## 0.522126 6.4 Sigfnificance testing \\(r\\) It may be useful to perform tests of significance on \\(r\\). Typically, there are two types of tests that we perform: a test of the observed \\(r\\) to \\(r=0\\), and a test of the difference between two \\(r\\) values. Regarding the first case, the test that the observed correlation is different from 0, Howell correctly notes the following (Section 9.11): - “a test on b is equivalent to a test on r in the one-predictor case we are discussing in this chapter. If it is true that \\(X\\) and \\(Y\\) are related, then it must also be true that \\(Y\\) varies with \\(X\\)—that is, that the slope is nonzero. This suggests that a test on \\(b\\) (beta coefficient) will produce the same answer as a test on \\(r\\), and we could dispense with a test for \\(b\\) altogether.” The reverse is also true. In the single predictor case, if one has a test of \\(b\\) this obviates the need to run an independent test of \\(r\\). As you will see below, we indeed do have a simple test that gives us the significance of the beta coefficient, \\(b\\). In the second case, the difference between \\(r\\) values, we may invoke the paired.r() function from the psych package. Using the \\(r\\) values from the example in Section 9.11 of the Howell text, “Testing the Difference Between Two Independent \\(r\\)s”: psych::paired.r(xy = 0.5, xz = 0.4, n = 53, n2 = 53, twotailed = T) ## Call: psych::paired.r(xy = 0.5, xz = 0.4, n = 53, n2 = 53, twotailed = T) ## [1] &quot;test of difference between two independent correlations&quot; ## z = 0.63 With probability = 0.53 where: xy: the correlation in the first data set xz: the correlation in the second data set n: the number of samples in the first data set n2: the number of samples in the second data set -twotailed: run a two-tailed test, TRUE or FALSE This output tells us the resulting Fisher \\(z\\) score and corresponding probability. Question 5: For example assume that you sample 35 men and find a correlation of .42 between Symptoms and Stress. In another sample of 42 women you find a correlation of .51. Perform a test to see if those two correlations are different from one another. psych::paired.r(xy = 0.42, xz = 0.51, n = 35, n2 = 42, twotailed = T) ## Call: psych::paired.r(xy = 0.42, xz = 0.51, n = 35, n2 = 42, twotailed = T) ## [1] &quot;test of difference between two independent correlations&quot; ## z = 0.48 With probability = 0.63 6.5 Fitting the data to a model As social scientists, we are not only concerned with observation, but also with explanation. Measures of correlation provide us with the former; we use regression methods to build models in service of the latter. Again both the Field and Howell texts provide excellent overviews of regression, so I won’t repeat much of what they say here. For the purposes of this vignette (and course) we will limit ourselves to linear regression, that is describing a line that best fits our data. By “fit” we mean a line than when compared to our observed data minimizes our squared residuals. To perform a linear regression we may call upon the lm() function. This function uses formula notation outcome variable ~ predictor variable(s). A simple regression has a single predictor. More often in our analyses we are concerned with the relative effects of multiple predictors, but this is multiple regression and saved for the Spring course. Here, we may be interested in the degree to which perceived Stress contributes to the number of lnSymptons, or using formula terminology: “How do lnSymptoms (outcome) vary as a function of Stress (predictor)”. This is represented in R as example$lnSymptoms~example$Stress 6.6 Linear models in R Our model assigned to example.model: example.model &lt;- lm(example$lnSymptoms ~ example$Stress) print(example.model) ## ## Call: ## lm(formula = example$lnSymptoms ~ example$Stress) ## ## Coefficients: ## (Intercept) example$Stress ## 4.300537 0.008565 While this output only gives us our coefficients (intercept and slope of our line), our object example.model has the class lm. This simply means that R understands that this object is storing a linear model. Hiding behind this spartan output is a multitude of info that may be accessed via its attributes or it may be thrown into other functions for additional info and analysis. 6.6.1 Attributes of class lm: As I mentioned above, lm objects contain various attributes that may be accessed using the names convention, $nameOfAttribute. For example, returning to the example.model object we created above, we may get a glimpse of its attributes by: attributes(example.model) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; ## ## $class ## [1] &quot;lm&quot; This command tells us the names of several attributes (values) that are stored in example.model. Typically we are interested in the $coefficients, fitted.values (aka predicted values), and $residuals. Recall that in our model fitted.values are the predicted values of lnSymptoms for each value of Stress, that is those values that fall along the line of best fit. residuals are the difference between our observed values of lnSymptoms and those predicted by the model. Mathematically example.model$residuals is equivalent to: example$lnSymptoms - example.model$fitted.values 6.6.2 Testing the residuals An important assumption test for our model is that the residuals are normally distributed. In fact some argue that this is more important than than having normal distributions in the raw data itself. Quickly, we can test this using the regular methods: hist(example.model$residuals, breaks = 10) car::qqPlot(example.model$residuals) %&gt;% show() psych::describe(example.model$residuals) Question 6: Run a Shapiro-Wilkes test on our example.model residuals. How do we feel about the residuals here? shapiro.test(example.model$residuals) ## ## Shapiro-Wilk normality test ## ## data: example.model$residuals ## W = 0.9876, p-value = 0.429 6.6.3 Using your lm object with other functions As I mentioned above you may also use lm objects with other functions. For example, for our model example.model: plot(example.model) produces a host of successive plots, including a plot of “Residuals v. Fitted”, A Q-Q plot that includes possible outliers, and a leverage plot. The last is outside of the scope of this course, but may be useful for you to remember in the Spring! A useful call for our present purposes is: summary(example.model) ## ## Call: ## lm(formula = example$lnSymptoms ~ example$Stress) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.42889 -0.13568 0.00478 0.09672 0.40726 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.300537 0.033088 129.974 &lt; 2e-16 *** ## example$Stress 0.008565 0.001342 6.382 4.83e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1726 on 105 degrees of freedom ## Multiple R-squared: 0.2795, Adjusted R-squared: 0.2726 ## F-statistic: 40.73 on 1 and 105 DF, p-value: 4.827e-09 which provides us with info about the coefficients, their standard error, t-scores, and corresponding p-values (Pr&gt;|t|). It also provides us with our \\(r^2\\) and adjusted \\(r_{adj}^2\\) (which corresponds to our \\(r\\) and \\(r_{adj}\\) from above). Coincidentally we can also call the individual attributes of this summary as well: summary(example.model) %&gt;% attributes() ## $names ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; ## ## $class ## [1] &quot;summary.lm&quot; summary(example.model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.300536618 0.033087547 129.974478 8.085268e-118 ## example$Stress 0.008564802 0.001342063 6.381819 4.827486e-09 We’ll return to each of these later, but first let’s return to our plot: 6.6.4 Plotting the regression line It’s useful practice to plot the regression line. In fact, I would typically recommend doing this at the outset of the analysis (way back at the beginning) but we had a few things to cover make this step clear. To add a simple regression line to our previous plot we can use the geom_smooth() function. geom_smooth() takes the following arguements: -method: what kind of regression line do you want to create? in this case we are performing a linear regression, so lm. -mapping: is you did not specify your aes() in the original call you need to specify them here (whats on the x-axis, y-axis, etc). -level: create a ribbon specifying a confidence interval of each predicted value (here I’m using 95% CI). -color: what color do you want the line. -linetype: solid line? dotted line? dashed line? default is solid. # creating a new plot with our original scatter plot as a template: p &lt;- ggplot2::ggplot(example, aes(x = example$Stress, y = example$lnSymptoms)) + geom_point() + theme_cowplot() + xlab(&quot;Stress&quot;) + ylab(&quot;lnSymptoms&quot;) smooth_p &lt;- p + geom_smooth(mapping = aes(x = Stress, y = lnSymptoms), method = lm, level = 0.95, color = &quot;black&quot;) show(smooth_p) The shaded ribbon around the line of regression represents the 95% CI of the estimate at each value of the predictor. The size of the CI may be adjusted by using the level parameter in geom_smooth(). Comparing this plot to our example.model output: summary(example.model) ## ## Call: ## lm(formula = example$lnSymptoms ~ example$Stress) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.42889 -0.13568 0.00478 0.09672 0.40726 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.300537 0.033088 129.974 &lt; 2e-16 *** ## example$Stress 0.008565 0.001342 6.382 4.83e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1726 on 105 degrees of freedom ## Multiple R-squared: 0.2795, Adjusted R-squared: 0.2726 ## F-statistic: 40.73 on 1 and 105 DF, p-value: 4.827e-09 From this plot it becomes apparent that the model coefficients obtained above represent the values of the slope and intercept of the regression line. The beta coefficient conveys the slope of the line—unit change in outcome per unit change in predictor. Typically this is expressed as b in text or B in SPSS. In R output, the beta estimate is tied to the name of the corresponding predictor variable. In our case that’s example$Stress. The intercept tells us what the value of the outcome would be if the predictor was 0. In most instances the intercept is not terribly useful. For example if you were looking at the relationship between height and weight it would make no sense to concern yourself with instances where height is absolutely 0. Obviously, a caveat here is if the value 0 is meaningful for your analysis. For example perhaps you are looking at depression as a function of alcohol consumption and want to explicitly say something about people that have never had a single drink in their life. One way of making a meaningless intercept meaningful is by centering your data (subtracting every value of your predictor from a constant). For example, if you center around the mean, the intercept will tell you the predicted value of the outcome variable at the mean value of the predictor. Just for giggles, lets plot our mean-centered data. We mean center Stress by subtracting each value of Stress from its mean: ggplot2::ggplot(data = example) + # plot each data point as a small red open circle geom_point(aes(x = (Stress - mean(Stress)), y = lnSymptoms), color = &quot;black&quot;, size = 2, shape = 19) + geom_smooth(aes(x = (Stress - mean(Stress)), y = lnSymptoms), method = lm, level = 0.95, color = &quot;black&quot;) + theme_cowplot() + xlab(&quot;Stress&quot;) + ylab(&quot;lnSymptoms&quot;) as you can see slope doesn’t change, but the values become centered around the intercept. 6.6.5 A note on degrees of freedom of our model The general equation for calculating the requisite degrees of freedom of a model is \\(n-k\\) where \\(n\\) is the number of observations (or later conditions) and \\(k\\) is the number of parameters that are required to be known for the calculation. Recall how we calculate variance / standard deviation. We take the sum of the squared deviations from the mean \\(\\sum(X-\\bar{X})\\) and divide by \\((n-1)\\). Remember that we are subtracting that 1 because in order to calculate the variance we must keep one thing constant, the mean. In this case the mean is our parameter that must be known in order to calculate the variance. We can now ask the question “what must be known in order to derive the line of best fit?”, or more simply what must one know in order to create a particular line. We must know both the line’s slope and its intercept. Knowing slope alone leads to the indeterminism presented in Fig A (identical slopes, infinitely-many intercepts), intercept alone leads to indeterminism presented in Fig B (identical intercept, infinitely-many slopes). We need both to define a particular line. Just like the mean above, these are the two parameters that must be locked-in to derive our model. A simple way of restating this is that your model degrees of freedom are the: \\[number \\space of \\space observations - number \\space of \\space predictors\\] where at least one of your predictors is used as a constant (the intercept) and the remainder provide your \\(\\beta\\) coefficients. For simple regression we have 1 predictor and 1 constant so our degrees of freedom are \\(n-k\\); where \\(k=2\\). However, in your future there be be dragons… models that may have multiple predictors and multiple intercepts (multiple regression, mixed effects / multi-level models, etc). 6.6.6 Interpreting the output With our plot in hand, we can return to our summary(example.model) output: Call: tells us the formula we originally input for this model Residuals: descriptive stats of the residuals Coefficients: provides the estimated values, standard error of the estimates, and NHST of the intercept and beta coefficient. In both cases the null hypothesis is Estimate = 0. Again for the intercept this may or may not be useful. For example you might use it to test whether people who haven’t had a single drink of alcohol have no depression at all. The test of beta tests against the null hypothesis of zero correlation (relationship). All tests are conducted provided the t-value on \\(n-k\\) degrees of freedom (see next bullet point) Residual standard error: This is calculated by first getting the sum of the squared residuals and dividing that number by the degrees of freedom of the model. In this case the df equals the number of total observations (107) minus the number of parameters (\\(k\\)) that were calculated by the model in order to generate the predicted values (2). This may be interpreted as a measure of goodness of fit of the model, and perhaps more importantly its predictive value (see Howell 9.7). Multiple R-squared and Adjusted R-squared: The coefficient of determination calculated from \\(r\\) or \\(r_{adj}\\). \\(r^2\\) tells us the degree to which our model accounts for observed variance in the observed outcomes. On was to think about this is, to what degree does variation in our predictor explain the observed variation in our outcomes. More on this below. Finally, this output gives is the resulting \\(F\\)-ratio and corresponding \\(df\\) for an \\(F\\)-test. This is a significance test for our \\(r^2\\). Does our model explain a significant amount of the variance? 6.6.7 The coeff. of determination as index of explanatory value Here we may turn to a (a little bit longer than expected) digression involving J.S. Mill and exact and inexact sciences. One philosophical conceit is that we can never really empirically observe causes, but only effects. That is, I can never truly observe that one billiard ball caused another to move, I can only say with a high degree of certainty that I observed the first billiard ball striking the second and subsequently the second moved at a particular velocity. In order to deal with this epistemological crisis (how can you really ever know anything!!!) we build explanatory models. The degree to which our model is able to account for the range of our previous observations tells us how useful our model parameters (predictors) are in explaining a phenomenon in question. For example, to explain what I observe about the contact and subsequent motion of billiard balls I might build a model that takes into account the mass and velocity (speed and direction) of the first ball, the mass and velocity of the second ball and the point and angle of contact. Presuming I use these as predictors, I can account with varying degrees of success the heading and direction of the balls, or the outcome. In fact, in this example, I’m likely to be very successful in describing most, but not all observed outcomes. That is, there are some variations in the outcomes that just using these parameters does not result in a successful account. I can refine my billiard contact model to include things like the friction caused from the felt, the force of gravity, the angle of the table top, etc., and it’s likely the case that with each new predictor my model becomes better. However at some point I begin to reach diminishing returns (see over-fitting next semester). This is why physics appears to be an exact science (well, mechanical physics at least). They’ve had centuries to develop and refine their models about the mechanical workings of our very local corner of the universe to the point that these models are able to account for a very high degree of variation in observed phenomena. These models typically have a very high \\(r^2\\) and as such tend to have high predictive value. Which is why with some understanding of gravitational force, mass, and a few simple calculations, high school students all over country are able to do this in their physics lab. Indeed within some disciplines having an \\(r^2\\) &lt; .9 means you model isn’t good enough. Typically this is the case when the phenomenon under observation is not complex (hope that doesn’t offend). As you move away from physics and chemistry (or at least the low complexity phenomena under each discipline), the explanatory value of models tends to decrease. Us folks in the social sciences, we are moving into inexact-science-land. Weather patterns, economies, ecosystems, and human behaviors are highly complex; in many cases in inordinate number of contributing factors may be present in an observed outcome. As a result degree of variation that these models may account for is typically diminished, especially in the social sciences. For example depending on the phenomena and sub-discipline, an \\(r^2\\) of .3 might mean you’re cooking with gas! In fact, for some phenomena, a reported \\(r^2\\) that is too high might be a cause for suspicion (“they weren’t so much cooking with gas so much as they were cooking the books”). As a final note, its typically poor practice to get a significant \\(r^2\\) and just stop there. Remember, this is just a claim that your model accounts for significantly more variation than 0%. If you were to have a model that accounted for 6% (\\(r^2 = .06\\)) of the variance but was significant (\\(p&lt;.05\\)), you need to ask yourself whether that model is useful at all. It certainly has very little predictive value (I wouldn’t bet my life… or even a few bills on it predicting a future outcome), and its true explanatory value is quite questionable. 6.7 Write up and presentaion Our obtained results can be reported in several different ways. However, typically it’s best practice to report both the \\(beta\\) coefficient with corresponding \\(t-test\\); as well as the general statistics about the model including the \\(r^2\\) and corresponding \\(F-test\\). Prior to any reporting of the stats, you also need to articulate the structure of the model. I think a good template to start from is: “[Clearly restate the alternative hypothesis identifying operationalized variables]. To test this hypothesis the data were [what type of analysis?] with [what is your dv] as the outcome and [what is your predictor] as our predictor. The resulting model was significant, r^2=XXX; F(df1,df2)=F-ratio, p=p-value and revealed a significant relationship between the [covariables], b=XX; t(df)=XX, p=XX$.” For example, with this data you may report it as: “We hypothesized that increases in self-reported measures of Stress would correspond to increases in the number of self-reported symptoms. Given that our Symptoms data showed a significant violation of the normality assumption, we transformed these scores using a log-transformation (lnSymptoms). To test this hypothesis the data were submitted to a simple linear regression with lnSymptoms as the outcome and Stress as our predictor. The resulting model was significant, \\(r^2\\)=.28; \\(F\\)(1,105)=40.73, p&lt;.001$, and revealed a significant positive relationship between the Stress and Symptoms, \\(b\\)=.008; \\(t\\)(105)=6.38, \\(p\\)&lt;.001.” That said, in many circles it’s typical practice to report the standardized \\(beta\\) coefficients (\\(\\beta\\)). These are the \\(beta\\) coefficients we would have obtained if we had initially converted both our Stress and lnSymptom values to \\(z\\)-scores. We can retroactively obtain these coefficients by using the lm.beta() function from the lm.beta package (duplicate names, confusing I know). We can then input an lm class objects into the lm.beta() function and get the summary of those results. The new column Standardized is reported rather than Estimate: Question 7 Use lm.beta::lm.beta() to obtain the standard coefficients. How would you report these results? lm.beta::lm.beta(example.model) %&gt;% summary() So amending the example reporting paragrah above: The resulting model was significant, \\(r^2\\)=.28; \\(F\\)(1,105)=40.73, \\(p\\)&lt;.001, and revealed a significant positive relationship between the Stress and Symptoms, \\(\\beta =.528\\); \\(t(105)=6.38\\), \\(p&lt;.001\\).&quot; Coincidentally you may have noticed that this value is the correlation that we established earlier. Again I point to Howell 9.11 for how these two are similar in the single predictor case. 6.8 Advanced plotting Depending on what info you are trying to convey, there are several ways of dressing up your plot. In this section we’ll cover three things that you can do with the plots above: - adding text information like \\(r^2\\) and the equation of the regression line - placing two (or more) plots side-by-side - placing two sample (and regression lines) on the same plot 6.8.1 Adding text to the plot Text may be added to a plot using the annotate() function in ggplot2. This function has several arguments: the type of annotation (typically geom=&quot;text&quot;), the x position of the plot (measured in units of the x axis), the y position of the plot, the label or text that you want to add (in string form) and the whether or not R needs to parse the text. For simple text like “Hi my name is..” doesn’t need parsing. For more complex text like some of what we will be doing (text that needs to be italicized, superscripts, etc) you need to set parse=TRUE. Finally you can adjust the size of the text. We can create the necessary text for out plot by hand by simply adding these repective values in format that we construct. However, there are a few leaps and bounds that we need to do in order to format the text and plug in our values. For the same of simplicity just take this at face value for now, but feel free to follow-up in your free time to see exactly what is going on here: # putting together my equation, telling R to add superscripts and italics: equation &lt;- italic(y) == 4.301 + 0.009 %.% italic(x) * &quot;,&quot; ~ ~italic(r)^2 ~ &quot;=&quot; ~ 0.28 # turn it into an expression for R to parse: equation &lt;- as.expression(equation) %&gt;% as.character() and now add it to my previous plot using annotate() # create scatter plot p &lt;- ggplot2::ggplot(data = example) + # plot each data point as a small red open circle geom_point(aes(x = Stress, y = lnSymptoms), colour = &quot;black&quot;, size = 2, shape = 19) + theme_cowplot() # adding the regression line p &lt;- p + geom_smooth(aes(x = Stress, y = lnSymptoms), colour = &quot;black&quot;, method = lm, level = 0.95) # adding text p &lt;- p + annotate(geom = &quot;text&quot;, x = 20, y = 5, label = equation, parse = T, size = 4) show(p) You’ll note that my text is centered at the intesection of where x=20 and y=5. You may need to play around with these values to get this right. 6.8.2 addnig text like a R-ninja: The other useful thing to remember is that you can access different parts of an lm class object using its attributes (see above). In this case we can call on various attributes of example.model. For example we can get the coefficients by: example.model$coefficients ## (Intercept) example$Stress ## 4.300536618 0.008564802 we call also get them by using the coef() function: coef(example.model) ## (Intercept) example$Stress ## 4.300536618 0.008564802 the same can be said about the \\(r^2\\). It’s available from the summary model attributes: # list our attributes: summary(example.model) %&gt;% attributes() ## $names ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; ## ## $class ## [1] &quot;summary.lm&quot; # we want &#39;r.squared&#39;: summary(example.model)$r.squared ## [1] 0.2794776 knowing this we can actually create a function to do the above for us automatically (see I told you building you own functions would com in handy). True, there are a few things that we haven’t covered yet in the code, so feel free to look them up: # create a function called lm_eqn lm_eqn = function(m) { l &lt;- list(a = format(coef(m)[1], digits = 2) %&gt;% unname(), b = format(abs(coef(m)[2]), digits = 2) %&gt;% unname(), r2 = format(summary(m)$r.squared, digits = 3)) if (coef(m)[2] &gt;= 0) { eq &lt;- substitute(italic(y) == a + b %.% italic(x) * &quot;,&quot; ~ ~italic(r)^2 ~ &quot;=&quot; ~ r2, l) } else { eq &lt;- substitute(italic(y) == a - b %.% italic(x) * &quot;,&quot; ~ ~italic(r)^2 ~ &quot;=&quot; ~ r2, l) } as.character(as.expression(eq)) } after running the chunk above (or executing the function in to memory) you can just plug in you lm object into the new function lm_eqn(): # plug in the lm.model equation &lt;- lm_eqn(example.model) # create scatter plot p &lt;- ggplot2::ggplot(data = example) + # plot each data point as a small red open circle geom_point(aes(x = Stress, y = lnSymptoms), colour = &quot;black&quot;, size = 2, shape = 19) + theme_cowplot() # adding the regression line p &lt;- p + geom_smooth(aes(x = Stress, y = lnSymptoms), colour = &quot;black&quot;, method = lm, level = 0.95) # adding text from &#39;equation&#39; object above p &lt;- p + annotate(geom = &quot;text&quot;, x = 20, y = 5, label = equation, parse = T, size = 4) show(p) 6.8.3 Placing plots side-by-side in a grid You can place two or more plots in a grid using the plot_grid() function from the cowplot library. You can check out an intro to cowplot here. Indeed, cowplot does so much more than grid plotting! cowplot can be installed from CRAN, so we can simply: pacman::p_load(cowplot) The plot.grid() function has several arguments. For now, we’ll just focus on a few, including the list of plots to include, labels for each plot, and their arrangement specified in the number of rows and columns. For the purpose of this example and the next, let’s create a new data set that takes a look at the data above as broken down by men and women: # read in the data above: example2 &lt;- read_table(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab9-2.dat&quot;) ## Parsed with column specification: ## cols( ## ID = col_integer(), ## Stress = col_integer(), ## Symptoms = col_integer(), ## lnSymptoms = col_double() ## ) # create a gender vector of 57 women and 50 men: gender &lt;- c(rep(&quot;w&quot;, 57), rep(&quot;m&quot;, 50)) # randomize gender using sample set.seed(1) # just so we randomize the same gender &lt;- sample(gender) # add gender to our example 2 data.frame example2$gender &lt;- gender show(example2) ## # A tibble: 107 x 5 ## ID Stress Symptoms lnSymptoms gender ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 30 99 4.60 w ## 2 2 27 94 4.54 w ## 3 3 9 80 4.38 m ## 4 4 20 70 4.25 m ## 5 5 3 100 4.61 w ## 6 6 15 109 4.69 m ## 7 7 5 62 4.13 m ## 8 8 10 81 4.39 m ## 9 9 23 74 4.30 m ## 10 10 34 121 4.80 w ## # ... with 97 more rows Now that we have our data set we can separate out women’s data from men’s data. We’ve done this before using the filter function in dplyr. Another way is to use the subset() function: womenData &lt;- subset(example2, gender == &quot;w&quot;) menData &lt;- subset(example2, gender == &quot;m&quot;) now we can create two separate plots… # women plot wPlot &lt;- ggplot2::ggplot(data = womenData, aes(x = Stress, y = lnSymptoms)) + # plot each data point as a small red open circle geom_point(colour = &quot;red&quot;, size = 2, shape = 1) + geom_smooth(method = lm, level = 0.95) + theme_cowplot() # men plot mPlot &lt;- ggplot2::ggplot(data = menData, aes(x = Stress, y = lnSymptoms)) + # plot each data point as a small red open circle geom_point(colour = &quot;red&quot;, size = 2, shape = 1) + geom_smooth(method = lm, level = 0.95) + theme_cowplot() And plot them side-by-side. To do so, in the cowplot::plot_grid() function I need to enter the name of each plot; the corresponding labels that I choose, and tell plot_grid how many columns there should be in my grid: plot_grid(wPlot, mPlot, labels = c(&quot;A&quot;, &quot;B&quot;), ncol = 2) Note here I’ve labeled the women’s plot “A” and men’s plot “B”. I can label these whatever my hear desires, though typical convention is lettering. I can also stack them vertically by specifying nrow instead of ncol: plot_grid(wPlot, mPlot, labels = c(&quot;W&quot;, &quot;M&quot;), nrow = 2) 6.8.4 Two series on the same damn plot on the same damn plot, on the same damn plot… For our last trick, let’s plot both men and women’s data on the same figure. This can be accomplished by using our original example2 data frame and using aes() to specify different shape, colors, and regression lines as a function of gender: # add color and shape arguments here. These arguements in aes() indicate that # different shapes and colors should be used for each level of gender: bothPlot &lt;- ggplot2::ggplot(data = example2, aes(x = Stress, y = lnSymptoms, shape = gender, color = gender)) + # only need to specify size here: geom_point(size = 2) + geom_smooth(method = lm, level = 0.95) show(bothPlot) Of course to make this APA I’ll need to add my cowplot() to the end. I’ll also need to perform a few manual tweaks of the color, scale_color_manual() takes a vector of color names. bothPlot &lt;- ggplot2::ggplot(data = example2, aes(x = Stress, y = lnSymptoms, shape = gender, color = gender)) + # only need to specify size here: geom_point(size = 2) + geom_smooth(method = lm, level = 0.95) + theme_cowplot() + # colors, make one series black and the other grey greys have a numeric value 10, # 20, 30 etc that determines how light: scale_color_manual(values = c(&quot;black&quot;, &quot;grey60&quot;)) show(bothPlot) Note that I can also specify linetype aesthetic in geom_smooth() for different lines: bothPlot &lt;- ggplot2::ggplot(data = example2, aes(x = Stress, y = lnSymptoms, shape = gender, color = gender)) + # only need to specify size here: geom_point(size = 2) + geom_smooth(method = lm, level = 0.95, aes(linetype = gender)) + theme_cowplot() + # colors, make one series black and the other grey greys have a numeric value 10, # 20, 30 etc that determines how light: scale_color_manual(values = c(&quot;black&quot;, &quot;grey60&quot;)) show(bothPlot) "],
["testing-differences-in-means-t-test.html", "Week 7 Testing differences in means: t-test 7.1 Things to consider before running the t-test 7.2 Performing the t-test (Paired sample t-test) 7.3 Measuring effect size 7.4 Other \\(t\\) tests: 7.5 Independent or Paired Sample?", " Week 7 Testing differences in means: t-test This week we covered when and how to conduct a \\(t-test\\). We use a t-test to assess whether the observed difference between sample means is greater than would be predicted be chance. Both the Field text and Howell text do a wonderful job of explaining t-tests conceptually so I will defer to those experts on matters of the underlying their statistical basis. Instead my goal this week is to walk through some examples on performing, interpreting, and reporting t-tests using R. This walkthough assumes that the following packages are installed and loaded on your computer: pacman::p_load(tidyverse, car, cowplot, lsr) 7.1 Things to consider before running the t-test Before running a t.test there are a few practical and statistical considerations that must be taken. In fact, these considerations extend to every type of analysis that we will encounter for the remainder of the semester (and indeed the rest of your career) so it would be good to get in the habit of running through your checks. In what proceeds here I will walk step by step with how I condunct a t.test (while also highlighting certain decision points as they come up). 7.1.1 What is the nature of your sample data? In other words where is the data coming from? Is it coming from a single sample of participants? Is it coming from multiple samples of the SAME participants? Is it coming from multiple groups of participants. This will not only determine what analysis you choose to run, but in also how you go about the business of preparing to run this analysis. Of course, truth be told this information should already be known before you even start collecting your data, which reinforces an important point, your central analyses should already be selected BEFORE you start collecting data! As you design your experiments you should do so in a way in which the statistics that you run are built into the design, not settled upon afterwards. This enables you to give the tests you perform the most power, as you are making predictions about the outcomes of your test a priori. This will become a major theme on the back half of the class, but best to introduce it now. For this week, it will determine what test we will elect to perform. Let’s grab some sample data from the Howell text (Table 7.3): Description from Howell: Everitt, in Hand, et al., 1994, reported on family therapy as a treatment for anorexia. There were 17 girls in this experiment, and they were weighed before and after treatment. The weights of the girls, in pounds, is provided in the data below: Tab7_3 &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab7-3.dat&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Parsed with column specification: ## cols( ## ID = col_character(), ## Before = col_double(), ## After = col_double() ## ) So what is known: we have 17 total participants from (hypothetically) the same population that are measured twice (once Before treatment, and once After treatment). Based upon the experimental question we need to run a paired-sample (matched-sample) test. (Although I’ll use this data to provide an example of a one-sample test later on). 7.1.2 What is the structure of your data file? Before doing anything you should always take a look at your data: show(Tab7_3) ## # A tibble: 17 x 3 ## ID Before After ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 83.8 95.2 ## 2 02 83.3 94.3 ## 3 03 86 91.5 ## 4 04 82.5 91.9 ## 5 05 86.7 100. ## 6 06 79.6 76.7 ## 7 07 76.9 76.7 ## 8 08 94.2 102. ## 9 09 73.4 94.9 ## 10 10 80.5 75.2 ## 11 11 81.6 77.8 ## 12 12 82.1 95.5 ## 13 13 77.6 90.7 ## 14 14 83.5 92.6 ## 15 15 89.9 93.8 ## 16 16 86 91.7 ## 17 17 87.3 98 So what do we have here, three columns: ID: the participant number Before: participants’ weights before treatment After: participants’ weights after treatment Most important for present purposes this data is in WIDE format—each line represents a participant. While this might be intuitve for tabluar visualization, many statistical softwares prefer when LONG format, where each line represents a single observation (or some mixed of WIDE and LONG like SPSS). ** [See this page for a run down of WIDE v. LONG format] (https://www.theanalysisfactor.com/wide-and-long-data/) ** More, for those that are interested I suggest taking a look at these two wonderful courses on DataCamp: Working with Data in the Tidyverse Cleaning Data in R 7.1.2.1 Getting data from WIDE to LONG So the data are in wide format, each line has multiple observations of data that are being compared. Here both Before scores and After scores are on the same line. In order to make life easier for analysis and plotting in ggplot, we need to get the data into long format (Before scores and After scores are on different lines). This can be done using the gather() function from the tidyr package. We touched briefly on this function in this week’s lecture. Here we go into a little more detail. Before gathering, one thing to consider is whether or not you have a column that defines each subject. In this case we have ID. This tells R that these data are coming from the same subject and will allow R to connect these data when performing analysis. That said, for t.test() this is not crucially important—t.test() assumes that the order of lines represents the order of subjects, e.g., the first Before line is matched to the first After line. Later on when we are doing ANOVA, however, this participant column will be important an we will need to add if it is missing. Using gather(): It may first make sense to talk a bit about the terminology that this function uses. For every data point you have a key and a value. Think of the key as how the data point is defined or described, while the value is the measure of the data point. Typically in research we describe the data in terms of the condition under which it was collected—so if it helps, think of the key as your IV(s) and the value as your DV. With this data, our values would be the weights of each participant. The keys would be how they are differentiated, Before and After. So for value I would input “weight” and for key I might choose “treatment” noting that the levels are Before-treatment and After-treatmemt. One other thing that I have to consider is that I don’t want every column of my data gathered. For example in this case I don’t want my ID column gathered, but rather duplicated. Let’s look at what happens if I just gather: gather(data = Tab7_3, key = &quot;treatment&quot;, value = &quot;weight&quot;) ## # A tibble: 51 x 2 ## treatment weight ## &lt;chr&gt; &lt;chr&gt; ## 1 ID 01 ## 2 ID 02 ## 3 ID 03 ## 4 ID 04 ## 5 ID 05 ## 6 ID 06 ## 7 ID 07 ## 8 ID 08 ## 9 ID 09 ## 10 ID 10 ## # ... with 41 more rows That’s not right! In order to overcome this I need to exclude the column(s) that I don’t want gathered. What R will do is copy those appropriately. This can be accomplished by stating with columns for the original data set you do not want gathered at the end of the gather() function, placing a “-” (negative sign) in front of them. In this case I don’t want the first column, ID gathered, so: # &#39;-1&#39; here means exclude the first column gather(data = Tab7_3, key = &quot;treatment&quot;, value = &quot;weight&quot;, -1) ## # A tibble: 34 x 3 ## ID treatment weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 Before 83.8 ## 2 02 Before 83.3 ## 3 03 Before 86 ## 4 04 Before 82.5 ## 5 05 Before 86.7 ## 6 06 Before 79.6 ## 7 07 Before 76.9 ## 8 08 Before 94.2 ## 9 09 Before 73.4 ## 10 10 Before 80.5 ## # ... with 24 more rows # or, &#39;-ID&#39; means exclude the column named ID, both will work. note that I&#39;m # just making this output invisible to avoid duplicates: gather(data = Tab7_3, key = &quot;treatment&quot;, value = &quot;weight&quot;, -ID) %&gt;% invisible() Ok data is structured correctly, on to the next step. 7.1.3 Testing assumptions Remember that you should always test to see if the data fit the assumptions of the test you intend to perform. In this case, we need to assess two things: 7.1.3.1 Is the data normally distributed? Knowing the design of your experiment also has implications for testing your assumptions. For example, whether you have a paired (matched) sample design (e.g., two samples from the same participants) or an independent sample design (e.g., two groups) determines how you go about the business of testing the normality assumption. If you have an independent samples test, you test each sample seperately, noting measures of skew, kurtosis, inspecting the qqPlot, and Shapiro-Wilkes test (though acknowledging that SW is very sensitive). However, if you are running a paired (matched) samples test, you need to be concerned with the distribution of the difference scores. In the present example we are comparing participants’ weights Before treatment to their weight After. First, let me save my gathered data to a data_frame Tab7_3_gathered and then filter() accordingly for Before and After (though note that you could elect to gather after this step and simply use the Before and After columns for the original dataset): Tab7_3_gathered &lt;- gather(data = Tab7_3, key = &quot;treatment&quot;, value = &quot;weight&quot;, -ID) beforeTreatment &lt;- filter(Tab7_3_gathered, treatment == &quot;Before&quot;) afterTreatment &lt;- filter(Tab7_3_gathered, treatment == &quot;After&quot;) And now compute the difference scores, and run my assumption tests: diffWeights &lt;- beforeTreatment$weight - afterTreatment$weight psych::describe(diffWeights) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 17 -7.26 7.17 -9.1 -7.15 5.93 -21.5 5.3 26.8 0.18 -0.77 ## se ## X1 1.74 car::qqPlot(diffWeights) ## [1] 9 10 shapiro.test(diffWeights) ## ## Shapiro-Wilk normality test ## ## data: diffWeights ## W = 0.9528, p-value = 0.5023 7.1.3.2 Is the data variability homogeneous? Another important assumption is that the variablility within each sample is similar. For a t-test this can be tested by using the leveneTest() from the car package: # using long-format enter as a formula: car::leveneTest(weight ~ treatment, data = Tab7_3_gathered, center = &quot;mean&quot;) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. ## Levene&#39;s Test for Homogeneity of Variance (center = &quot;mean&quot;) ## Df F value Pr(&gt;F) ## group 1 3.64 0.06542 . ## 32 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You’ll note above I elected to mean center my samples. This is consistent with typical practive although “median” centering may be more robust (both Field and Howell reading this week get into the particulars of this test). Given that my obtained Pr(&gt;F), or p-value of Levene’s F-test, is greater than .05 I may elect to assume that my variances are equal. However, if you remained skeptical, there are adjustments that you may make. This includes adjusting the degrees of freedom according to the Welch-Satterthwaite. Recall later on that we are looking at our obtained \\(t\\) value with respect to the number of \\(df\\). This adjustment effectively reduces the \\(df\\) in turn making your test more conservative. 7.1.4 Getting the descriptive stats and plotting the means. Finally, as we will be performing a test of difference in means, it would be a good idea to get descriptive measures of means and variability for each group. Indeed, these data were already obatined when we used psych::describe() to assess the normailty of each sample. Here I’ll just do it again to get these values: psych::describeBy(Tab7_3_gathered$weight, group = Tab7_3_gathered$treatment) ## ## Descriptive statistics by group ## group: After ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 17 90.49 8.49 92.6 90.77 3.85 75.2 101.6 26.4 -0.74 -0.93 ## se ## X1 2.06 ## -------------------------------------------------------- ## group: Before ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 17 83.23 5.02 83.3 83.15 4.15 73.4 94.2 20.8 0.15 -0.29 ## se ## X1 1.22 Typically along with the mean, you need to report a measure of variability of your sample. This can be either the SD, SEM, or if you choose the 95% CI, although this is more rare in the actual report. See the supplied HW example and APA examples on BOX for conventions on how to report these in your results section. Plotting in ggplot * I’ve mentioned several times the limits and issues with plotting bar plots, but they remain a standard, so we will simply proceed using these plots. But I’ll note that boxplots, violin plots, bean plots, and pirate plots are all modern alteratives to bar plots and are easy to execute in ggplot(). Try a Google search. In the meantime, to produce a bar plot in R we simply modify a few of the arguments that we are familiar width. Here is the code for plotting these two groups: ggplot(data = Tab7_3_gathered, aes(x = treatment, y = weight)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;col&quot;) + stat_summary(fun.data = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, width = 0.1) + scale_y_continuous(expand = c(0, 0)) + theme_cowplot() Breaking this down line-by-line: ggplot(data = Tab7_3_gathered, aes(x=treatment, y=weight)): standard fare for starting a ggplot. See the Appendix treatment on intro to ggplot to refresh your memory (or the DataCamp course). stat_summary(fun.y = &quot;mean&quot;, geom = &quot;col&quot;): stat_summary() gets summary statistics and projects them onto the geom of your choice. In this case we are getting the mean values, fun.y = &quot;mean&quot; and using them to create a column plot geom = &quot;col&quot; . stat_summary(fun.data = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, width = .1) : here we are greating error bars, geom = &quot;errorbar&quot;. Important to note here is that error bars require knowing three values: mean, upper limit, and lower limit. Whenever you are asking for a single value, like a mean, you use fun.y. When multiple values are needed you use fun.data. Here “mean_se” requests Standard error bars. Other alternatives include 95% CI “mean_cl_normal” and Standard deviation “mean_sdl”. The width argument adjusts the width of the error bars. scale_y_continuous(expand = c(0,0)): Typically R will do this strange thing where it places a gap bewteen the data and the x-axis. This line is a hack to remove this default. It says along the y-axis add 0 expansion (or gap). theme_cowplot(): quick APA aesthetics. You may also feel that the zooming factor is off. This may especially be true in cases where there is little visual discrepency between the bars. To “zoom in” on the data you can use coord_cartesian(). For example, you might want to only show the range between 70 lbs and 100 lbs. When doing this, be careful not to truncate the upper limits of your bars and importantly your error bars. ggplot(data = Tab7_3_gathered, aes(x = treatment, y = weight)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;col&quot;) + stat_summary(fun.data = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, width = 0.1) + scale_y_continuous(expand = c(0, 0)) + theme_cowplot() + coord_cartesian(ylim = c(70, 100)) Additionally, to get this into true APA format I would need to adjust my axis labels. Here capitalization is needed. Also, because the weight has a unit measure, I need to be specific about that: ggplot(data = Tab7_3_gathered, aes(x = treatment, y = weight)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;col&quot;) + stat_summary(fun.data = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, width = 0.1) + scale_y_continuous(expand = c(0, 0)) + theme_cowplot() + coord_cartesian(ylim = c(70, 100)) + xlab(&quot;Treatment&quot;) + ylab(&quot;Weight (lbs)&quot;) Finally, you may have notice that the order of Treatment on the plot is opposite of what we might like to logically present. In this case the “After” data comes prior to the “Before” data on the x-axis. This is because R defaults to alphabetical order when loading in data. To correct this I can use scale_x_discrete() and specify the order that I want in limits: ggplot(data = Tab7_3_gathered, aes(x = treatment, y = weight)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;col&quot;) + stat_summary(fun.data = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, width = 0.1) + scale_y_continuous(expand = c(0, 0)) + theme_cowplot() + coord_cartesian(ylim = c(70, 100)) + xlab(&quot;Treatment&quot;) + ylab(&quot;Weight (lbs)&quot;) + scale_x_discrete(limits = c(&quot;Before&quot;, &quot;After&quot;)) All good (well maybe check with Sierra first)! One other thing to consider (although please do not worry about it here) is the recent argument that when dealing with repeated measures data you need to adjust you error bars. See this pdf by Richard Morey (2005) for more information on this issue. 7.2 Performing the t-test (Paired sample t-test) Okay, now that we’ve done all of our preparation, we’re now ready to perform the test. We can do so using the t.test() function. In this case, the experimental question warrants a paired samples t-test. Given that our Levene’s test failed to reject the null, we will assume that our variances are equal. Again, since we’ve got long-format data we will use the formula syntax: t.test(weight ~ treatment, data = Tab7_3_gathered, paired = T, var.equal = T) ## ## Paired t-test ## ## data: weight by treatment ## t = 4.1802, df = 16, p-value = 0.0007072 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 3.580571 10.948840 ## sample estimates: ## mean of the differences ## 7.264706 The output provides us with our \\(t\\) value, the \\(df\\) and the \\(p\\) value. It also includes a measure of the 95% CI, and the mean difference. Remember that the null hypothesis is that there is no difference between our two samples. In the case of repeated measures especially, it makes sense to think of this in terms of a difference score of change, where the null is 0. The resulting interpretation is that on average participants’ weight increased 7.26 pounds due to the treatment, with a 95% likelihood that the true mean change is between 3.58 lbs and 10.95 lbs. Important for us is that 0 is not in the 95% CI, reinforcing that there was indeed a non-zero change (rejecting the null). 7.3 Measuring effect size As always, rejecting the null due to a significant \\(p\\) value is not the end of the story. The absolute null is almost always guarenteed to be false. The question is whether or not you have compelling evidence to demonstrate it so. For example I likely could have collected data from 1000 participants here and found a “significant” difference with a weight gain of 0.5 lbs (\\(p&lt;.05\\)). Would we feel like that was compelling evidence of the effect of the treatment? Probably not! Whenever you report any statistic, you need to report an effect size. For \\(t\\) tests, the appropriate effect size measure is Cohen’s D. Cohen’s D expresses the observed difference as a ratio of the standard deviation of the sample(s)—in this respect its conceptually similar to our understanding of the standard distribution where we understand the magnitude of any score as expressed in the number of standatd deviations away from a 0 mean. Here, the mean difference betweem means (7.26) is divided by either the pooled standard deviations of both samples (weighted average) or the standard deviation of one of the samples. For example using the Before group SD: \\[D = \\frac{\\mid \\bar{X}_{before}-\\bar{X}_{after} \\mid}{SD_{before}} = \\frac{7.26}{5.02}\\] Typically you will use the pooled standard deviation, though in some circumstances it may make sense to use the standard deviation of a single group. For example if your variences are unequal you may elect to use the SD of the control group, that way you understand the observed change in scores in scales of magnitude to the control. Let’s calculate Cohen’s D the pooled variance, and only using the varience of a single group. We use lsr::cohensD() with the formula syntax: # pooled SD lsr::cohensD(weight ~ treatment, data = Tab7_3_gathered, method = &quot;pooled&quot;) ## [1] 1.042123 # using SD of the first group (alphabetical order): After lsr::cohensD(weight ~ treatment, data = Tab7_3_gathered, method = &quot;x.sd&quot;) ## [1] 0.8560091 # using SD of the first group (alphabetical order): Before lsr::cohensD(weight ~ treatment, data = Tab7_3_gathered, method = &quot;y.sd&quot;) ## [1] 1.448107 As you can see you end up with very different values depending on what you choose. As I said, the default is the “pool” your variences. If you elect to do otherwise you’ll need to mention it in your report. 7.4 Other \\(t\\) tests: 7.4.1 One sample: The data in our example warranted running a paired t-test. However, as noted we can run a t.test() to compare a single sample to a single value. For example it might be reasonable to ask whether or not the 17 adolescent girls that Hand, et al., 1994 treated were different from what would be considered the average weight of a teenaged girl. A quick Google search suggests that the average weight of girls 12-17 in 2002 was 130 lbs. How does this compare to Hand et al.’s participants Before treatment? We can run a one sample t-test to answer this question: beforeTreatment &lt;- filter(Tab7_3_gathered, treatment == &quot;Before&quot;) t.test(beforeTreatment$weight, mu = 130) ## ## One Sample t-test ## ## data: beforeTreatment$weight ## t = -38.44, df = 16, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 130 ## 95 percent confidence interval: ## 80.65007 85.80876 ## sample estimates: ## mean of x ## 83.22941 Yes, this group of girls was significantly underweight compared to the national average. 7.4.2 Independent samples example We run an independent samples t-test when we have no reason to believe that the data in the two samples is meaningfully related in any fashion. Consider this example from Howell, Table 7.7 regarding Joshua Aronson’s work on stereotype threat: Joshua Aronson has done extensive work on what he refers to as “stereotype threat,” which refers to the fact that “members of stereotyped groups often feel extra pressure in situations where their behavior can confirm the negative reputation that their group lacks a valued ability” (Aronson, Lustina, Good, Keough, Steele, &amp; Brown, 1998). This feeling of stereo- type threat is then hypothesized to affect performance, generally by lowering it from what it would have been had the individual not felt threatened. Considerable work has been done with ethnic groups who are stereotypically reputed to do poorly in some area, but Aronson et al. went a step further to ask if stereotype threat could actually lower the performance of white males—a group that is not normally associated with stereotype threat. Aronson et al. (1998) used two independent groups of college students who were known to excel in mathematics, and for whom doing well in math was considered impor- tant. They assigned 11 students to a control group that was simply asked to complete a difficult mathematics exam. They assigned 12 students to a threat condition, in which they were told that Asian students typically did better than other students in math tests, and that the purpose of the exam was to help the experimenter to understand why this difference exists. Aronson reasoned that simply telling white students that Asians did better on math tests would arousal feelings of stereotype threat and diminish the students’ performance. Here we have two mutually exclusive groups of white men, those controls and those under induced threat.. Importantly we have no reason to believe that any one control man’s score is more closely tied to any individual experimental group counterpart than any others (we’ll return to this idea in a bit). Here is the data: Tab7_7 &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab7-7.dat&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## ID = col_character(), ## Score = col_integer(), ## Group = col_integer() ## ) As before, let’s take a look at the file structure: show(Tab7_7) ## # A tibble: 23 x 3 ## ID Score Group ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 01 4 1 ## 2 02 9 1 ## 3 03 12 1 ## 4 04 8 1 ## 5 05 9 1 ## 6 06 13 1 ## 7 07 12 1 ## 8 08 13 1 ## 9 09 13 1 ## 10 10 7 1 ## # ... with 13 more rows I want to look at this example as it give is an opportunity to deal with another common issue in data cleaning. If you take a look at Group you see it’s either 1 or 2. Based upon Howell Table 7.7 we can deduce that Group 1 are the control subjects and Group 2 are the threat subjects. Using numbers instead of names to identify levels of a factor is a convention from older methods and software. In more modern software you don’t need to do this sort of dummy coding (the software works this out in the background). If you want to change this, you can use the recode() function from dplyr package in the tidyverse (https://dplyr.tidyverse.org/reference/recode.html). For what it’s worth there are several other ways to do this including a recode() function in car. See http://rprogramming.net/recode-data-in-r/ for examples. Here I’m just going to overwrite the Group column with the recoded names: Tab7_7$Group &lt;- dplyr::recode(Tab7_7$Group, `1` = &quot;Control&quot;, `2` = &quot;Threat&quot;) show(Tab7_7) ## # A tibble: 23 x 3 ## ID Score Group ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 01 4 Control ## 2 02 9 Control ## 3 03 12 Control ## 4 04 8 Control ## 5 05 9 Control ## 6 06 13 Control ## 7 07 12 Control ## 8 08 13 Control ## 9 09 13 Control ## 10 10 7 Control ## # ... with 13 more rows An now to run the requisite assumption tests. Note that in this case I am running an Indepednent samples tet, so I need to test the assumptions on each sample seperately: Control: controlGroup &lt;- filter(Tab7_7, Group == &quot;Control&quot;) qqPlot(controlGroup$Score) ## [1] 1 11 Threat: threatGroup &lt;- filter(Tab7_7, Group == &quot;Threat&quot;) qqPlot(threatGroup$Score) ## [1] 10 4 Homogeniety of Variance: car::leveneTest(data = Tab7_7, Score ~ Group) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.4639 0.5033 ## 21 T-test The Levene’s test failed to reject the null so I may proceed with my t.test assuming variances are equal. Note that paired=FALSE for independent sample tests: t.test(data = Tab7_7, Score ~ Group, paired = FALSE, var.equal = T) ## ## Two Sample t-test ## ## data: Score by Group ## t = 2.3614, df = 21, p-value = 0.02795 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.3643033 5.7417573 ## sample estimates: ## mean in group Control mean in group Threat ## 9.636364 6.583333 This output gives us the \\(t\\)-value, \\(df\\) and \\(p\\)-value. Based on this output I may conclude that the mean score in the Control group is significantly greater than the Threat group. Just as an example, let’s set var.equal to FALSE: t.test(data = Tab7_7, Score ~ Group, paired = FALSE, var.equal = F) ## ## Welch Two Sample t-test ## ## data: Score by Group ## t = 2.3565, df = 20.614, p-value = 0.02843 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.3556143 5.7504463 ## sample estimates: ## mean in group Control mean in group Threat ## 9.636364 6.583333 Comparing the outputs you see that in this case R has indicated that it has run the test with the Welsh correction. Note that this changes the \\(df\\) and consequently the resulting \\(p\\) value. That this change was neglible reinforces that the variances were very similar to one another. However in cases where they are not close to one another you may see dramatic changes in \\(df\\). In R, the t.test() function sets var.equal=FALSE by default. Why you ask? Well, you can make the argument that the variances are ALWAYS unequal, its only a matter of degree. Assuming variances are unequal makes your test more conservative, meaning that if the test suggests that you should reject the null, you can be slightly more confident that you are not committing Type I error. At the same time, it could be argued that setting your var.equal=TRUE in this case (where the Levene test failed to reject the null) makes your test more powerful, and you should take advantage of that power to avoid Type II error. 7.5 Independent or Paired Sample? It is safe to assume that anytime that you are collecting data samples from the same person at two different points in time that you need to run a paired-samples test. However, it would not be safe to assume that if the samples are coming from different groups of people that you always run an independent samples test. Remember the important qualifier mentioned above: That there no reason to believe that any one participant in the first group is is more closely related to any single counterpart in the second group than the remaining of others. In our Independent test example we have no reason to assume this is the case, we assume that members of the Control and Threat groups were randomly selected. But what if we instead recruited brothers or twins? In this case, it may make sense to treat members of the two groups as paired; brothers have a shared history (education, socio-economic level, family dymanic, etc) that would make their scores more likely to be related to one another than by random chance. Howell makes a similar point in Exercise Question 7.19 at the end of the chapter. OK. That’s it for this week. Be sure to check the Appendix for an additional write-up connecting t-test (and ultimately ANOVA) to the General Linear Model and regression analyses that we performed last week. "],
["analysis-of-variance-i-the-one-way-anova.html", "Week 8 Analysis of Variance I: the One-way ANOVA 8.1 Required packages 8.2 ANOVAs for comparing means 8.3 Pre-processing the data: 8.4 Assumptions for ANOVA 8.5 Running the ANOVA in R: 8.6 Reporting your data 8.7 DIGGING DEEPER: ANOVA and Regression", " Week 8 Analysis of Variance I: the One-way ANOVA In this week’s class we covered ANOVA. A major emphasis in the lecture was that ANOVA (much like many of the other tests that we cover in this course) is an extension of the general linear model. In many respects, ANOVA and regression are conceptually identical—whereas in linear regression our predictor variables are typically continuous (or, in some cases ordinal) we usually reserve ANOVA for instances when our predictors are discrete or nominal. This would be the difference say in predicting weight as a function of height (linear regression) in contrast to weight as function of hometown (Dayton, Youngstown, Cleveland, Cincinnati). Given that both the Howell (Chapter 11) and Field (Chapter 10) do a wonderful job of explaining the underlying principles of ANOVA, I won’t spend too much time here rehashing what is already available there. Field, especially does an excellent job of demonstrating how even though regression and ANOVA are often treated differently in terms of research focus (e.g., observation v. experimentation) and data focus (correlation/goodness of fit v. comparing means) they are indeed one and the same. Here, my goal is to reinforce this idea using examples in R, as well as providing a practical tutorial that will serve as our entry point into ANOVA. As always, for an example of how to perform these analyses in SPSS please check the appropriate course materials on Box Drive. 8.1 Required packages This write-up assumes that you have the following packages installed and loaded in R: # check to make your computer has pacman installed, if not install if (!require(pacman)) { install.packages(&quot;pacman&quot;) } # use pacman to check, install, and load necessary packages pacman::p_load(car, cowplot, tidyverse, psych) 8.2 ANOVAs for comparing means We typically understand ANOVA as a method for allowing us to compare means from more than two samples. Two see how this connects with what we have learned from regression lets use the data provided in Howell’s main example from Chapter 11.1 (see book for background info). 8.3 Pre-processing the data: To start, lets download Eysenck’s (1974) dataset: dataset &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Ex11-1.dat&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## GROUP = col_integer(), ## RECALL = col_integer() ## ) show(dataset) ## # A tibble: 50 x 2 ## GROUP RECALL ## &lt;int&gt; &lt;int&gt; ## 1 1 9 ## 2 1 8 ## 3 1 6 ## 4 1 8 ## 5 1 10 ## 6 1 4 ## 7 1 6 ## 8 1 5 ## 9 1 7 ## 10 1 7 ## # ... with 40 more rows 8.3.1 Recoding the factors (if dummy coded) As we can see right off the bat the data is number (dummy) coded, where 1 = ‘Counting’, 2 = ‘Rhyming’, 3 = ‘Adjective’, 4 = ‘Imagery’ and 5 = ‘Intentional’. My advice for what to do if you get dummy coded data is to create a corresponding column in your data set that contains the factors in nominal (name) format. Recall from last week that we can use the recode() function to reassign the dummy variables. In that case we recoded for the purpose of plotting. Here, we are recoding the levels of a factor. Here, the preferred function is an extension of recode(): recode_factor(). When we recode_factor() we have the added benefit of automatically factorizing, telling R to treat the IV as a factor. Let’s create a new column dataset$GROUP_FACTOR that contains this data: # assigning the appropriate names for the dummy codes dataset$GROUP_FACTOR &lt;- dplyr::recode_factor(dataset$GROUP, `1` = &quot;Counting&quot;, `2` = &quot;Rhyming&quot;, `3` = &quot;Adjective&quot;, `4` = &quot;Imagery&quot;, `5` = &quot;Intentional&quot;) 8.3.2 Renaming column headers (optional) And now, just to be clear, let’s rename the original GROUP to GROUP_NUM. This can be accomplished by using the names() function. Heres the logic: names() gives you the column names of your data frame: names(dataset) ## [1] &quot;GROUP&quot; &quot;RECALL&quot; &quot;GROUP_FACTOR&quot; You can isolate any individual name using indexing: names(dataset)[1] ## [1] &quot;GROUP&quot; And after you’ve isolated a column you can re-assign a new name: names(dataset)[1] &lt;- &quot;GROUP_NUM&quot; Check out this link for info on how to rename multiple columns at once using names() or dplyr::rename() from the tidyverse. 8.3.3 Reordering your levels One important consideration that you should have even before you look at your data is what is your control (group, condition). Proper experimentation requires a proper control in order to properly isolate the influence of the manipulation. Here the best candidates for our control group might either be “Counting” or “Intentional”, depending on how the original problem was approached. If the larger comparison involved “Intentional v. incidental” learning for recall, then the “Intentional” group serves best as your control. If the original question involved levels of processing, then “Counting”&quot; (theoretically the lowest level of incidental processing) is best. Here I am assuming the latter (although I believe theoretically Eysenck originally was interested in the former). I bring this up, as its typically best to ensure that your control is entered first into the ANOVA model. To check the order of your levels, you may simply: levels(dataset$GROUP_FACTOR) ## [1] &quot;Counting&quot; &quot;Rhyming&quot; &quot;Adjective&quot; &quot;Imagery&quot; &quot;Intentional&quot; Here we see that “Counting” is first and will therefore be entered first into the model. Assuming that we wanted to reorder the sequence, say to have Intentional as the control, then we might simply: dataset$GROUP_FACTOR &lt;- factor(dataset$GROUP_FACTOR, levels = c(&quot;Intentional&quot;, &quot;Counting&quot;, &quot;Rhyming&quot;, &quot;Adjective&quot;, &quot;Imagery&quot;)) levels(dataset$GROUP_FACTOR) ## [1] &quot;Intentional&quot; &quot;Counting&quot; &quot;Rhyming&quot; &quot;Adjective&quot; &quot;Imagery&quot; However, I liked the original order, so let’s change it back: dataset$GROUP_FACTOR &lt;- factor(dataset$GROUP_FACTOR, levels = c(&quot;Counting&quot;, &quot;Rhyming&quot;, &quot;Adjective&quot;, &quot;Imagery&quot;, &quot;Intentional&quot;)) levels(dataset$GROUP_FACTOR) ## [1] &quot;Counting&quot; &quot;Rhyming&quot; &quot;Adjective&quot; &quot;Imagery&quot; &quot;Intentional&quot; That’s better. Why the order is important will be made clear later in this write up. For now, think back to our example of running a t-test using lm(). You may recall that the group level that was first entered into the model served as the model intecept where the second group level was expressed in terms of the slope of the line (beta). A similar account will be happening here. Finally, the data here is presenting in LONG format. This is the desired format for most analysis and plotting in R. Check last week’s write-up for a discussion on WIDE and LONG format and how to switch from WIDE to LONG. 8.4 Assumptions for ANOVA 8.4.1 Checking the normality assumption, OPTION 1 To check the distribution of outcomes in ANOVA, you have two options. The first would be to check the distribution of outcomes for EACH group/condition independently. In the case of the example dataset we could get info related to the skew and kurtosis of RECALL for each GROUP_FACTOR using `describeBy(): psych::describeBy(dataset, group = &quot;GROUP_FACTOR&quot;) ## ## Descriptive statistics by group ## GROUP_FACTOR: Counting ## vars n mean sd median trimmed mad min max range skew ## GROUP_NUM 1 10 1 0.00 1 1 0.00 1 1 0 NaN ## RECALL 2 10 7 1.83 7 7 1.48 4 10 6 0 ## GROUP_FACTOR* 3 10 1 0.00 1 1 0.00 1 1 0 NaN ## kurtosis se ## GROUP_NUM NaN 0.00 ## RECALL -1.22 0.58 ## GROUP_FACTOR* NaN 0.00 ## -------------------------------------------------------- ## GROUP_FACTOR: Rhyming ## vars n mean sd median trimmed mad min max range skew ## GROUP_NUM 1 10 2.0 0.00 2.0 2.00 0.00 2 2 0 NaN ## RECALL 2 10 6.9 2.13 6.5 6.88 0.74 3 11 8 0.18 ## GROUP_FACTOR* 3 10 2.0 0.00 2.0 2.00 0.00 2 2 0 NaN ## kurtosis se ## GROUP_NUM NaN 0.00 ## RECALL -0.4 0.67 ## GROUP_FACTOR* NaN 0.00 ## -------------------------------------------------------- ## GROUP_FACTOR: Adjective ## vars n mean sd median trimmed mad min max range skew ## GROUP_NUM 1 10 3 0.00 3 3.00 0.00 3 3 0 NaN ## RECALL 2 10 11 2.49 11 11.25 2.97 6 14 8 -0.66 ## GROUP_FACTOR* 3 10 3 0.00 3 3.00 0.00 3 3 0 NaN ## kurtosis se ## GROUP_NUM NaN 0.00 ## RECALL -0.84 0.79 ## GROUP_FACTOR* NaN 0.00 ## -------------------------------------------------------- ## GROUP_FACTOR: Imagery ## vars n mean sd median trimmed mad min max range skew ## GROUP_NUM 1 10 4.0 0.0 4.0 4.00 0.00 4 4 0 NaN ## RECALL 2 10 13.4 4.5 11.5 12.75 1.48 9 23 14 0.99 ## GROUP_FACTOR* 3 10 4.0 0.0 4.0 4.00 0.00 4 4 0 NaN ## kurtosis se ## GROUP_NUM NaN 0.00 ## RECALL -0.53 1.42 ## GROUP_FACTOR* NaN 0.00 ## -------------------------------------------------------- ## GROUP_FACTOR: Intentional ## vars n mean sd median trimmed mad min max range skew ## GROUP_NUM 1 10 5 0.00 5 5 0.00 5 5 0 NaN ## RECALL 2 10 12 3.74 11 12 2.97 5 19 14 0.05 ## GROUP_FACTOR* 3 10 5 0.00 5 5 0.00 5 5 0 NaN ## kurtosis se ## GROUP_NUM NaN 0.00 ## RECALL -0.47 1.18 ## GROUP_FACTOR* NaN 0.00 If we wanted to perform for extensive methods like hist(), qqPlot(), and shapiro.test(), in the past I had you filter be each level (group) and proceed. So for example for Counting: countingGroup &lt;- filter(dataset, GROUP_FACTOR == &quot;Counting&quot;) hist(countingGroup$RECALL) qqPlot(countingGroup$RECALL) ## [1] 6 5 shapiro.test(countingGroup$RECALL) ## ## Shapiro-Wilk normality test ## ## data: countingGroup$RECALL ## W = 0.98372, p-value = 0.9819 In the past you would have to repeat this for each other level. An alterative to generate an output by each level of a factor is to use the by() function. For example to generate a sequence of qqPlots (for the sake of space I’m not going to execute this code here, but try at home) # by(dependent variable, grouping factor, name of function) by(dataset$RECALL, INDICES = dataset$GROUP_FACTOR, qqPlot) You can do the same with hist() and shapiro.test(). 8.4.2 Checking the normality assumption, OPTION 2 Although by() may or may not make life easier for you in this test case, things rapidly become more complicated when attempting to check normality by condition. For example if you’re running a 2×3×3 mixed effect ANOVA, you would need to run through 18 conditions. So what to do. A simpler alternative is to run you model and analyze your residuals. This web link does a nice and quick job of explaining the logic. In this case we would run our ANOVA model using aov: dataset_aov &lt;- aov(RECALL ~ GROUP_FACTOR, data = dataset) Congrats, you’ve just run an ANOVA, but for now we aren’t interested in the results from the model. Remember from a view weeks back that many outputs have attributes that may be accessed. For example: attributes(dataset_aov) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;contrasts&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; ## [13] &quot;model&quot; ## ## $class ## [1] &quot;aov&quot; &quot;lm&quot; We want those residuals!! From here, we can simply take the residuals and submit them to our standard tests for normality: psych::describe(dataset_aov$residuals) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 50 0 2.98 -0.45 -0.14 2.59 -7 9.6 16.6 0.64 1.22 0.42 hist(dataset_aov$residuals) qqPlot(dataset_aov$residuals) ## [1] 36 44 shapiro.test(dataset_aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: dataset_aov$residuals ## W = 0.95964, p-value = 0.08586 So, between OPTION 1 and OPTION 2, I’d recommend typically going with OPTION 2. 8.4.3 Homogeneity of Variance Another assumption of ANOVA is the homogeneity of variance between groups. An easy-way to get an eyeball test of this assumption is two perform a box plot of the data. Here I am performing this plot using ggplot2: boxplots &lt;- ggplot(data = dataset, aes(x = GROUP_FACTOR, y = RECALL)) + geom_boxplot() show(boxplots) Huge differences in the IQR regions may be a clue that the homogeneity assumption is violated. We can run more specific tests in R including the Levene Test and Figner-Killeen (non-parametric, to be used if the data is not normal) Test of Homogeneity of Variances. As is typically the case \\(p&lt;.05\\) indicates a violation of this assumption: # Levene Test of Homogeneity of Variances car::leveneTest(RECALL ~ GROUP_FACTOR, data = dataset) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 4 0.8932 0.4759 ## 45 # Figner-Killeen Test of Homogeneity of Variances fligner.test(RECALL ~ GROUP_FACTOR, data = dataset) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: RECALL by GROUP_FACTOR ## Fligner-Killeen:med chi-squared = 1.9733, df = 4, p-value = 0.7407 8.4.4 What to do if the assumptions are violated? If either assumption is violated, one option that you have is to transform you data. We’ve talked several times in class about the pros and cons of doing this, and both the Field and Howell texts provide examples for how this is done. Another option is to use a non-parametric test such as the Kruskal-Wallis Test if the data is not normal, or Welch’s ANOVA is the variances are not homogenous. That said, one of reasons that ANOVA is so popular is that it has been demonstrated to be robust in the face of violated assumptions (as long as the sample sizes are equal). For example, in Design and Analysis of Experiments (1999, p. 112) Dean &amp; Voss argue that the maximum group variance may be as high as 3× the minimum group variance without any issue. With this in mind, a question (gray area) before us is how much of a violation is there in the data? And if not so much, you may be fine just running an ANOVA regardless. 8.5 Running the ANOVA in R: There a many, many ways to run an ANOVA in R. Throughout the semester we will be highlighting three: aov(), lm(), and using the afex package. For the next two weeks we will concentrate on aov() which is the standard method, as well as the lm() method that you have used before, just to reinforce that ANOVA and regression are one in the same. In fact, SPOILER ALERT, aov() is just a fancy wrapper for lm(). Like lm() from weeks past, aov() asks us to enter our dependent and independent variables into the model in the formula format DV ~ IVs. In this case, we only have a single IV, GROUP_FACTOR. Thus our model is: aov.model &lt;- aov(RECALL ~ GROUP_FACTOR, data = dataset) From here, an anova() of the model gives us our ANOVA table. anova(aov.model) ## Analysis of Variance Table ## ## Response: RECALL ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GROUP_FACTOR 4 351.52 87.880 9.0848 1.815e-05 *** ## Residuals 45 435.30 9.673 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Also note that we can get residuals from the aov() output as well. In fact take a look at the object’s $class… see I told you, lm()!!! attributes(aov.model) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;contrasts&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; ## [13] &quot;model&quot; ## ## $class ## [1] &quot;aov&quot; &quot;lm&quot; 8.6 Reporting your data First let’s grab the descriptive stats: describeBy(dataset$RECALL, dataset$GROUP_FACTOR) ## ## Descriptive statistics by group ## group: Counting ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 10 7 1.83 7 7 1.48 4 10 6 0 -1.22 0.58 ## -------------------------------------------------------- ## group: Rhyming ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 10 6.9 2.13 6.5 6.88 0.74 3 11 8 0.18 -0.4 0.67 ## -------------------------------------------------------- ## group: Adjective ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 10 11 2.49 11 11.25 2.97 6 14 8 -0.66 -0.84 0.79 ## -------------------------------------------------------- ## group: Imagery ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 10 13.4 4.5 11.5 12.75 1.48 9 23 14 0.99 -0.53 1.42 ## -------------------------------------------------------- ## group: Intentional ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 10 12 3.74 11 12 2.97 5 19 14 0.05 -0.47 1.18 8.6.1 Reporting in the text Reporting the omnibus ANOVA includes: the F value, the degress of freedom (between and within), the p-value effect size: typically with ANOVA we report partial eta squared, although note that Howell advocates for omega squared. Howell gives you the equations to calculate both; and indeed they can be calculated given the info that is conveyed in the anova(aov) output. I won’t delve too much into the how to do that here, because, well… homework. But it should be included. For our data our report takes the form: \\(F(4,45)=9.085,p&lt;.001, \\eta_p^2=.45\\). As we typically focus on means with ANOVA, it is typically a good idea to report means and some report of of the distribution of each group (typically either standard deviation or standard error; although 95% CI may be useful in certain scenarios). So for example reporting the Rhyming group: M ± SD: 6.90 ± 2.13 SE: 6.90 ± 0.67 8.6.2 What the omnibus ANOVA tells you While it may be useful to report the means, you need to be mindful of what the omnibus ANOVA tells you. Remember, that the the null hypothesis of the omnibus ANOVA is that “there are no differences between observed means”. Our significant result tells us that there are differences between our means, but does not tell us what those specific differences are. So while it may be useful to report general relationships, e.g., “Recall for people in the Intentional group tended to be greater than for the Counting group” you cannot say definitively “Recall was significantly greater for the Intentional Group”. Typically when you only have tested the omnibus ANOVA, you only speak in generalities (e.g., “Recall tended to increase with level of processing.”) Always be mindful that you don’t over interpret your data. 8.6.3 Plots For ANOVA plots, there are typically three acceptable plots used to convey, box plots, bar plots, and line plots. As the ANOVA become more complex, we tend towards using line plots (box plots and bar plots may become very busy in complex designs). Unless there are compelling reasons not to we tend to plot the means (although note that box plots usually give you medians). Since we have already produced a box plot above, I’ll show examples of bar and line plots. To create a line plot with points at the means and error lines representing the 95% CI (known as a point range), we can use call that you are familar with and a new geom pointrange. Whats nice about point range is that it allows you to create your points and the error bars all in a single line. p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = GROUP_FACTOR, y = RECALL)) # create the points p &lt;- p + stat_summary(fun.data = mean_cl_normal, size = 1, color = &quot;black&quot;, geom = &quot;pointrange&quot;) show(p) Now to add the lines to this plot. For this you will need another stat_summary() line specifying that the vertices of the lines should be the means, fun.y = mean and a parameter that specifies how the lines should be grouped. Since we have a One-way ANOVA, group=1. When we built to more complex designs you may elect to group=FACTOR_NAME. Something like this is useful to say make some lines dashed and some lines solid according to levels on a factor. More on this in two weeks when we get to factorial ANOVA. While we’re at it let’s fix those axes titles. We can use the functions xlab() and ylab() to do so. # this is from before: p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = GROUP_FACTOR, y = RECALL)) p &lt;- p + stat_summary(fun.data = mean_cl_normal, size = 1, color = &quot;black&quot;, geom = &quot;pointrange&quot;) # and now adding the lines p &lt;- p + stat_summary(data = dataset, fun.y = mean, size = 1, color = &quot;black&quot;, mapping = aes(group = 1), geom = &quot;line&quot;) # and fixing the axis titles: p &lt;- p + xlab(&quot;Group&quot;) + ylab(&quot;Words recalled&quot;) show(p) Finally, for aesthetic reasons yo may elect to expand the y-axis. For example to make the range of y-axis values (0,20): p + expand_limits(y = c(0, 20)) Now a bar plot, with standard error bars. This is similar to how you built your bar plots from last week (t-test) p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = GROUP_FACTOR, y = RECALL)) p &lt;- p + stat_summary(fun.y = mean, geom = &quot;bar&quot;) p &lt;- p + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, aes(width = 0.25)) p &lt;- p + xlab(&quot;Group&quot;) + ylab(&quot;Words recalled&quot;) show(p) and fixing the gap below the zero: # fixing gap below zero: p &lt;- p + scale_y_continuous(expand = c(0, 0)) + expand_limits(y = c(0, 17.5)) show(p) Note that when you present a figure, (A) you need to refer to the figure in the text and (B) you need to provide a figure caption that gives adequate detail. The caption should go below the figure: show(p) Figure 1. Mean words recalled as a function of learning group. Error bars represent standard error. 8.7 DIGGING DEEPER: ANOVA and Regression Now that we’ve got practical matters out of the way, I want to take some time to dig a little deeper into the connections between ANOVA this week and our work on correlations and regressions in the past. As we’ve already mentioned (and spent time discussing in class) ANOVA is just an extension of the simple linear model that we covered last week, where ANOVA is used when our predictors are discrete. In fact aov() is simply a wrapper for the lm() function that we used last week. For example, let’s run our model using lm() and then pipe it into anova() lm.model &lt;- lm(RECALL ~ GROUP_FACTOR, data = dataset) anova(lm.model) ## Analysis of Variance Table ## ## Response: RECALL ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GROUP_FACTOR 4 351.52 87.880 9.0848 1.815e-05 *** ## Residuals 45 435.30 9.673 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All aov() does is take an lm object and produce an ANOVA table from the results. If we were simply to look at the lm() model we see that it gives us the info in our ANOVA table at the end of the summary, including the F-statistic (9.085), the degrees of freedom (4 and 45) and the p-value (1.815e-05): lm(RECALL ~ GROUP_FACTOR, data = dataset) %&gt;% summary() ## ## Call: ## lm(formula = RECALL ~ GROUP_FACTOR, data = dataset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.00 -1.85 -0.45 2.00 9.60 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.0000 0.9835 7.117 6.83e-09 *** ## GROUP_FACTORRhyming -0.1000 1.3909 -0.072 0.943004 ## GROUP_FACTORAdjective 4.0000 1.3909 2.876 0.006138 ** ## GROUP_FACTORImagery 6.4000 1.3909 4.601 3.43e-05 *** ## GROUP_FACTORIntentional 5.0000 1.3909 3.595 0.000802 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.11 on 45 degrees of freedom ## Multiple R-squared: 0.4468, Adjusted R-squared: 0.3976 ## F-statistic: 9.085 on 4 and 45 DF, p-value: 1.815e-05 Taking a look at this output, we see that the lm() model also gives us a lot of other additional useful info. For example the \\(R^2\\) of the model may be understood as the effect size of the ANOVA. However, when we report it for ANOVA we express it as… dun, dun, dunnnn… partial eta-squared!!, or \\(\\eta_p^2\\). Zooming in on the coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.0 0.9835311 7.11721300 6.830993e-09 ## GROUP_FACTORRhyming -0.1 1.3909230 -0.07189471 9.430043e-01 ## GROUP_FACTORAdjective 4.0 1.3909230 2.87578833 6.137702e-03 ## GROUP_FACTORImagery 6.4 1.3909230 4.60126133 3.425167e-05 ## GROUP_FACTORIntentional 5.0 1.3909230 3.59473541 8.019114e-04 We see information about the means of each group relative to the (Intercept). This is why I stressed earlier that it may be useful to rearrange the order of your levels such that R enters your control group into the model first. In this case, the first predictor entered is assigned to the (Intercept). The remaining predictors in the model are then presented relative to the first. Since we entered “Counting” first, the estimate of the (Intercept) represents its mean. The means for each remaining group are the sum of its estimate coefficient and the (Intercept). So for example the mean of the Rhyming group is \\((-0.1)+(7.0)=6.9\\). The coefficients section also gives us one other useful bit of information, the t-values of the estimate. As we learned two weeks ago, for a simple regression the beta coefficient gives us information about the slope of the regression line and the corresponding t-value is a test of the null \\(beta=0\\). So for a simple regression this tells us if our slope is significantly different from 0. A similar logic applies to the ANOVA model. As I mentioned above, deriving the means of each level of our IV is a matter of summing the (Intercept) and the beta estimate for that level. It should be apparent, then, that the beta estimates here represent the slope of a line that between the intercept and the mean of the level (where the distance between the predictor and coefficient is treated as a unit 1). Therefore, a significant t-value for beta tells us that the slope between the two means is significant, or that those two means are significantly different from one another. Keep in mind the only comparisons that are being made here are between the individual levels of our IV and the control (Intercept). So while this output allows us to make a claim about the difference between the means of the Counting (Intercept) group compared to each of the other groups, it does not allow us to make a claim about differences between our other level—for example no information about a statistical test of differences between the Rhyming and Imagery groups is conveyed here. However, assuming you are interested in deviations from your control, you can get info here quickly. There is a caveat here, in that our alpha criterion will need to be adjusted to be more conservative than .05. More on this next week. 8.7.1 Regression method v. means method By now you may ask: “Well, if ANOVA is simply linear regression then why involve different functions?” You may even note (and be peeved about the fact) that the manner which we mathematically derived the F ratio used an entirely different equation than was used this week, even though the both equations yield the same result! Why?!?!?!?!?! Sadly, tradition and convention are all I can tell you. While in both cases the F-value is an expression of amount of variance that your model (lm or aov) explains given total variance that exists in the data. In the ANOVA case we are concerned with the means of discrete groups or IVs. Since lm deals with continuous predictors (or IVs) then it makes little sense to talk about means, other than the grand mean. In both instances, however, the derived F is an expression of the model’s fit. My goal for this section is to demonstrate to you that the regression method we used to derive the the F ratio last week is identical to the means method Howell introduced you to this week. First to recap the regression method, our equation from the correlation week’s lecture slides was: \\[F=\\frac{r^2 / df_{reg}}{(1-r^2)/df_{res}}\\] where \\(r^2\\) is the model’s coefficient of determination \\(df_{reg}\\): the model’s \\(df\\) (based on # of predictors), and \\(df_{res}\\): the model’s residual \\(df\\) moreover, \\[r^2=\\frac{SS_Y-SS_{res}}{SS_Y}\\] where \\(SS_Y\\) is the sum of squared differences between the observed values \\(Y\\) and the grand mean \\(\\bar{Y}\\) \\(SS_{res}\\) is the sum of squared differences between the observed values \\(Y\\) and the model-predicted values \\(\\hat{Y}\\) (or distance to regression line). 8.7.2 Comparing two models To conceptualize lets take another look at our data again. But first it may help to take a look at Section 2.6 on this site where I talk about means and better models. Go ahead… I’ll wait. OK, now that you’re back let’s have another look at our example data. In the plot below, the dotted line represents the grand mean model, where we are using the mean of all data to predict individual scores. This corresponds to our assumption that all samples come from the same population and are ramdomly distributed (aka random error). Is from this means model that \\(SS_Y\\) is calculated. p &lt;- ggplot(data = dataset, mapping = aes(x = GROUP_NUM, y = RECALL)) + geom_point() p &lt;- p + geom_hline(yintercept = mean(dataset$RECALL), linetype = &quot;dashed&quot;) p &lt;- p + geom_smooth(method = &quot;lm&quot;, level = 0.95, se = FALSE) # p &lt;- p + stat_summary(fun.data = mean_se, size=1, color=&#39;red&#39;, # geom=&#39;pointrange&#39;) show(p) The solid line in this plot represents the line of best fit for our ANOVA model which, in addition to assuming random error, also assumes that GROUP (encoding) is a predictor of variation that we see in the data (aka systematic error). Its from this error which \\(SS_{res}\\) is calculated. Thus \\(r^2\\) is simply a statment about the difference in the grand mean model and the experimental model (in this case our ANOVA model with GROUP predicotr) residuals as a percentage of \\(SS_Y\\). In addition, with ANOVA we are dealing with discrete predictors rather than continuous. Visually, as you can see that the data in the plot are stacked into columns, with the mean of each group located at the center of the columns’ density. This also means that rather than a continuous range of predicted values \\(\\hat{Y}\\) we have a set number of \\(\\hat{Y}\\), one for each group mean that we are considering in the ANOVA. Therefore: \\(SS_Y\\) may be thought of in terms of the differences between every score compared to the grand mean. \\(SS_{res}\\) may be thought of as the differences between every score within the group and the group mean \\(\\hat{Y}\\) To put it more succinctly, with ANOVA we are directly comparing our GROUP predictor model’s ability to account for variance in the data above what we might expect by chance. Unpacking this a bit, we have a grand_mean model and a GROUP_mean model. Recall from our discussions in Chi-square that we can use a Likelihood Ratio Test to directly compare to models to see if the second accounts for significantly more variance than the first. In R this is accomplished using…. dun, dun, dun anova(): grand_mean_model &lt;- lm(RECALL ~ 1, data = dataset) # Here, &#39;1&#39;&#39; indicates let scores simply vary randomly GROUP_mean_model &lt;- lm(RECALL ~ 1 + GROUP_FACTOR, data = dataset) # add GROUP as a predictor to our original model # run Likelihood Ratio Test to compare two models anova(grand_mean_model, GROUP_mean_model, test = &quot;LRT&quot;) ## Analysis of Variance Table ## ## Model 1: RECALL ~ 1 ## Model 2: RECALL ~ 1 + GROUP_FACTOR ## Res.Df RSS Df Sum of Sq Pr(&gt;Chi) ## 1 49 786.82 ## 2 45 435.30 4 351.52 2.464e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output above tells you the structure (formula) of each model and that Model 2 accounts for significantly more variance than Model 1 (as assessed on the Chi-sq. distribution). For what it’s worth, let’s run this same comparision of models using a different test, F: # run Likelihood Ratio Test to compare two models anova(grand_mean_model, GROUP_mean_model, test = &quot;F&quot;) ## Analysis of Variance Table ## ## Model 1: RECALL ~ 1 ## Model 2: RECALL ~ 1 + GROUP_FACTOR ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 786.82 ## 2 45 435.30 4 351.52 9.0848 1.815e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Those numbers look familar? What does that suggest is really going on with ANOVA? 8.7.3 Means method calculations Now lets turn to our F-ratio equation from Howell (means method): \\[F=\\frac{MS_{treat}}{MS_{error}}\\] where \\(MS_{error}\\) = \\(SS_{res}/df_{res}\\) and \\(MS_{treat}\\) = (\\((SS_{Y}-SS_{res})/df_{reg}\\); note that \\(df_{res}\\) may be calculated as \\(k(n-1)\\) where \\(k\\) is the number of IV and \\(n\\) is the number of scores in each group; \\(df_{reg}\\) is \\(k-1\\). so for our model, lm.model we can calucalte the F-ratio using the means method as follows: SS_Y &lt;- ((dataset$RECALL - mean(dataset$RECALL))^2) %&gt;% sum() SS_res &lt;- (lm.model$residuals^2) %&gt;% sum() k &lt;- 5 # predictors n &lt;- 10 # number in each group df_res &lt;- k * (n - 1) df_reg &lt;- k - 1 MS_error &lt;- SS_res/df_res MS_treat &lt;- (SS_Y - SS_res)/df_reg F.ratio &lt;- (MS_treat/MS_error) %&gt;% print() ## [1] 9.084769 8.7.4 Bringing it together: it’s all about the residuals, baby So on one hand we have the F-ratio being calculated using \\(r^2\\) and on the other we have it being calculated using the mean square. Underlying both is a calculation of various \\(SS\\) and it is using this fact that we can show that the two methods are equivalent. Drawn out, we have: \\[F=\\frac{r^2 / df_{reg}}{(1-r^2)/df_{res}} =\\frac{MS_{treat}}{MS_{error}}\\] rewriting our \\(r^2\\) in terms of \\(SS\\) (sums of squares) we get: \\[\\frac{\\frac{SS_Y-SS_{res}}{SS_Y} / df_{reg}}{(1-\\frac{SS_Y-SS_{res}}{SS_Y})/df_{res}} =\\frac{MS_{treat}}{MS_{error}}\\] and rewriting our \\(MS\\) side of the equation in terms of \\(SS\\): \\[\\frac{\\frac{SS_Y-SS_{res}}{SS_Y} / df_{reg}}{(1-\\frac{SS_Y-SS_{res}}{SS_Y})/df_{res}} =\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}\\] multiplying the left side of the equation by 1, or \\(\\frac{SS_Y}{SS_Y}\\): \\[\\frac{(SS_Y-SS_{res}) / df_{reg}}{(SS_Y-(SS_Y-SS_{res}))/df_{res}} =\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}\\] which gives us: \\[\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}=\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}\\] What does this mean for you… well perhaps nothing. Honestly, if you are using your computer for stats a correct calculation is a correct calculation. However, conceptually it may be easier to get and keep yourself in general linear model mode, as it will make incorporating more complex modeling techniques (higher-order ANOVA, ANCOVA, mixed-models, multi-level models, growth curve models) a more intuitive step in the future. "],
["analysis-of-variance-ii-multiple-comparisons-in-one-way-anova.html", "Week 9 Analysis of Variance II: Multiple comparisons in One-way ANOVA 9.1 Getting started (loading packages and data) 9.2 Running the One-way ANOVA 9.3 Post-hoc tests 9.4 The logic of Planned contrasts 9.5 Performing planned contrasts in R 9.6 Reporting your results 9.7 A note on using the multcomp package v. using the base stats method", " Week 9 Analysis of Variance II: Multiple comparisons in One-way ANOVA In last weeks vignette we covered One-Way ANOVA. ANOVA is useful when we are comparing 3 or more group means such that the null hypothesis is: \\[\\mu_1=\\mu_2=\\mu_3...=\\mu_n\\]. In this case, if a single mean is revealed to be significantly different from the others, then the null is rejected. However, rejecting the null only tells us that at least one mean was different from the others; it does not tell us which one or how many. For example with just three means, it could be the case that: \\(\\mu_1≠\\mu_2=\\mu_3\\) \\(\\mu_1=\\mu_2≠\\mu_3\\) \\(\\mu_1=\\mu_3≠\\mu_2\\) \\(\\mu_1≠\\mu_2≠\\mu_3\\) Simply getting a significant F-value does not tell us this at all. In order to suss out any differences in our groups we are going to need to make direct comparisons between them. Enter multiple contrasts. Multiple contrasts are a way of testing the potential inequalities between group means like those above. As always, both Howell (Chapter 12) and Field (Chapter 10, specifically 10.4+) do wonderful jobs of laying out the mathematics and logic of multiple comparisons. As with last week I focus on practical implementation and spend some time focusing a bit on potential landmines and theoretical concerns as I see them. 9.1 Getting started (loading packages and data) This vignette assumes that you have the following packages installed and loaded in R: # check to make your computer has pacman installed, if not install if (!require(pacman)) { install.packages(&quot;pacman&quot;) } # use pacman to check, install, and load necessary packages pacman::p_load(agricolae, cowplot, tidyverse, multcomp, psych) To start, lets download Siegel’s (1975) data set on Morphine Tolerance. This data set can be found on Howell’s website. Please check the Howell text for background info: # grab data from online location: dataset &lt;- read_table2(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab12-1.dat&quot;) ## Parsed with column specification: ## cols( ## ID = col_character(), ## Group = col_integer(), ## Time = col_integer() ## ) # convert dataset$Group dummycodes to named factor levels: dataset$Group &lt;- recode_factor(dataset$Group, `1` = &quot;MS&quot;, `2` = &quot;MM&quot;, `3` = &quot;SS&quot;, `4` = &quot;SM&quot;, `5` = &quot;McM&quot;) # get descriptive stats for this data by Group psych::describeBy(dataset$Time, dataset$Group) ## ## Descriptive statistics by group ## group: MS ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 8 4 3.16 3.5 4 3.71 1 9 8 0.43 -1.59 1.12 ## -------------------------------------------------------- ## group: MM ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 8 10 5.13 10.5 10 4.45 2 19 17 0.15 -0.99 1.81 ## -------------------------------------------------------- ## group: SS ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 8 11 6.72 10.5 11 8.15 3 21 18 0.23 -1.69 2.38 ## -------------------------------------------------------- ## group: SM ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 8 24 6.37 23 24 5.93 17 36 19 0.59 -1.07 2.25 ## -------------------------------------------------------- ## group: McM ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 8 29 6.16 28.5 29 5.93 20 40 20 0.28 -1.07 2.18 And a quick peek at this data: ggplot(data = dataset, aes(x = Group, y = Time)) + stat_summary(fun.y = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, aes(width = 0.25)) + scale_y_continuous(expand = c(0, 0)) + expand_limits(y = c(0, 35)) + theme_cowplot() 9.2 Running the One-way ANOVA Now that our data is properly coded we can run our omnibus ANOVA. My own personal preference is to run the ANOVA using lm(). This makes like a lot easier when dealing with contrasts, especially if you decide to employ the method that Field suggests in his guide. I’ll mention more on this alternative below. That said. recall from last week that using the aov() function gives you the same result. Depending on which you choose, you can use the summary(lm.model) or anova(lm.model) to switch back and forth to get the info that you desire: # running the ANOVA using lm: lm.model &lt;- lm(formula = Time ~ Group, data = dataset) # using the summary.aov() function to display as ANOVA table anova(lm.model) ## Analysis of Variance Table ## ## Response: Time ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Group 4 3497.6 874.4 27.325 2.443e-10 *** ## Residuals 35 1120.0 32.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # summary(lm.model) # alternative output So we see here that we have: \\(F(4,35)=27.33,p&lt;.001,\\eta_p^2=.75\\) Remember again that the only thing that the omnibus ANOVA tells us is that there is an inequality in our means. In this respect, the omnibus begs more questions than it answers—which means are different from which. In order to get this answer we need to run direct comparisons between our means. There are two ways of going about this, we can either (1) plan beforehand what differences in means are especially relevant for us and focus on those, or (2) take a look at all potential differences without any specified predictions. In Case 1, we are performing planned contrasts; in Case 2, we use post hoc tests. More often than not, you will see researchers analyzing differences in means using post hoc tests—that is they run the ANOVA, find that it is significant, and run a battery of pairwise comparisons. It is sometimes the case that of that battery of comparisons, only a select few are actually theoretically relevant. However, if there is a theory-driven case to be made that you are predicting differences between a few select means in your data, then there is an argument to be made that you should run your planned contrasts independent of your ANOVA. That is, you are technically only permitted to run post-hoc tests if your ANOVA is significant (you can only go looking for differences in means if your ANOVA tells you that they exist), whereas planned contrasts can be run regardless of the outcome of the omnibus ANOVA (indeed, some argue that they obviate the need to run the omnibus ANOVA altogether). My guess is that most of you have experience with post-hoc tests. They are more commonly performed tend to be touched upon in introductory stats courses. So we will spend a little time on these first before proceeding to a more in depth treatment of planned contrasts. 9.3 Post-hoc tests We use a post-hoc test when we want to test for differences in means that we have not explicitly predicted prior to conducting our experiment. As a result, whenever we perform a post-hoc test, we need to adjust our critical p-values to correct for inflation of Type 1 error. Recall from earlier discussions that the odds of committing a Type 1 error (falsely rejecting the null) is \\(1-(1-\\alpha)^c\\) where \\(\\alpha\\) is you critical p-value and \\(c\\) is the number of comparisons that are to be performed. Typically we keep this at .05, so when conducting a single test, the likelihood of committing a Type 1 error is: \\(1-(1-.05)^1=1-0.95^1=0.05\\) However as we increase the number of comparisons, assuming an \\(\\alpha\\) of 0.05: 2 comparisons = \\(1-.95^2=0.0975\\) 3 comparisons = \\(1-.95^3=0.1426\\) 4 comparisons = \\(1-.95^4=0.1855\\) 5 comparisons = \\(1-.95^5=0.2262\\) Obviously, we need to control for this. The post-hoc methods that were introduced this week are all similar in that they involve comparing two means (a la t-test) but differ in how the error is controlled. For example a Bonferroni-Dunn correction (which is often used as a post-hoc correction, although initially intended for correcting planned comparisons) adjusts for this by partitioning the significance (by diving your original alpha by the number of comparisons). A popular variant of this method, the Holm test, is a multistage test. It proceeds by ordering the obtained t-values from smallest to largest. We then evaluate the largest t according to the Bonferroni-Dunn correction \\(\\alpha/c\\). Each subsequent comparison t value, \\(n\\) is evaluated against the correction \\(\\alpha/(c-n)\\). Please note I mention the these two methods with post-hoc analyses, although in true they are intended for planned comparisons. However, in instances in which the number of comparisons is relatively small, I’ve often seen them employed as post-hocs. So how many comparisons is relatively small? I’d suggest best form is to use the above methods when you have 5 or fewer comparisons, meaning that your critical \\(\\alpha\\) is .01. That said, with a post hoc test, you really do not have a choice in the number of comparisons you can make, you need to test for all possible comparisons on the IV. Why? well if not you are simply cherry picking your data. For example it would be poor form to run our ANOVA and plot your data like so: p &lt;- ggplot(data = dataset, aes(x = Group, y = Time)) + stat_summary(fun.y = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, aes(width = 0.25)) + scale_y_continuous(expand = c(0, 0)) + expand_limits(y = c(0, 35)) + theme_cowplot() show(p) and then decide that you only want to compare ‘McM’ to ‘MS’ because that’s where you see the greatest differences. Or that you simply want to take a look at “MM” and “SS” without considering the rest. Since you did not plan for or explicitly predict these differences from the outset, you are simply banking on what I like to say might be a “historical accident”, that you simply stumbled into these results. As such, it’s deemed as proper for to test all contingencies. In the case above there are \\((5!)/(2!)(5-2)!\\) = 10 combinations. If we were to run a Bonferroni correction in this case or critical p would need to be \\(.05/10=.005\\) which is an extremely conservative value, and thus dramatically inflates the likelihood of Type II error. In cases like this Tukey’s HSD is the traditionally preferred method, as it takes into account the characteristics of your data (in particular the standard error of the distribution) when calculating the critical p value. As such in cases where many post-hoc, pairwise comparisons are made, Tukey’s HSD is less conservative than a Bonferroni adjustment. One final method that is becoming more en vogue is the Ryan, Einot, Gabriel, Welsch method (REGWQ). Whereas Tukey’s method holds the critical p constant for all comparisons (at the loss of power) the REGWQ allows for an adjustment for the number of comparisons. It is currently being promoted as the most desirable post-hoc method. 9.3.1 Bonferonni-Dunn and Holm tests In R there are several ways in which we can call post hoc corrections. For example we can call the Bonferonni and Holm adjustments using pairwise.t.test() function from the base package (already installed). The pairwise.t.test() method asks you to input: x = your DV g = your grouping factor p.adjust.method = the name of your desired correction in string format First let’s run the pairwise.t.tests with no adjustment (akin to uncorrected p values): pairwise.t.test(x = dataset$Time, g = dataset$Group, p.adjust.method = &quot;none&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: dataset$Time and dataset$Group ## ## MS MM SS SM ## MM 0.041 - - - ## SS 0.018 0.726 - - ## SM 3.1e-08 1.9e-05 5.4e-05 - ## McM 1.9e-10 8.9e-08 2.6e-07 0.086 ## ## P value adjustment method: none You see above that we get a cross-matrix containing the p values for each cross pair (row × column). Remember this is something we would never do in a post hoc (no corrections) but I wanted to first run this to illustrate a point. Now let’s run the the Bonferroni and Holm corrections: 9.3.1.1 Bonferroni example (pairwise.t.test()) pairwise.t.test(x = dataset$Time, g = dataset$Group, p.adjust.method = &quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: dataset$Time and dataset$Group ## ## MS MM SS SM ## MM 0.41051 - - - ## SS 0.18319 1.00000 - - ## SM 3.1e-07 0.00019 0.00054 - ## McM 1.9e-09 8.9e-07 2.6e-06 0.85818 ## ## P value adjustment method: bonferroni 9.3.1.2 Holm example (pairwise.t.test()) pairwise.t.test(x = dataset$Time, g = dataset$Group, p.adjust.method = &quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: dataset$Time and dataset$Group ## ## MS MM SS SM ## MM 0.12315 - - - ## SS 0.07327 0.72579 - - ## SM 2.8e-07 0.00011 0.00027 - ## McM 1.9e-09 7.1e-07 1.8e-06 0.17164 ## ## P value adjustment method: holm As you can see in the remaining tables, R actually adjusts the p values for you. This is different from (but analogous to) the critical t-value method described in Howell’s text. What this means is that you may interpret the output against your original (familywise) \\(\\alpha\\). So here, any values that are still less than .05 after the corrections are significant. 9.3.2 Tukey HSD and REGWQ tests In order to run Tukey’s HSD and REGWQ methods we call upon the agricolae package. In this case, we need to input our lm() model into the function, as well as identify our “treatment” (in this case our “Group” factor). For example: 9.3.2.1 Tukey HSD example (agricolae) lm.model &lt;- lm(formula = Time ~ Group, data = dataset) # from above agricolae::HSD.test(lm.model, trt = &quot;Group&quot;, group = T, console = T) ## ## Study: lm.model ~ &quot;Group&quot; ## ## HSD Test for Time ## ## Mean Square Error: 32 ## ## Group, means ## ## Time std r Min Max ## McM 29 6.164414 8 20 40 ## MM 10 5.126960 8 2 19 ## MS 4 3.162278 8 1 9 ## SM 24 6.369571 8 17 36 ## SS 11 6.718843 8 3 21 ## ## Alpha: 0.05 ; DF Error: 35 ## Critical Value of Studentized Range: 4.065949 ## ## Minimun Significant Difference: 8.131899 ## ## Treatments with the same letter are not significantly different. ## ## Time groups ## McM 29 a ## SM 24 a ## SS 11 b ## MM 10 b ## MS 4 b Note that the group and console arguments pertain to the output. You typically will want to keep console set to TRUE as that simply prints the output of your test. The group argument controls how the output is presented. Above we set it to TRUE. This results in an output that groups the treatment means into subsets where treatments with the same letter are not significantly different from one another (i.e., as are not significantly different from each other, bs are not significantly different from each other, but as are different from bs). Conversely if you wanted to see each comparison you can set this to FALSE: agricolae::HSD.test(lm.model, trt = &quot;Group&quot;, group = FALSE, console = TRUE) ## ## Study: lm.model ~ &quot;Group&quot; ## ## HSD Test for Time ## ## Mean Square Error: 32 ## ## Group, means ## ## Time std r Min Max ## McM 29 6.164414 8 20 40 ## MM 10 5.126960 8 2 19 ## MS 4 3.162278 8 1 9 ## SM 24 6.369571 8 17 36 ## SS 11 6.718843 8 3 21 ## ## Alpha: 0.05 ; DF Error: 35 ## Critical Value of Studentized Range: 4.065949 ## ## Comparison between treatments means ## ## difference pvalue signif. LCL UCL ## McM - MM 19 0.0000 *** 10.868101 27.131899 ## McM - MS 25 0.0000 *** 16.868101 33.131899 ## McM - SM 5 0.4078 -3.131899 13.131899 ## McM - SS 18 0.0000 *** 9.868101 26.131899 ## MM - MS 6 0.2340 -2.131899 14.131899 ## MM - SM -14 0.0002 *** -22.131899 -5.868101 ## MM - SS -1 0.9965 -9.131899 7.131899 ## MS - SM -20 0.0000 *** -28.131899 -11.868101 ## MS - SS -7 0.1198 -15.131899 1.131899 ## SM - SS 13 0.0005 *** 4.868101 21.131899 Finally, if you do decide to group (group=TRUE), you can take the outcome of this function and use it to generate a nice group plot. This is useful for quick visual inspection. agricolae::HSD.test(lm.model, trt = &quot;Group&quot;, group = T, console = T) %&gt;% plot() ## ## Study: lm.model ~ &quot;Group&quot; ## ## HSD Test for Time ## ## Mean Square Error: 32 ## ## Group, means ## ## Time std r Min Max ## McM 29 6.164414 8 20 40 ## MM 10 5.126960 8 2 19 ## MS 4 3.162278 8 1 9 ## SM 24 6.369571 8 17 36 ## SS 11 6.718843 8 3 21 ## ## Alpha: 0.05 ; DF Error: 35 ## Critical Value of Studentized Range: 4.065949 ## ## Minimun Significant Difference: 8.131899 ## ## Treatments with the same letter are not significantly different. ## ## Time groups ## McM 29 a ## SM 24 a ## SS 11 b ## MM 10 b ## MS 4 b 9.3.2.2 REGWQ example (agricolae) The same applies to REGW, using the REGW.test() function (with group=F, I’m showing all of the comparisons): agricolae::REGW.test(lm.model, trt = &quot;Group&quot;, group = F, console = T) ## ## Study: lm.model ~ &quot;Group&quot; ## ## Ryan, Einot and Gabriel and Welsch multiple range test ## for Time ## ## Mean Square Error: 32 ## ## Group, means ## ## Time std r Min Max ## McM 29 6.164414 8 20 40 ## MM 10 5.126960 8 2 19 ## MS 4 3.162278 8 1 9 ## SM 24 6.369571 8 17 36 ## SS 11 6.718843 8 3 21 ## ## Comparison between treatments means ## ## difference pvalue signif. LCL UCL ## McM - MM 19 0.0000 *** 12.1234674 25.876533 ## McM - MS 25 0.0000 *** 17.4611210 32.538879 ## McM - SM 5 0.3056 -2.6279930 12.627993 ## McM - SS 18 0.0000 *** 9.8681013 26.131899 ## MM - MS 6 0.0995 . -0.8765326 12.876533 ## MM - SM -14 0.0001 *** -21.5388790 -6.461121 ## MM - SS -1 0.9846 -8.6279930 6.627993 ## MS - SM -20 0.0000 *** -26.8765326 -13.123467 ## MS - SS -7 0.0771 . -14.5388790 0.538879 ## SM - SS 13 0.0001 *** 6.1234674 19.876533 9.4 The logic of Planned contrasts If you have reasons to predict differences between particular sets of group means that are theory-driven, then you may perform a priori or planned contrasts. Logically, planned contrasts are similar to post-hoc tests in that we are comparing against two means, but there are some differences that make planned contrasts more powerful. Since your predictions are made prior to collecting data you, technically do not need to get a significant result on the omnibus ANOVA to run your contrasts. Remember, when doing post-hoc tests, if the omnibus ANOVA fails to reject the null, you are not permitted to run follow-up post hoc tests. Since you are making predictions prior to seeing the outcome of your observed data, then you are safe to make a limited number of comparisons without the charge of cherry-picking. For example, if you have predicted-ahead that there would be differences between groups “MS” and “McM”, or groups “MM” and “MS” then you are free to run those comparisons and those comparisons-only. This is especially useful, since by limiting the number of comparisons, you can effectively reduce the problem of Type I error inflation while limiting the possibiliy of Type II error, keeping the required corrections relatively minimal. For example, recall that using a Bonferonni correction on this data in the post-hoc case mandates that I use an adjusted \\(\\alpha\\) of .005 (.05/10 comparisons), even if I was only really interested in these two comparisons. Here, I am allowed to only perform these two, so my adjusted p is .025. Depending on the number of comparisons, you may be justified in not performing any p correction at all. For example some recommend no need for correction if the number of contrasts is low or when the comparisons are complementary (e.g. orthogonal). See here, here, and here for discussion of this issue. We can easily perform a variety of comparisons using planned contrasts. For example, say we are interested in whether MS is different from the mean of the remaining groups (McM+MM+SM+SS), or that MM+MS is different from McM+SM+SS. We can test this using planned contrasts. 9.5 Performing planned contrasts in R As outlined in both Howell and Field, planned contrasts begin by creating contrast weights. The idea with contrast weights is that the groups that are being compared should sum to equal -1 and +1 respectively, resulting in a null test against 0 (i.e. the two weighted means are equal). Any groups that are not being included in the comparison should be assigned coefficients of 0. For example assume we are comparing “MS” to “MM” and were not concerned with the remaining groups. Then our contrast weights would be: MS = +1 MM = -1 McM = SM = SS = 0 How about we want to compare McM + MM against the remaining three groups? MS + MM = 1 McM + SM + SS = -1 From there we distribute the weight equally between the number of groups on each side of the contrast, so MS = +1/2; MM = +1/2 McM = -1/3; SM = -1/3; SS = -1/3 in R we can construct each of these contrasts like so: # checking the order of groups: levels(dataset$Group) ## [1] &quot;MS&quot; &quot;MM&quot; &quot;SS&quot; &quot;SM&quot; &quot;McM&quot; # building contrasts # MS v. MM contrast1 &lt;- c(1, -1, 0, 0, 0) # MS + MM v. SS + SM + McM contrast2 &lt;- c(1/2, 1/2, -1/3, -1/3, -1/3) When performing planned contrasts in R I recommend using the multicomp package rather than the base package examples in the Field text. For those that are interested as to why, in my experience the base method has a hard time dealing with non-orthogonal contrasts. To perform the contrast requires 3 steps: run your omnibus ANOVA lm() model and save it as an object input your lm() model into the glht() function, specifying the name of your IV and planned contrasts; save this object run a summary() of your object from step 2, including any desired adjustments. For example, say I wanted to run contrast1 from my data, Comparing “MS” to “MM”. lm.model &lt;- lm(formula = Time ~ Group, data = dataset) contrast.model &lt;- multcomp::glht(lm.model, linfct = mcp(Group = contrast1)) # where Group is the name of my IV, and contrast1 are my planned contrasts from # above summary(contrast.model, test = adjusted(&quot;bonferroni&quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: lm(formula = Time ~ Group, data = dataset) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 -6.000 2.828 -2.121 0.0411 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- bonferroni method) The above result tells me: Estimate: the difference in means between my contrasted groups Std. Error: a measure of variability t-value: obtained from the t-test between groups Pr(&gt;|t|): the resulting p-value You’ll also note in the output the specified correction method is mentioned (in this case Bonferroni) Based upon these results I conclude that “MS” and “MM” are significantly different from one another. Suppose I wanted to run both simultaneously? My preferred method is to create a contrast matrix by rbind() my contrasts. You can then place that matrix object into glht(): # create contrast matrix contrast.matrix &lt;- rbind(contrast1, contrast2) # run contrasts lm.model &lt;- lm(formula = Time ~ Group, data = dataset) contrast.model &lt;- multcomp::glht(lm.model, linfct = mcp(Group = contrast.matrix)) # where Group is my IV, and contrast.matrix are my planned contrasts summary(contrast.model, test = adjusted(&quot;bonferroni&quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: lm(formula = Time ~ Group, data = dataset) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## contrast1 == 0 -6.000 2.828 -2.121 0.0821 . ## contrast2 == 0 -14.333 1.826 -7.851 6.32e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- bonferroni method) You’ll note that the reported p.value for contrast1 has changed from the previous example, due to the Bonferroni correction (in this case we have 2 tests). To get a list and description of accepted adjustment methods type ? adjusted in your console. 9.5.1 Making the output easier to read One thing you may have noticed above is that your contrasts aren’t labelled very transparently. Looking at the output you would have to remember what was being contrasted in contrast1 and contrast2. You can save yourself a little headache if you label your contrasts while constructing the matrix. For example, here I’m performing the linear contrasts from the Howell text: contrast1 &lt;- c(0, -1, 0, 0, 1) contrast2 &lt;- c(-1, 0, 1, 0, 0) contrast3 &lt;- c(0, 1, -1, 0, 0) contrast4 &lt;- c(-1/3, -1/3, -1/3, 1/2, 1/2) From here you can modify the previous code to assign names to the rows in your contrast matrix: contrast.matrix &lt;- rbind(`MM v. McM` = contrast1, `MS v. SS` = contrast2, `MM v. SS` = contrast3, `MS+MM+SS v. SM+McM` = contrast4) And now running these contrasts (this time using a Holm adjustment): lm.model &lt;- lm(formula = Time ~ Group, data = dataset) contrast.model &lt;- multcomp::glht(lm.model, linfct = mcp(Group = contrast.matrix)) # where Group is my IV, and contrast1 are my planned contrasts from above summary(contrast.model, test = adjusted(&quot;holm&quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: lm(formula = Time ~ Group, data = dataset) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## MM v. McM == 0 19.000 2.828 6.718 2.66e-07 *** ## MS v. SS == 0 7.000 2.828 2.475 0.0366 * ## MM v. SS == 0 -1.000 2.828 -0.354 0.7258 ## MS+MM+SS v. SM+McM == 0 18.167 1.826 9.950 3.86e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- holm method) For practice, try re-running this last analysis but using the Bonferroni adjustment instead of Holm. Note how that changes the p-values for some of your contrasts. 9.5.2 Calculating your effect size Typically when reporting the effect size off the difference between two means we use Cohen’s D. However, calculating Cohen’s D in a planned contrast is slighly more involved than the method used for a regular t-test. This is because with a regular t-test you only have 2 means from 2 samples that you have collected. In the case of Planned Contrasts in ANOVA, while you are only comparing two means, those means are nested within a larger group (e.g., comparing MS and MM, we still need to account for the fact that we also collected samples from SS, SM, and McM) or may be derived from multiple samples (e.g., contrasting the mean of MS + MM against the mean of SS + SM + McM). Simply put, in our calculations we need to account for the influence of all of our collected groups. This is done by placing the contrasted difference in the context of the Root Mean Square Error, or the square root of the Mean Square Error of the residuals in our ANOVA model. To do this we need two things: a vector containing our contrasts— e.g., c(-1/2,-1/2,1/3,1/3,1/3) the mean of each group the Mean Square Error of the residuals from anova() So to calculate Cohen’s d for the contrast of our first two groups (MS + SM) against our last three groups (SS + SM + McM): # contrast vector: MS + MM + SS v. SM + McM contrast_vector &lt;- c(-1/3, -1/3, -1/3, 1/2, 1/2) # using by() to create a vactor of means means &lt;- by(data = dataset$Time, INDICES = dataset$Group, FUN = mean) # running an anova() of the omnibus model aov.table &lt;- lm.model %&gt;% anova() # get the sqrt of the `Mean Sq` of residuals RMSE &lt;- sqrt(aov.table[&quot;Residuals&quot;, &quot;Mean Sq&quot;]) # calculate d from formula see Howell, Ch. 12 d &lt;- sum(contrast_vector * means)/RMSE print(d) ## [1] 3.211443 Easy as that. You’ll notice above that I pulled out the Mean Sq of the Residuals from my aov.table using the indexing method instead of the names method. In the past we’ve covered how to call rows and columns by their index numbers. If the rows and columns also have names assigned to them you can use those as well. To see the associated names, try: rownames(aov.table) ## [1] &quot;Group&quot; &quot;Residuals&quot; colnames(aov.table) ## [1] &quot;Df&quot; &quot;Sum Sq&quot; &quot;Mean Sq&quot; &quot;F value&quot; &quot;Pr(&gt;F)&quot; In this case I prefer this method as it always ensures that I get the correct value. In contrast, depending on the number of factors the Residuals may be on the 2nd row (as in this case), 3rd, 4th, or 5th row of the Mean Sq column. 9.6 Reporting your results When reporting your results, we typically report: the omnibus ANOVA a statement on what multiple comparisons were run and how corrected note which comparisons were significant. Since our comparisons, if corrected, rely on adjusted p.values its kosher to simply state that \\(p&lt;\\alpha\\). If you elect to use the actual p values, then you need to note that they are corrected. So for post hocs, something analogous to: …To test our hypothesis, we conducted a One-way ANOVA assessing latency as a function of morphine group. This ANOVA was significant—\\(F(4,35)=27.33,p&lt;.001,\\eta_p^2=.75\\). As can be seen in Figure 1, pairwise Tukey HSD tests revealed that SS, MM, and MS groups were not significantly different from one another, but were significantly less than McM and SM (p&lt;.05). For planned comparisons, we might say: Our prevailing hypothesis predicted that response latency for groups SS, MM, and MS would be significantly less than groups SM and McM. To test this hypothesis we performed a planned contrast (no corrections). Our results revealed statistically significant difference between these groups, t(35) = 9.95, p &lt; .001, d = 3.21, where the responce latency in MS, MM, and SS was lower than SM and McM. Note that the df in the t-test are the error df from the omnibus model (35). 9.7 A note on using the multcomp package v. using the base stats method You may have noticed as you were moving through the Field text that he recommends a different method for constructing contrasts. The base method that he uses has you apply the contrast matrix directly to the factor, then re-run the ANOVA in lm() as opposed to the method I’ve outline above. Of course, Field’s is appropriate and conceptually it may give you a better understanding of what is going on. In what follows I briefly walk through othe considerations if you use the stats::contrasts() method. For most of you, you can simply stop right here and things will be fine. For those interested in an alternative, press on… There is one crucial factor that you must consider when using the stats::contrasts() method—your number of contrasts cannot be larger that the number of IV levels minus 1. If you have too many contrasts (can only happen if using non-orthogonal contrasts) then you have to go through several steps to derive the inverse of the matrix using the Moore-Penrose inversion method, which depending on how constructed may still not work if the determinant of your matrix = 0. In other words, you don’t want to do that! R automatically defaults to creating (number of IV levels) minus 1 contrasts. So if you intend to run fewer, then you need to be wary that you can only trust the contrasts that you specified. To see what I’m getting at, let’s use the stats::contrasts() method (no need to load stats, it’s already loaded when you start R). To demonstrate the issue at hand let’s start with only one contrast. But first let’s run a regular ANOVA # creating lm.model for comparison: lm.model &lt;- lm(Time ~ Group, data = dataset) summary(lm.model) ## ## Call: ## lm(formula = Time ~ Group, data = dataset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.00 -3.25 0.00 3.00 12.00 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.000 2.000 2.000 0.0533 . ## GroupMM 6.000 2.828 2.121 0.0411 * ## GroupSS 7.000 2.828 2.475 0.0183 * ## GroupSM 20.000 2.828 7.071 3.09e-08 *** ## GroupMcM 25.000 2.828 8.839 1.93e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.657 on 35 degrees of freedom ## Multiple R-squared: 0.7574, Adjusted R-squared: 0.7297 ## F-statistic: 27.32 on 4 and 35 DF, p-value: 2.443e-10 And now let’s re-run after applying our contrasts to Group: # create contrast contrast1 &lt;- c(0, -1, 0, 0, 1) # MM v McM # assign that contrast to our IV: contrasts(dataset$Group) &lt;- contrast1 # re-running lm model after assigning contrasts: contrast.model &lt;- lm(Time ~ Group, data = dataset) summary(contrast.model) ## ## Call: ## lm(formula = Time ~ Group, data = dataset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.00 -3.25 0.00 3.00 12.00 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.6000 0.8944 17.441 &lt; 2e-16 *** ## Group1 9.5000 1.4142 6.718 8.87e-08 *** ## Group2 -1.0154 2.0000 -0.508 0.615 ## Group3 11.9846 2.0000 5.992 7.90e-07 *** ## Group4 10.5848 2.0000 5.292 6.62e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.657 on 35 degrees of freedom ## Multiple R-squared: 0.7574, Adjusted R-squared: 0.7297 ## F-statistic: 27.32 on 4 and 35 DF, p-value: 2.443e-10 When comparing or original lm.model to the model run after applying the contrasts we see that the latter gives us a different result—the contrasts that we requested. Actually, as you can see it give us more that we requested. We only requested 1 contrast, it supplies us with 4. To see what has happened here let’s take a look at what contrasts we indeed applied to Group: # setting contrasts contrasts(dataset$Group) &lt;- contrast1 # looking at the set contrast matrix attributes(dataset$Group) ## $levels ## [1] &quot;MS&quot; &quot;MM&quot; &quot;SS&quot; &quot;SM&quot; &quot;McM&quot; ## ## $class ## [1] &quot;factor&quot; ## ## $contrasts ## [,1] [,2] [,3] [,4] ## MS 0 -0.4472136 -0.4472136 -0.6324555 ## MM -1 -0.1381966 -0.1381966 0.5116673 ## SS 0 0.8618034 -0.1381966 -0.1954395 ## SM 0 -0.1381966 0.8618034 -0.1954395 ## McM 1 -0.1381966 -0.1381966 0.5116673 Column 1 contains the contrast that we requested. Columns 2-4 contain non-orthogonal contrasts that R has generated. So, when reading the resulting lm() output, you should only focus on the first contrast that is reported. "],
["analysis-of-varience-iii-factorial-anova.html", "Week 10 Analysis of Varience III: Factorial ANOVA 10.1 Main effect, main effect, and interactions… oh my! 10.2 Example: a 2×2 ANOVA 10.3 Making sense of plots 10.4 Running the ANOVA lm() method 10.5 Running the ANOVA in the afex::aov_ez() method: 10.6 Getting cell means 10.7 More examples: a 2 × 3 ANOVA 10.8 2 × 5 ANOVA (Howell 13.1) 10.9 One last example, 10.10 Interaction v. no interaction 10.11 What about planned contrasts?", " Week 10 Analysis of Varience III: Factorial ANOVA In this week’s vignette we are simply building upon the previous two weeks coverage of One-way ANOVA and multiple comparisons. I’m assuming you’ve taken a look at all of the assigned material related to these topics. This week we up-the ante by introducing more complex ANOVA models, aka factorial design. As we discussed in lecture, a factorial ANOVA design is required (well, for the purposes of this course) when your experimental design has more than one IV. Our examples this week focus on situations involving two IVs, however, what is said here applies for more complex designs involving 3, 4, 5, or however many IV’s you want to consider. Well, maybe not however many… as we we’ll see this week and the next, the more IVs you include in your analysis, the more difficult interpreting your results becomes. This is especially true if you have interaction effects running all over the place. But perhaps I’m getting a little bit ahead of myself. Let’s just way I wouldn’t recommend including more than 3 or 4 IVs in your ANOVA at a single time and for now leave it at that. Note that this week’s vignette assumes you have the following packages: pacman::p_load(afex, # a new way to do ANOVA emmeans, # a different way to do contrasts multcomp, ez, # get info on design of ANOVA, also can run ezANOVA Rmisc, # getting summary data cowplot, tidyverse, psych) 10.1 Main effect, main effect, and interactions… oh my! When we are performing a factorial ANOVA we are performing a series of independent comparisons of means as a function of our IVs (this assumption of independence is one of the reasons that we don’t typically concern ourselves with adjusting our p-values in the omnibus factorial ANOVA). For any given number of IVs, or factors, we test for a main effect of that factor on the data—that is “do means grouped by levels within that factor differ from one another not taking into consideration the influence of any of the other IVs. Our tests for interactions do consider the possibility that our factors influence one another—that is,”do the differences that are observed in one factor depend on the intersecting level of another?&quot; For the sake of simplicity, we will start with a 2 × 2 ANOVA and work our way up by extending the data set. Given our naming conventions, saying that we have a 2 × 2 ANOVA indicates that there are 2 IVs and each has 2 levels. A 2 × 3 ANOVA indicates that there are 2 IVs, and that one IV has 2 levels and the other has 3 levels; a 2 × 3 × 4 ANOVA indicates that we have 3 IVs, the first has 2 levels, the second has 3 levels, and the third has 4 levels. Our example ANOVA comes from Howell (13.5), testing the effects of smoking on performance in different types of putatively (I’m showing my biases here) information processing tasks. There were 3 types of cognitive tasks: the first, a pattern recognition task where participants had to locate a target on a screen; the second, a cognitive task where participants had to read a passage and recall bits of information from that passage later; and the third, participants performed a driving simulation. Three groups of smokers were recruited— those that were actively smoking prior to and during the experiment; those that were smokers, but did not smoke 3 hours prior to the experiment; and finally non-smokers. As this is a between design, each participants only completed one of the cognitive tasks. 10.2 Example: a 2×2 ANOVA Let’s grab the data from Howell’s website. Note that for now we are going to ignore the covar column: dataset &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Sec13-5.dat&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Parsed with column specification: ## cols( ## Task = col_integer(), ## Smkgrp = col_integer(), ## score = col_integer(), ## covar = col_integer() ## ) dataset$Task &lt;- recode_factor(dataset$Task, `1` = &quot;Pattern Recognition&quot;, `2` = &quot;Cognitive&quot;, `3` = &quot;Driving Simulation&quot;) dataset$Smkgrp &lt;- recode_factor(dataset$Smkgrp, `1` = &quot;Nonsmoking&quot;, `2` = &quot;Delayed&quot;, `3` = &quot;Active&quot;) To get a quick view of our data structure we can use two kinds of calls: summary() provides us with info related to each column in the data frame. If a column contains a factor it provides frquency counts of each level. It the column is numeric it provides summary stats: summary(dataset) ## Task Smkgrp score covar ## Pattern Recognition:45 Nonsmoking:45 Min. : 0.00 Min. : 64.0 ## Cognitive :45 Delayed :45 1st Qu.: 6.00 1st Qu.: 99.0 ## Driving Simulation :45 Active :45 Median :11.00 Median :111.0 ## Mean :18.26 Mean :112.5 ## 3rd Qu.:26.00 3rd Qu.:128.0 ## Max. :75.00 Max. :191.0 In addition, I like to use the ezDesign() function from the ez package to get a feel for counts in each cell. This is useful for identifying conditions that may have missing data. ez::ezDesign(data = dataset, x = Task, y = Smkgrp, row = NULL, col = NULL) This provides me with a graphic representation of cell counts. In this case, every condition (cell) has 15 participants. As you can see right now this is a 3 x 3 ANOVA. To start, let’s imagine that we are only comparing the active smokers to the nonsmokers, and that we are only concerned with the pattern recognition v driving simulation. In this circumstance we are running a 2 (smoking group: active v. passive) × 2 (task: pattern recognition v. driving simulation) ANOVA. We can do a quick subsetting of this data using the filter() command. For our sake, let’s create a new object with this data, dataset_2by2: # subsetting the data. Remember that &#39;!=&#39; means &#39;does not equal&#39;; &#39;&amp;&#39; suggests # that both cases must be met, so dataset_2by2 &lt;- filter(dataset, Smkgrp != &quot;Delayed&quot; &amp; Task != &quot;Cognitive&quot;) To get a quick impression of what this dataset looks like, we can use the summary() function, or ezDesign(): # getting a summary of dataset_2by2: summary(dataset_2by2) ## Task Smkgrp score covar ## Pattern Recognition:30 Nonsmoking:30 Min. : 0.00 Min. : 64.00 ## Cognitive : 0 Delayed : 0 1st Qu.: 2.75 1st Qu.: 98.75 ## Driving Simulation :30 Active :30 Median : 8.00 Median :111.00 ## Mean : 7.90 Mean :110.73 ## 3rd Qu.:11.00 3rd Qu.:123.00 ## Max. :22.00 Max. :168.00 You may notice from the summary above that the groups that were dropped “Delayed” smokers and “Cognitive” task still show up in the summary, albeit now with 0 instances (you’ll also notice that the remaining groups decreased in number, can you figure out why?). In most cases, R notices this an will automatically drop these factors in our subsequent analyses. However, if needed (i.e. it’s causing errors), these factors can be dropped by invoking the droplevels()function like so: dataset_2by2$Smkgrp &lt;- droplevels(dataset_2by2$Smkgrp) dataset_2by2$Task &lt;- droplevels(dataset_2by2$Task) summary(dataset_2by2) ## Task Smkgrp score covar ## Pattern Recognition:30 Nonsmoking:30 Min. : 0.00 Min. : 64.00 ## Driving Simulation :30 Active :30 1st Qu.: 2.75 1st Qu.: 98.75 ## Median : 8.00 Median :111.00 ## Mean : 7.90 Mean :110.73 ## 3rd Qu.:11.00 3rd Qu.:123.00 ## Max. :22.00 Max. :168.00 And to see the cell counts: ez::ezDesign(data = dataset_2by2, x = Task, y = Smkgrp, row = NULL, col = NULL) 10.3 Making sense of plots Let’s go ahead and plot the data using a line plot with 95% CI error bars. Note that these plots (up until the last section) are not APA-complete!!!! Since, I’m going to examples of several different plots, I’m going to create a general canvas p and build from that. p &lt;- ggplot2::ggplot(data = dataset_2by2, mapping = aes(x = Smkgrp, y = score, group = Task)) 10.3.1 Interaction plots Interaction plots take into consideration the influence of each of the IVs on one another—in this case the mean and CI of each smoking group (Active v. Nonsmoking) as a function of Task (Driving Simulation v. Pattern Recognition). For example, a line plot might look like this: line_p &lt;- p + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0.5)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.5), aes(linetype = Task)) + theme_cowplot() show(line_p) A brief inspection of the plot can be quite informative. Let’s start with the interaction, in fact: you should always start with the interaction. Since this is an “interaction” plot, often a quick visual inspection will allow us to predict whether our subsequent ANOVA will likely yield an interaction effect (it’s good practice to plot before running your ANOVA). A simple rule of thumb is that if you see the lines converging or intersecting then more than likely an interaction is present (whether its significant is another question). You might think that this rule of thumb is useful if you use a line plot, and well, you’d be right. What about a bar plot or box plot you ask? bar_p &lt;- p + stat_summary(geom = &quot;errorbar&quot;, width = 0.3, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0.9)) + stat_summary(geom = &quot;bar&quot;, fun.y = &quot;mean&quot;, color = &quot;black&quot;, aes(fill = Task), position = position_dodge(0.9)) + theme(legend.position = &quot;none&quot;) + scale_fill_manual(values = c(&quot;light grey&quot;, &quot;white&quot;)) show(bar_p) box_mean &lt;- function(x) { r &lt;- quantile(x, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)) # replace median with mean r[3] &lt;- mean(x) names(r) &lt;- c(&quot;ymin&quot;, &quot;lower&quot;, &quot;middle&quot;, &quot;upper&quot;, &quot;ymax&quot;) r } # Add points outside of whiskers box_out &lt;- function(x) { subset(x, x &lt; quantile(x, probs = 0.025) | quantile(x, probs = 0.975) &lt; x) } box_p &lt;- p + stat_summary(fun.data = box_mean, size = 0.5, color = &quot;black&quot;, geom = &quot;boxplot&quot;, position = position_dodge(0.9), aes(fill = Task)) + theme(legend.position = &quot;none&quot;) + scale_fill_manual(values = c(&quot;light grey&quot;, &quot;white&quot;)) plot_grid(bar_p, box_p) You should note that in my boxplots what is typically the median line is now represents the means (this was accomplished by the box_mean() function that I custom wrote in the chunk above. Note that the grey fills above are the “Driving Simulation” group and the white are the “Pattern Recognition”. In both cases just take a look at the means that are grouped together. If the relative difference between grouped means changes as you move from one category on the x axis to the next, you likely have an interaction. Note that this is a general rule of thumb and applies to the line plots as well (the reason that the lines intersect is because of these sorts of changes). In this case, the means on the “Active” grouping are nearly identical, while the means in the “Nonsmoking” grouping are much further apart. So we likely have interaction. 10.3.2 Plotting main effects If we wanted we could also create separate plots related to our mean effects. These plots would look something like this: Smoke_p &lt;- ggplot2::ggplot(data = dataset_2by2, mapping = aes(x = Smkgrp, y = score, group = 1)) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0)) + coord_cartesian(ylim = c(4, 12)) + theme_cowplot() Task_p &lt;- ggplot2::ggplot(data = dataset_2by2, mapping = aes(x = Task, y = score, group = 1)) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0)) + coord_cartesian(ylim = c(4, 12)) + theme_cowplot() plot_grid(Smoke_p, Task_p) and would take a look at changes due to each IV without considering the other. Here we might infer that there is a main effect for both of our IVs. That said, the interaction plot is useful as well in assessing main effects as well: show(line_p) Here, to infer whether there might be main effects we can imagine where the means would be if we collapsed our grouped plots (this is exactly what the main effects take a look at). To help with your imagination I’m going to plot our main effect means on the interaction plot. Here the grey-filled triangles represent the the collapsed Smoking Group means indifferent to task. To get these, just imagine finding the midpoint of the two circles in each level of Smoking group. The slope suggests the possibility a main effect. line_p + stat_summary(aes(x = Smkgrp, y = score, group = 1), geom = &quot;point&quot;, fun.y = &quot;mean&quot;, color = &quot;dark grey&quot;, size = 3, shape = 17) + stat_summary(aes(x = Smkgrp, y = score, group = 1), geom = &quot;line&quot;, fun.y = &quot;mean&quot;, color = &quot;dark grey&quot;, size = 1) To imagine the collapsed Task means, we can just find the y-values that intersect with the midpoints of each line (note the red line is the mean value of the Driving Simulation group): line_p + geom_hline(yintercept = 9.6667, color = &quot;red&quot;) + geom_hline(yintercept = 6.1333, color = &quot;blue&quot;) The difference in y-intercepts suggests the possibility of a main effect. So in summary, I’m guessing from plot I’ve got 2 main effects and an interaction. Let’s test this. 10.4 Running the ANOVA lm() method Now we can run the using the lm() method as we have previously done with the One-way ANOVA. The new wrinkle is simply adding our additional IV and interaction terms the formula equation the structural equation: \\[y=IV_1+IV_2+(IV_1*IV_2)\\] where the first and second terms capture our main effects and the third is our interaction. Using our data in R this formula becomes: lm(score ~ Smkgrp + Task + Smkgrp:Task, data = dataset_2by2) %&gt;% anova() The operator : may be understood here as “interaction between”. Note that we can write this same equation in shorthand: lm(score ~ Smkgrp * Task, data = dataset_2by2) %&gt;% anova() where the * operator tells R to examine all related main effects and interactions. This is usually the best way to code this, as the long-form becomes unweildy as we increase IVs and combinations of interactions. However, note that if you have planned comparisons you may elect to only specify certain main effects and interactions in log-form. OK, running the above gives us: ## Analysis of Variance Table ## ## Response: score ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Smkgrp 1 248.07 248.067 11.5687 0.001244 ** ## Task 1 187.27 187.267 8.7333 0.004564 ** ## Smkgrp:Task 1 187.27 187.267 8.7333 0.004564 ** ## Residuals 56 1200.80 21.443 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 One thing to note here are that the apparent equal F-values in our Task effect and Interaction is due to rounding. This is simply an anomaly, and is not to be expected in most cases. Even here, they are different, but only nearly identical. As a thought experiment, can we think of why they are nearly identical in this case (Hint: can you see what might be causing the interaction)? 10.5 Running the ANOVA in the afex::aov_ez() method: We can also use the aov_ez method from the afex library. In fact, this might be preferable, as the output gives you most everything you need to report your ANOVA, and in the future this method makes the necessary corrections for you if your data violates certain assumptions needed for ANOVA. While I have been stressing the lm() method to re-enforce that ANOVA and linear regression are two sides of the same coin, when performing ANOVA I typically use afex::aov_ez() unless I am planning some very complex a priori comparisons in my model (this is rarely the case). With aov_ez() we can easily specify the ANOVA model by inputting our parameters into the function. Please note which need to be input as strings (i.e., have quoations around them), and which are input as objects or numbers. One important note before continuing is that this method requires that you have a column that specifies participant number, here I’ll use PartID. Since we are running a purely between-design, we need to assign each participant a unique number. If R finds instances where PartID are the same it will assume that data comes from the same participant (i.e., we have a within-subjects or repeated measures design). For our purposes now, this is easily solved by simply creating a column that runs from 1 to the number of observations in our data. After that we can proceed with the ANOVA: # create a PartID column and number dataset_2by2$PartID &lt;- 1:nrow(dataset_2by2) # run the ANOVA afex::aov_ez(id = &quot;PartID&quot;, dv = &quot;score&quot;, data = dataset_2by2, between = c(&quot;Smkgrp&quot;, &quot;Task&quot;), within = NULL, covariate = NULL, observed = NULL, fun_aggregate = NULL, type = 3, factorize = TRUE, check_contrasts = TRUE, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Contrasts set to contr.sum for the following variables: Smkgrp, Task ## Anova Table (Type 3 tests) ## ## Response: score ## Effect df MSE F pes p.value ## 1 Smkgrp 1, 56 21.44 11.57 ** .17 .001 ## 2 Task 1, 56 21.44 8.73 ** .13 .005 ## 3 Smkgrp:Task 1, 56 21.44 8.73 ** .13 .005 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Note that above I specified every argument in the function. This was not necessary (indeed if you use Rstudio to help you fill this in, you’ll notice that many of the defaults are NULL). A complete write-up of what is going on here may be found by ?afex::aov_ez. For now I want to comment on a few of the choices I made. For type I selected Type III sum of squares. This is typically the type that we will choose in ANOVA, however see Field, Ch 11, “Jane Superbrain 11.1” for an excellent overview of the different Types of Sum of Squares and when to use different values. Also you’ll notice I made a few adjustments in my anova_table output via list(). These were to get the table to output partial eta-squared es=&quot;pes&quot; for my effect size. 10.6 Getting cell means Up until this point we’ve used psych::describeBy() to generate summary stats. This becomes a little more difficult as the designs become more complex. Instead I recommend summarySE() from the Rmisc package: Rmisc::summarySE(data = dataset_2by2, measurevar = &quot;score&quot;, groupvars = c(&quot;Task&quot;, &quot;Smkgrp&quot;)) ## Task Smkgrp N score sd se ci ## 1 Pattern Recognition Nonsmoking 15 9.400000 1.404076 0.3625308 0.7775512 ## 2 Pattern Recognition Active 15 9.933333 6.518837 1.6831565 3.6100117 ## 3 Driving Simulation Nonsmoking 15 2.333333 2.288689 0.5909368 1.2674335 ## 4 Driving Simulation Active 15 9.933333 6.005553 1.5506271 3.3257644 We can also use this function to report the means related to the main effects (irrespective of interaction). For example, Smoking effect I can re-write the call above, simply dropping Task from groupvars: Rmisc::summarySE(data = dataset_2by2, measurevar = &quot;score&quot;, groupvars = &quot;Smkgrp&quot;) ## Smkgrp N score sd se ci ## 1 Nonsmoking 30 5.866667 4.049124 0.7392655 1.511968 ## 2 Active 30 9.933333 6.158444 1.1243730 2.299601 and for the Task effect: Rmisc::summarySE(data = dataset_2by2, measurevar = &quot;score&quot;, groupvars = &quot;Task&quot;) ## Task N score sd se ci ## 1 Pattern Recognition 30 9.666667 4.641145 0.8473533 1.733032 ## 2 Driving Simulation 30 6.133333 5.905774 1.0782418 2.205252 Note that If you are reporting means related to the main effects, you need to report these marginal means! 10.7 More examples: a 2 × 3 ANOVA In the example above we focused in the 2 × 2 scenario for ease, however, remember that our original dataset was a 2 (Smoking group) by 3 (Task) design. 10.7.1 Running the ANOVA I’m going to use afex::aov_ez(). Also, I’ll need to add a participant identification number column to the original dataset # create a PartID column and number dataset$PartID &lt;- 1:nrow(dataset) # run the ANOVA afex::aov_ez(id = &quot;PartID&quot;, dv = &quot;score&quot;, data = dataset, between = c(&quot;Smkgrp&quot;, &quot;Task&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Contrasts set to contr.sum for the following variables: Smkgrp, Task ## Anova Table (Type 3 tests) ## ## Response: score ## Effect df MSE F pes p.value ## 1 Smkgrp 2, 126 107.83 8.41 *** .12 .0004 ## 2 Task 2, 126 107.83 132.90 *** .68 &lt;.0001 ## 3 Smkgrp:Task 4, 126 107.83 2.94 * .09 .02 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 10.7.2 APA plotting For the plot I’m going to give examples of presenting the data in line plot, bar plot, and box plot form. Which you choose depends on what you want to convey, and how complex the plot becomes. Note that in addition to using theme_cowplot() which you are familiar with, I’m also including several arguements in theme() which allow for greater customization. These include arguments to change the fontface and size of legend and axes titles, as well as re-positioning the legend. While the former may not be needed for APA (but might be aesthetically useful) not that APA format has strict guidelines pertaining to legend position. That said, all we are doing is extending the plotting methods that you have been using for the past few weeks. The important addition here is the addition of group= in the first line the ggplot. For example: p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = Smkgrp, y = score, group = Task)) indicates that we are: - using the dataset data set - putting first IV, Smkgrp, on the x-axis - putting our dv, score on the y-axis - and grouping our data by our other IV, Task This last bit is important as it makes clear that the resulting mean plots should be of the cell means related to Smkgrp x Task 10.7.2.1 Line plot with 95% CI bars # setting original parameters p &lt;- ggplot2::ggplot(data = dataset, mapping=aes(x=Smkgrp,y=score,group=Task)) # making a basic line plot line_p &lt;- p + # add pointranges, diferent Task by shape: stat_summary(geom=&quot;pointrange&quot;,fun.data = &quot;mean_cl_normal&quot;, size=0.75, position=position_dodge(.25), aes(shape=Task)) + # add lines, different Task by linetype (solid v dashed) stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position=position_dodge(.25), aes(linetype=Task)) # adding APA elements and other aesthetics line_p &lt;- line_p + theme_cowplot() + theme( axis.title = element_text(size = 16, face = &quot;bold&quot;), # make axes larger and bold axis.text = element_text(size = 12), # specify text size legend.title = element_text(size = 12, face = &quot;bold&quot;), # make lengend title bold legend.position = c(.1,.9) # change the legend position ) + xlab(&quot;Smoking Group&quot;) + ylab (&quot;Performance score&quot;) + # optional... create additional white space around the plot theme(plot.margin=unit(c(.25,.25,.25,.25),&quot;in&quot;)) show(line_p) A few notes: I elected to dodge the mean points. I find this to be clearer as I don’t have to worry about my overlapping error bars (this would be true even if I have the caps on the bars as well). This was accoumplished by adding position=position_dodge(.25) to both my calls for pointrange and line. I also differentiated each task group by both line and shape. In reality this is redundant, but I feel that it makes the differences in groups easier to distinguish with a quick glance. Note that I could have also chosen color, but with print APA color should only be chosen when using 2 or three shades of grey, and typically as a last resort. 10.7.2.2 Box plot Here, let’s do the same with box plots: # setting original parameters p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = Smkgrp, y = score, group = Task)) # note that I&#39;m using the functions I&#39;ve created in the previous boxplot example box_p &lt;- p + # make boxplots, set their width to half of the default stat_summary(fun.data = box_mean, color = &quot;black&quot;, geom = &quot;boxplot&quot;, position = &quot;dodge&quot;, aes(fill = Task, width = 0.5)) ## Warning: Ignoring unknown aesthetics: width # APA-ify box_p &lt;- box_p + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.1, 0.9)) + # used to specify the exact colors that I want, two shades of grey and white scale_fill_manual(values = c(&quot;grey50&quot;, &quot;grey75&quot;, &quot;white&quot;)) + xlab(&quot;Smoking Group&quot;) + ylab(&quot;Performance score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) show(box_p) A few notes: I decreased the widths of my box plots as otherwise the box plot may feel a little cramped. This was accomplished in the aes() call when I constructed the base box plot. (I changed width to 0.5) I also specified the fill-in color for each boxplot using scale_fill_manual(values = c(&quot;grey50&quot;,&quot;grey75&quot;,&quot;white&quot;)) 10.7.2.3 Bar plot And finally a bar plot. Remember when making a bar plot, we need to correct for the gap below zero on the y-axis: # setting original parameters p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = Smkgrp, y = score, group = Task)) # constructing the bar plot: bar_p &lt;- p + stat_summary(geom = &quot;errorbar&quot;, width = 0.3, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0.9)) + stat_summary(geom = &quot;bar&quot;, fun.y = &quot;mean&quot;, color = &quot;black&quot;, aes(fill = Task), position = position_dodge(0.9)) # adding APA: bar_p &lt;- bar_p + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.1, 0.9)) + scale_fill_manual(values = c(&quot;black&quot;, &quot;grey50&quot;, &quot;white&quot;)) + xlab(&quot;Smoking Group&quot;) + ylab(&quot;Performance score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + # fixing y-axis: scale_y_continuous(expand = c(0, 0)) + expand_limits(y = c(0, 25)) show(bar_p) 10.7.2.4 Cell means Rmisc::summarySE(data = dataset, measurevar = &quot;score&quot;, groupvars = c(&quot;Task&quot;, &quot;Smkgrp&quot;)) ## Task Smkgrp N score sd se ## 1 Pattern Recognition Nonsmoking 15 9.400000 1.404076 0.3625308 ## 2 Pattern Recognition Delayed 15 9.600000 4.404543 1.1372481 ## 3 Pattern Recognition Active 15 9.933333 6.518837 1.6831565 ## 4 Cognitive Nonsmoking 15 28.866667 14.686567 3.7920552 ## 5 Cognitive Delayed 15 39.933333 20.133365 5.1984125 ## 6 Cognitive Active 15 47.533333 14.652482 3.7832547 ## 7 Driving Simulation Nonsmoking 15 2.333333 2.288689 0.5909368 ## 8 Driving Simulation Delayed 15 6.800000 5.440588 1.4047538 ## 9 Driving Simulation Active 15 9.933333 6.005553 1.5506271 ## ci ## 1 0.7775512 ## 2 2.4391547 ## 3 3.6100117 ## 4 8.1331495 ## 5 11.1494858 ## 6 8.1142742 ## 7 1.2674335 ## 8 3.0128973 ## 9 3.3257644 10.7.2.5 Main effect means: Rmisc::summarySE(data = dataset, measurevar = &quot;score&quot;, groupvars = c(&quot;Task&quot;)) ## Task N score sd se ci ## 1 Pattern Recognition 45 9.644444 4.513392 0.6728168 1.355973 ## 2 Cognitive 45 38.777778 18.055330 2.6915297 5.424422 ## 3 Driving Simulation 45 6.355556 5.701497 0.8499290 1.712919 Rmisc::summarySE(data = dataset, measurevar = &quot;score&quot;, groupvars = c(&quot;Smkgrp&quot;)) ## Smkgrp N score sd se ci ## 1 Nonsmoking 45 13.53333 14.13024 2.106412 4.245194 ## 2 Delayed 45 18.77778 19.35892 2.885857 5.816063 ## 3 Active 45 22.46667 20.36218 3.035414 6.117475 10.8 2 × 5 ANOVA (Howell 13.1) For another example, we can return to the Eysenck data from Howell, Ch 13.1: dataset &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab13-2.dat&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Parsed with column specification: ## cols( ## Age = col_integer(), ## Condition = col_integer(), ## Recall = col_integer() ## ) # label dummy codes: dataset$Age &lt;- recode_factor(dataset$Age, `1` = &quot;Old&quot;, `2` = &quot;Young&quot;) dataset$Condition &lt;- recode_factor(dataset$Condition, `1` = &quot;Counting&quot;, `2` = &quot;Rhyming&quot;, `3` = &quot;Adjective&quot;, `4` = &quot;Imagery&quot;, `5` = &quot;Intentional&quot;) # re-leveling: dataset$Condition &lt;- factor(dataset$Condition, levels = c(&quot;Counting&quot;, &quot;Rhyming&quot;, &quot;Adjective&quot;, &quot;Imagery&quot;, &quot;Intentional&quot;)) summary(dataset) ## Age Condition Recall ## Old :50 Counting :20 Min. : 3.00 ## Young:50 Rhyming :20 1st Qu.: 7.00 ## Adjective :20 Median :11.00 ## Imagery :20 Mean :11.61 ## Intentional:20 3rd Qu.:15.25 ## Max. :23.00 10.8.1 Running the ANOVA # create a PartID column and number dataset$PartID &lt;- 1:nrow(dataset) # run the ANOVA afex::aov_ez(id = &quot;PartID&quot;, dv = &quot;Recall&quot;, data = dataset, between = c(&quot;Age&quot;, &quot;Condition&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Contrasts set to contr.sum for the following variables: Age, Condition ## Anova Table (Type 3 tests) ## ## Response: Recall ## Effect df MSE F pes p.value ## 1 Age 1, 90 8.03 29.94 *** .25 &lt;.0001 ## 2 Condition 4, 90 8.03 47.19 *** .68 &lt;.0001 ## 3 Age:Condition 4, 90 8.03 5.93 *** .21 .0003 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 10.8.1.1 Line plot with SE bars # setting original parameters p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = Condition, y = Recall, group = Age)) # making a basic line plot line_p &lt;- p + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, size = 0.75, position = position_dodge(0.25), aes(shape = Age)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.25), aes(linetype = Age)) # adding APA elements line_p &lt;- line_p + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.2, 0.75)) + xlab(&quot;Smoking Group&quot;) + ylab(&quot;Performance score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) show(line_p) We could elect to create bar plots and box plots here as well. One thing to keep in mind is that these plots become more and more cramped as you consider additional IVs and levels as in higher order designs. Anything above a 3 x 3, and I default to a line plot. At the same time, lineplots can become messy. A good rule of thumb is to place the IV with the greater number of levels on the x-axis and differentiate the other(s) by lines or shapes. For example, assuming we don’t follow this rule of thumb and place Age on the x-axis and Smoking by line type: # setting original parameters p &lt;- ggplot2::ggplot(data = dataset, mapping = aes(x = Age, y = Recall, group = Condition)) # making a basic line plot line_p &lt;- p + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, size = 0.75, position = position_dodge(0.25), aes(shape = Condition)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.25), aes(linetype = Condition)) # adding APA elements line_p &lt;- line_p + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0, 0.75)) + xlab(&quot;Smoking Group&quot;) + ylab(&quot;Performance score&quot;) + theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &quot;in&quot;)) show(line_p) This becomes more difficult to interpret. 10.9 One last example, dataset_no_inter &lt;- read_delim(&quot;https://raw.githubusercontent.com/tehjespah/tehjespah.github.io/master/teaching/PSYC7014/datasets/two_factor_ANOVA_no_interaction&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Lecture = col_integer(), ## Presentation = col_integer(), ## Score = col_integer() ## ) dataset_no_inter$Lecture &lt;- recode_factor(dataset_no_inter$Lecture, `1` = &quot;Phys&quot;, `2` = &quot;Soc&quot;, `3` = &quot;Hist&quot;) dataset_no_inter$Presentation &lt;- recode_factor(dataset_no_inter$Presentation, `1` = &quot;Comp&quot;, `2` = &quot;Stand&quot;) summary(dataset_no_inter) ## Lecture Presentation Score ## Phys:12 Comp :18 Min. :13.00 ## Soc :12 Stand:18 1st Qu.:27.75 ## Hist:12 Median :35.00 ## Mean :34.81 ## 3rd Qu.:42.00 ## Max. :53.00 10.9.1 Running the ANOVA I’m going to use afex::aov_ez(): # create a PartID column and number dataset_no_inter$PartID &lt;- 1:nrow(dataset_no_inter) # run the ANOVA afex::aov_ez(id = &quot;PartID&quot;, dv = &quot;Score&quot;, data = dataset_no_inter, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Contrasts set to contr.sum for the following variables: Lecture, Presentation ## Anova Table (Type 3 tests) ## ## Response: Score ## Effect df MSE F pes p.value ## 1 Lecture 2, 30 49.09 4.95 * .25 .01 ## 2 Presentation 1, 30 49.09 31.25 *** .51 &lt;.0001 ## 3 Lecture:Presentation 2, 30 49.09 0.32 .02 .73 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 10.9.2 Interaction plot Here we have a case where there is no interaction. Plotting this: # setting original parameters p &lt;- ggplot2::ggplot(data = dataset_no_inter, mapping = aes(x = Lecture, y = Score, group = Presentation)) # making a basic line plot line_p &lt;- p + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, size = 0.75, position = position_dodge(0.25), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.25), aes(linetype = Presentation)) # adding APA elements line_p &lt;- line_p + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.75, 0.9)) + xlab(&quot;Lecture type&quot;) + ylab(&quot;Performance score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) show(line_p) 10.10 Interaction v. no interaction Why is the fact that there is no interaction here important? Of all of the examples that we’ve covered to this point, the last example was the only example that we can trust our main effects at face value. This re-enforces the point I made earlier: Look at your interaction FIRST!!! Consider what the interaction means— that your observed main effects are contingent on certain conditions. If you have a significant interaction, you must run simple effects analysis to elucidate these contingencies. For example, looking at our very first example: p &lt;- ggplot2::ggplot(data = dataset_2by2, mapping = aes(x = Smkgrp, y = score, group = Task)) interaction_p &lt;- p + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0.25)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.25), aes(linetype = Task)) + theme_cowplot() show(interaction_p) # create a PartID column and number dataset_2by2$PartID &lt;- 1:nrow(dataset_2by2) # run the ANOVA afex::aov_ez(id = &quot;PartID&quot;, dv = &quot;score&quot;, data = dataset_2by2, between = c(&quot;Smkgrp&quot;, &quot;Task&quot;), within = NULL, covariate = NULL, observed = NULL, fun_aggregate = NULL, type = 3, factorize = TRUE, check_contrasts = TRUE, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Contrasts set to contr.sum for the following variables: Smkgrp, Task ## Anova Table (Type 3 tests) ## ## Response: score ## Effect df MSE F pes p.value ## 1 Smkgrp 1, 56 21.44 11.57 ** .17 .001 ## 2 Task 1, 56 21.44 8.73 ** .13 .005 ## 3 Smkgrp:Task 1, 56 21.44 8.73 ** .13 .005 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The ANOVA reveals a main effect for smoking group. However, looking at the plot, we know that this is not the entire story. That is, the means change from one smoking group to another for the Driving simulation task, but not for the Pattern recognition group. Our main effect for Smoking group is contingent on task. This is what I mean by “you can’t trust your main effects when you have an interaction”—they don’t tell the entire story. 10.10.1 No interaction? How about some posthocs? If you don’t have an interaction, you may simply proceed to run post-hoc analyses on any significant main effects in the manner you would with a One-way ANOVA. Easy, peasy, right. One thing to note, you need to make the appropriate multiple comparison corrections. The easy way to do this is to perform a Bonferroni correction on the number of post-hoc comparisons that you intend. For example, returning to our data with no interaction: # create a PartID column and number dataset_no_inter$PartID &lt;- 1:nrow(dataset_no_inter) # run the ANOVA afex::aov_ez(id = &quot;PartID&quot;, dv = &quot;Score&quot;, data = dataset_no_inter, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Contrasts set to contr.sum for the following variables: Lecture, Presentation ## Anova Table (Type 3 tests) ## ## Response: Score ## Effect df MSE F pes p.value ## 1 Lecture 2, 30 49.09 4.95 * .25 .01 ## 2 Presentation 1, 30 49.09 31.25 *** .51 &lt;.0001 ## 3 Lecture:Presentation 2, 30 49.09 0.32 .02 .73 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 # setting original parameters p &lt;- ggplot2::ggplot(data = dataset_no_inter, mapping = aes(x = Lecture, y = Score, group = Presentation)) # making a basic line plot line_p &lt;- p + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, size = 0.75, position = position_dodge(0.25), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.25), aes(linetype = Presentation)) # adding APA elements line_p &lt;- line_p + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.75, 0.85)) + xlab(&quot;Lecture type&quot;) + ylab(&quot;Performance score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) show(line_p) We need to test for differences in both the Lecture and Presentation main effects. Presentation: This one is easy. We only have two levels of Presentation, so the omnibus Ftest tells us that our two groups are different. Nothing else to do here other than note which mean (Computer v. Standard) is greater than the other. Lecture: We have three levels of lecture, so were are going to need to run a post-hoc analysis. In this case, we may call upon our old standbys, Tukey and Bonferroni. When using afex() we need to call on a different library to perform our post hoc analyses: emmeans. For example, to run a Tukey, you need the ANOVA model to be saved to an object. Here I’m saving it to the object aov.model aov.model &lt;- afex::aov_ez(id = &quot;PartID&quot;, dv = &quot;Score&quot;, data = dataset_no_inter, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Contrasts set to contr.sum for the following variables: Lecture, Presentation From here you may call upon the emmeans() function to derive your posthocs. By itself, emmeans produces the means by levels of the IV(s) listed in its spec= argument. It takes the afex_aov model as a first argument, and the IVs of interest as the second. # input your model into the emmeans, interested in Lecture emmeans(aov.model, specs = &quot;Lecture&quot;) ## NOTE: Results may be misleading due to involvement in interactions ## Lecture emmean SE df lower.CL upper.CL ## Phys 40.00000 2.022672 30 35.86915 44.13085 ## Soc 32.16667 2.022672 30 28.03582 36.29751 ## Hist 32.25000 2.022672 30 28.11915 36.38085 ## ## Results are averaged over the levels of: Presentation ## Confidence level used: 0.95 emmeans() allows another method for making contrasts (planned and posthoc). If you want to perform a Tukey test follow this procedure you can simply pipe the previous (or save to an object and submit) to pairs(): emmeans(aov.model, specs = &quot;Lecture&quot;) %&gt;% pairs(., adjust = &quot;tukey&quot;) ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio p.value ## Phys - Soc 7.83333333 2.860491 30 2.738 0.0270 ## Phys - Hist 7.75000000 2.860491 30 2.709 0.0289 ## Soc - Hist -0.08333333 2.860491 30 -0.029 0.9995 ## ## Results are averaged over the levels of: Presentation ## P value adjustment: tukey method for comparing a family of 3 estimates While this method doesn’t provide the nice grouping plots from agricolae you can, if you choose, get the grouping letters by, instead, adding CLD() to your pipe: emmeans(aov.model, specs = &quot;Lecture&quot;) %&gt;% CLD(., adjust = &quot;tukey&quot;) ## NOTE: Results may be misleading due to involvement in interactions ## Lecture emmean SE df lower.CL upper.CL .group ## Soc 32.16667 2.022672 30 27.05240 37.28094 1 ## Hist 32.25000 2.022672 30 27.13573 37.36427 1 ## Phys 40.00000 2.022672 30 34.88573 45.11427 2 ## ## Results are averaged over the levels of: Presentation ## Confidence level used: 0.95 ## Conf-level adjustment: sidak method for 3 estimates ## P value adjustment: tukey method for comparing a family of 3 estimates ## significance level used: alpha = 0.05 Personally, I like to do both as it gives me the most detail about my results. In this case it appears that our Tukey does reveal differences between means, when performance of those getting Psychical Science Lectures is greater than both History and Social, but Social and History are not difference from one another. Note that you may call other p-value adjustments using these methods: emmeans(aov.model, specs = &quot;Lecture&quot;) %&gt;% pairs(., adjust = &quot;bonferroni&quot;) ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio p.value ## Phys - Soc 7.83333333 2.860491 30 2.738 0.0309 ## Phys - Hist 7.75000000 2.860491 30 2.709 0.0331 ## Soc - Hist -0.08333333 2.860491 30 -0.029 1.0000 ## ## Results are averaged over the levels of: Presentation ## P value adjustment: bonferroni method for 3 tests emmeans(aov.model, specs = &quot;Lecture&quot;) %&gt;% CLD(., adjust = &quot;bonferroni&quot;) ## NOTE: Results may be misleading due to involvement in interactions ## Lecture emmean SE df lower.CL upper.CL .group ## Soc 32.16667 2.022672 30 27.03769 37.29564 1 ## Hist 32.25000 2.022672 30 27.12103 37.37897 1 ## Phys 40.00000 2.022672 30 34.87103 45.12897 2 ## ## Results are averaged over the levels of: Presentation ## Confidence level used: 0.95 ## Conf-level adjustment: bonferroni method for 3 estimates ## P value adjustment: bonferroni method for 3 tests ## significance level used: alpha = 0.05 10.10.2 what to do if you DO have an interaction Well, for this week, you stop. Next week we will see how to press on! 10.11 What about planned contrasts? You need to be careful when running planned contrasts in factorial ANOVA. In genral I would recommend only running planned contrasts on a single main effect, or a planned contrast on the effects of one of your factors at a single level of your other (though you still need to proceed with caution here). For example, using the data from the last section, I would only run a planned contrast related to the main effect of Lecture Type, or a contrast of Lecture Type means only in Computer presentation conditions (or Standard presentation). DO NOT, I repeat DO NOT run contrasts that go across levels of your other factors. Well, truthfully, you can do whatever you want, but you may find that your ability to meaningfully interpret your results in such cases is extremely limited. We can run planned contrasts using emmeans() as well. In this case, we need to specify the contrasts. First we need to obtain the emmeans() of the model including all cells (all factors). Using aov.model from the previous example: emmeans(aov.model, specs = c(&quot;Lecture&quot;, &quot;Presentation&quot;)) ## Lecture Presentation emmean SE df lower.CL upper.CL ## Phys Comp 46.00000 2.860491 30 40.15810 51.84190 ## Soc Comp 40.00000 2.860491 30 34.15810 45.84190 ## Hist Comp 38.00000 2.860491 30 32.15810 43.84190 ## Phys Stand 34.00000 2.860491 30 28.15810 39.84190 ## Soc Stand 24.33333 2.860491 30 18.49143 30.17523 ## Hist Stand 26.50000 2.860491 30 20.65810 32.34190 ## ## Confidence level used: 0.95 OK. From here let’s build two custom contrasts. First lecture contrast on the main effect. In this case let’s assume I want to contrast Phys with the combined other two conditions. Using the output above, I identify which rows contain Phys and I ensure that the summation of those rows is 1. In this case there are two rows so each gets 0.5. My remaining conditions must also equal -1. In this case there are four, so each is -0.25. Following the output above, then my contrast vector is: main_eff_contrast &lt;- c(0.5, -0.25, -0.25, 0.5, -0.25, -0.25) %&gt;% list() You’ll note that I piped my contrast into a list. This is required and a quirk of using emmeans(). From here I simply call contrast() contrast matrix as an argument. So the entire pipe goes from: emmeans(aov.model, specs = c(&quot;Lecture&quot;, &quot;Presentation&quot;)) %&gt;% contrast(., main_eff_contrast) ## contrast estimate SE df t.ratio ## c(0.5, -0.25, -0.25, 0.5, -0.25, -0.25) 7.791667 2.477258 30 3.145 ## p.value ## 0.0037 Assuming that I want to run a similar contrast, but only on lectures done via computer, I would reaquaint myself with the cell means: emmeans(aov.model, specs = c(&quot;Lecture&quot;, &quot;Presentation&quot;)) ## Lecture Presentation emmean SE df lower.CL upper.CL ## Phys Comp 46.00000 2.860491 30 40.15810 51.84190 ## Soc Comp 40.00000 2.860491 30 34.15810 45.84190 ## Hist Comp 38.00000 2.860491 30 32.15810 43.84190 ## Phys Stand 34.00000 2.860491 30 28.15810 39.84190 ## Soc Stand 24.33333 2.860491 30 18.49143 30.17523 ## Hist Stand 26.50000 2.860491 30 20.65810 32.34190 ## ## Confidence level used: 0.95 And do my contrasts like so (see if you can follow the coding logic): # built my contrast computer_only_contrast &lt;- c(1, -0.5, -0.5, 0, 0, 0) %&gt;% list() # run the planned contrast emmeans(aov.model, specs = c(&quot;Lecture&quot;, &quot;Presentation&quot;)) %&gt;% contrast(., computer_only_contrast) ## contrast estimate SE df t.ratio p.value ## c(1, -0.5, -0.5, 0, 0, 0) 7 3.503371 30 1.998 0.0548 Assuming I wanted to perfom a set of orthoginal contrasts: Phys v. Soc and Hist and Soc v Hist # build the contrast matrix contrast1 &lt;- c(1, -0.5, -0.5, 0, 0, 0) contrast2 &lt;- c(0, -1, 1, 0, 0, 0) contrast_matrix &lt;- list(contrast1, contrast2) # run the contrasts emmeans(aov.model, specs = c(&quot;Lecture&quot;, &quot;Presentation&quot;)) %&gt;% contrast(., contrast_matrix) ## contrast estimate SE df t.ratio p.value ## c(1, -0.5, -0.5, 0, 0, 0) 7 3.503371 30 1.998 0.0548 ## c(0, -1, 1, 0, 0, 0) -2 4.045345 30 -0.494 0.6246 In both cases, my p-values are unadjusted. I can add an adjustment to the contrast() argument like so: emmeans(aov.model, specs = c(&quot;Lecture&quot;, &quot;Presentation&quot;)) %&gt;% contrast(., contrast_matrix, adjust = &quot;holm&quot;) ## contrast estimate SE df t.ratio p.value ## c(1, -0.5, -0.5, 0, 0, 0) 7 3.503371 30 1.998 0.1097 ## c(0, -1, 1, 0, 0, 0) -2 4.045345 30 -0.494 0.6246 ## ## P value adjustment: holm method for 2 tests # or emmeans(aov.model, specs = c(&quot;Lecture&quot;, &quot;Presentation&quot;)) %&gt;% contrast(., contrast_matrix, adjust = &quot;bonferroni&quot;) ## contrast estimate SE df t.ratio p.value ## c(1, -0.5, -0.5, 0, 0, 0) 7 3.503371 30 1.998 0.1097 ## c(0, -1, 1, 0, 0, 0) -2 4.045345 30 -0.494 1.0000 ## ## P value adjustment: bonferroni method for 2 tests That’s all for this week. "],
["anova-iv-interactions-simple-effects.html", "Week 11 ANOVA IV: Interactions &amp; simple Effects 11.1 the data set 11.2 Plotting the data and descriptive stats 11.3 the Omnibus ANOVA (and assumption tests) 11.4 Running the simple effects ANOVA in 6 steps: 11.5 Building a function to ninja this for us 11.6 Post hoc tests 11.7 Interpreting these results: 11.8 Example write-up: 11.9 Performing this in SPSS (video)", " Week 11 ANOVA IV: Interactions &amp; simple Effects This short vignette is a continuation of last weeks focus on Factorial ANOVA. Here we’re asking the question “what to do if you have an interaction”. As we discussed last week, when running a factorial ANOVA, your first question should be “do I have an interaction”. If you do have an interaction then you need to examine that interaction in detail. How do we do this? Well, let’s think about what that interaction means. As an example, let’s revisit some of the data that we looked at in lecture. This write-up requires the following packages: pacman::p_load(tidyverse, cowplot, afex, agricolae, Rmisc, psych) 11.1 the data set Background: Given the ease of access for new technologies and increasing enrollment demands, many university are advocating that departments switch over to E-courses, where students view an online, pre-recorded lecture on the course topic in lieu of sitting in a classroom in a live lecture. Given this push, a critical question remains regarding the impact of E-courses on student outcomes. More it may be the case that certain subject content more readily lends itself to E-course presentations than other subjects. To address this question we tested students performance on a test one week after participating in the related lecture. Lectures were either experienced via online (Computer) or in a live classroom (Standard). In addition, the lecture content varied in topic (Physical science, Social science, History) Here’s the data, where Score represents performance: ## Parsed with column specification: ## cols( ## Score = col_integer(), ## Presentation = col_character(), ## Lecture = col_character(), ## subID = col_integer() ## ) 11.2 Plotting the data and descriptive stats Here’s what our results look like: ggplot(dataset, mapping = aes(x = Lecture, y = Score, group = Presentation)) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0.25), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.25)) + theme_cowplot() + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) And now getting the cell means and marginal means. Remember that analysis of the marginal means is what is tested in the main effects. The test for an interaction focuses on the cell means: # cell means: summarySE(data = dataset, measurevar = &quot;Score&quot;, groupvars = c(&quot;Presentation&quot;, &quot;Lecture&quot;)) ## Presentation Lecture N Score sd se ci ## 1 Computer History 6 38 4.381780 1.788854 4.598397 ## 2 Computer Physical 6 46 6.985700 2.851900 7.331042 ## 3 Computer Social 6 40 4.816638 1.966384 5.054751 ## 4 Standard History 6 31 10.315038 4.211096 10.824968 ## 5 Standard Physical 6 34 11.009087 4.494441 11.553328 ## 6 Standard Social 6 12 3.847077 1.570563 4.037260 # marginal means summarySE(data = dataset, measurevar = &quot;Score&quot;, groupvars = &quot;Lecture&quot;) ## Lecture N Score sd se ci ## 1 History 12 34.5 8.393721 2.423058 5.333116 ## 2 Physical 12 40.0 10.795622 3.116428 6.859211 ## 3 Social 12 26.0 15.201675 4.388345 9.658683 summarySE(data = dataset, measurevar = &quot;Score&quot;, groupvars = &quot;Presentation&quot;) ## Presentation N Score sd se ci ## 1 Computer 18 41.33333 6.249706 1.473070 3.107906 ## 2 Standard 18 25.66667 13.105903 3.089091 6.517412 11.3 the Omnibus ANOVA (and assumption tests) Here we are running the omnibus ANOVA using the afex:aov_ex() function: omnibus.aov &lt;- afex::aov_ez(id = &quot;subID&quot;, dv = &quot;Score&quot;, data = dataset, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture, Presentation ## Contrasts set to contr.sum for the following variables: Lecture, Presentation In addition I’m using the lm attribute of the omnibus.aov output to get the residuals of the model. This in turn allows me to test for normality: # check the normailty assumption of the residuals: omnibus.aov$lm$residuals %&gt;% car::qqPlot() ## [1] 10 31 Unfortunately, I can’t use this same method to check for heterogeneity of varience using leveneTest(). Going to have to do it the old fashioned way: # check the heterogeneity assumption car::leveneTest(Score ~ Lecture * Presentation, data = dataset) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 1.6132 0.1868 ## 30 Finally let’s take a look at the omnibus ANOVA results. While I could simple call my omnibus.aov object by itself: omnibus.aov ## Anova Table (Type 3 tests) ## ## Response: Score ## Effect df MSE F pes p.value ## 1 Lecture 2, 30 55.60 10.74 *** .42 .0003 ## 2 Presentation 1, 30 55.60 39.73 *** .57 &lt;.0001 ## 3 Lecture:Presentation 2, 30 55.60 6.49 ** .30 .005 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 I prefer the output provided by its anova_table attribute. Why? because it allows me to easily extract values from the resulting table. This will be extremely helpful in a little bit: # show the anova table omnibus.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 30 55.6 10.7374 0.41719 0.000304 *** ## Presentation 1 30 55.6 39.7302 0.56977 5.993e-07 *** ## Lecture:Presentation 2 30 55.6 6.4928 0.30209 0.004539 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking at the ANOVA: Our assumptions tests check out, and our ANOVA reveals two main effects and in interaction. Looking back at the plot (always, always plot before you think about doing any sort of follow-ups!!) it is fairly apparent what is happening—when moving from one lecture to the next, we see a much more dramatic decrease in score for the Social group in the Standard presentation group compared to the Computer presentation. That is, moving from left to right the pattern of change is different for the Standard group, compared to the Computer group. This is what our eyeball test is telling us—we need to confirm it with some stats! There are two ways to address this interaction, each involves sub-setting the data for further analysis. When you have an interaction, you proceed by testing for differences between means for one condition, on each individual level of the other. For example, we can test for and effect of Lecture Type when the Presentation is Computer, and effect of Lecture Type when the Presentation is Standard. In this case you would run two separate simple effects ANOVAs, each taking a look at changes for each line in the plot above. –OR– You could check for difference between Presentations on each Lecture type. Here you would be comparing Computer v Standard in each of the Lecture conditions. This would involve running three ANOVAs each checking for the Presentation differences (circle v. triangle) present at History, Physical, and Social. Which you choose, ultimately depends on which narrative you are trying to convey in your data. Here it may make sense to do the former. That is eyeballing the data it looks like the means in the Computer presentation level are not as different from one another as the Standard presentation. 11.4 Running the simple effects ANOVA in 6 steps: Running a follow-up simple effects ANOVA is akin to running a One-way ANOVA with one important caveat. When you run the follow-up ANOVA you need to use the error terms from your omnibus ANOVA. That is your simple effects ANOVA calls for the omnibus ANOVA errors: Sum of Squares, Mean Square Error, and denominator degrees of freedom. What this means is that the F-value (and subsequent p-values) that you get in you initial follow-up ANOVA is WRONG. There are several steps that need to be run for each simple effects ANOVA: 11.4.1 step 1: get the omnibus ANOVA: Assuming you haven’t already we need to run the omnibus ANOVA. Here I’m running all ANOVAs using afex::aov_ez and saving them as afex_aov objects. This will be useful to me later on when extracting info for my correction calculations: require(afex) # need to make a subject column for afex: dataset$subID &lt;- 1:nrow(dataset) # run omnibus: omnibus.aov &lt;- afex::aov_ez(id = &quot;subID&quot;, dv = &quot;Score&quot;, data = dataset, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture, Presentation ## Contrasts set to contr.sum for the following variables: Lecture, Presentation omnibus.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 30 55.6 10.7374 0.41719 0.000304 *** ## Presentation 1 30 55.6 39.7302 0.56977 5.993e-07 *** ## Lecture:Presentation 2 30 55.6 6.4928 0.30209 0.004539 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.4.2 step 2: get the MSError, Error df, and Error SS from omnibus ANOVA In R, we can do this by using some of the attribues() of the omnibus.aov (afex_aov object). For example if we take a look at omnibus.aov$anova_table we see that we can get the MSE and den Df from its attributes (we only need one value, so I’m taking the first instance). We can calculate the omnibus Error SS by multiplying the two: omnibus_mse &lt;- omnibus.aov$anova_table$MSE[1] omnibus_df &lt;- omnibus.aov$anova_table$&quot;den Df&quot;[1] omnibus_ss_error &lt;- omnibus_mse * omnibus_df 11.4.3 step 3: subset your data accordingly In order to do this we will need to subset the data. For example I am interested in running my follow-ups on Computer data and Standard data separately, so my first move is to perform this subset: computer.data &lt;- filter(dataset, Presentation == &quot;Computer&quot;) standard.data &lt;- filter(dataset, Presentation == &quot;Standard&quot;) 11.4.4 step 4: run your One-way simple effects ANOVA(s) Here I’m focusing on the computer.data for my example. I’ll return to the standard.data later. Again, I’m testing for an effect of Lecture Type on the computer.data. Note that you only include Lecture in you between = call: computer.aov &lt;- afex::aov_ez(id = &quot;subID&quot;, dv = &quot;Score&quot;, data = computer.data, between = &quot;Lecture&quot;, within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture ## Contrasts set to contr.sum for the following variables: Lecture computer.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 15 30.4 3.4211 0.31325 0.0597 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.4.5 step 5: get the treatment MS, df, SS, and F-value from your simple ANOVA As before, this can be done by calling attributes from our simple ANOVA, computer.aov, and using what we know about calculating those values that can’t be extracted: simple_f &lt;- computer.aov$anova_table$F simple_df &lt;- computer.aov$anova_table$`num Df` simple_mse &lt;- computer.aov$anova_table$MSE[1] # we can calculate the treatment MS by: simple_ms &lt;- simple_mse * simple_f # and the simple SS: simple_ss &lt;- simple_ms * simple_df 11.4.6 step 6: make our corrections As I noted above the F-value, p-value, and effect size (pes, \\(\\eta_p^2\\)) that you originally obtained in the simple effects computer.aov are not correct. We need to make the appropriate corrections. This can be done my hand, using the values we’ve extracted and calculated above: # calculate F using simple treatment MS and omnibus MSE corrected_f &lt;- simple_ms/omnibus_mse # the pf function calculates the cummulative p.value using the corrected_f and # appropriate degrees of freedom. Since its cumulative we subtract this result # from 1: corrected_p &lt;- (1 - pf(corrected_f, df1 = simple_df, df2 = omnibus_df)) # calculate the pes by using simple treatment SS and omnibus MSE: corrected_pes &lt;- (simple_ss/(simple_ss + omnibus_ss_error)) Congrats!! You have made the appropriate corrections! Taking a look at your corrected result it appears that we should accept the null hypothesis of no difference in means on the computer data. 11.5 Building a function to ninja this for us Given what we know about how we made these calculations we can create a function in R to simplify this process. All we need an omnibus afex_aov object and simple afex_aov object to extract from. Given that, we can set up the function by copying the hand calculations from the previous steps. Here I’m building the function simpleEffectsAOV to handle this for us. Feel free to use this function BUT make sure you understand what is happening in each line: simpleEffectsAOV &lt;- function(full.aov, simple.aov) { # get important values from full: full_mse &lt;- full.aov$anova_table$MSE[1] full_df &lt;- full.aov$anova_table$&quot;den Df&quot;[1] full_ss_error &lt;- full_mse * full_df # get important values from simple effects aov simple_f &lt;- simple.aov$anova_table$F simple_df &lt;- simple.aov$anova_table$`num Df` simple_mse &lt;- simple.aov$anova_table$MSE[1] simple_ms &lt;- simple_mse * simple_f simple_ss &lt;- simple_ms * simple_df # get corrected F corrected_f &lt;- simple_ms/full_mse # get corrected P p.value &lt;- (1 - pf(corrected_f, df1 = simple_df, df2 = full_df)) # round p.value to 3 digits p.value &lt;- round(p.value, 3) # if the p-value is greater than .001, then report entire value else, report as # &#39;p &lt; .001&#39; p.value &lt;- ifelse(p.value &gt; 0.001, paste0(&quot;p = &quot;, p.value), &quot;p &lt; .001&quot;) # get simple effects pes: pes &lt;- (simple_ss/(simple_ss + full_ss_error)) # print our results: I&#39;m using the paste function to write my results as a string ftest &lt;- paste0(&quot;F(&quot;, simple_df, &quot;,&quot;, full_df, &quot;) = &quot;, round(corrected_f, digits = 3), &quot;, &quot;, p.value, &quot;, pes=&quot;, round(pes, 3)) # round our obtained full_mse to 3 digits: full_mse &lt;- round(full_mse, digits = 3) output &lt;- cbind(row.names(simple.aov[[1]]), full_mse, ftest) dimnames(output)[[2]][1] &lt;- &quot;Effect&quot; show(output) } Note that a copy of this function is available on my github cite to download and can be sourced by you using the following: source(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsScripts/simpleEffectsAOV.R&quot;) And now I can use this function like so: simpleEffectsAOV(full.aov = omnibus.aov, simple.aov = computer.aov) ## Effect full_mse ftest ## [1,] &quot;Lecture&quot; &quot;55.6&quot; &quot;F(2,30) = 1.871, p = 0.172, pes=0.111&quot; And now quickly let’s run the Standard data: standard.aov &lt;- afex::aov_ez(id = &quot;subID&quot;, dv = &quot;Score&quot;, data = standard.data, between = &quot;Lecture&quot;, within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture ## Contrasts set to contr.sum for the following variables: Lecture simpleEffectsAOV(full.aov = omnibus.aov, simple.aov = standard.aov) ## Effect full_mse ftest ## [1,] &quot;Lecture&quot; &quot;55.6&quot; &quot;F(2,30) = 15.36, p &lt; .001, pes=0.506&quot; 11.6 Post hoc tests So it looks like we have no Lecture type effect on our computer data, but we do have one on our standard data. Given our results, we need to run further tests on our standard.data results to tease out the differences between lecture types. Here we can call on our old friend Tukey HSD, but again with the same caveat. Our calculations (in this case of the critical q (see Howell text) must include the error terms from the omnibus ANOVA. In order to do so we can still use the agricolae::HSD.test function, but where before we simply used the lm object as one of our arguments, no we need to enter our appropriate MSE and df by hand. Here is a comparison of the WRONG way and the RIGHT way to perform this follow-up. Note the differences in the Critical Value of Studentized Range for each output: ### THIS IS WRONG !!!!! ### agricolae::HSD.test(standard.aov$lm, trt = &quot;Lecture&quot;, console = T, group = F) ## ## Study: standard.aov$lm ~ &quot;Lecture&quot; ## ## HSD Test for dv ## ## Mean Square Error: 80.8 ## ## Lecture, means ## ## dv std r Min Max ## History 31 10.315038 6 20 46 ## Physical 34 11.009087 6 18 48 ## Social 12 3.847077 6 6 16 ## ## Alpha: 0.05 ; DF Error: 15 ## Critical Value of Studentized Range: 3.673378 ## ## Comparison between treatments means ## ## difference pvalue signif. LCL UCL ## History - Physical -3 0.8337 -16.480178 10.48018 ## History - Social 19 0.0062 ** 5.519822 32.48018 ## Physical - Social 22 0.0019 ** 8.519822 35.48018 ### THIS IS RIGHT !!!! #### agricolae::HSD.test(standard.data$Score, trt = standard.data$Lecture, DFerror = 30, MSerror = 55.6, alpha = 0.05, console = T, group = F) ## ## Study: standard.data$Score ~ standard.data$Lecture ## ## HSD Test for standard.data$Score ## ## Mean Square Error: 55.6 ## ## standard.data$Lecture, means ## ## standard.data.Score std r Min Max ## History 31 10.315038 6 20 46 ## Physical 34 11.009087 6 18 48 ## Social 12 3.847077 6 6 16 ## ## Alpha: 0.05 ; DF Error: 30 ## Critical Value of Studentized Range: 3.48642 ## ## Comparison between treatments means ## ## difference pvalue signif. LCL UCL ## History - Physical -3 0.7671 -13.613081 7.613081 ## History - Social 19 0.0003 *** 8.386919 29.613081 ## Physical - Social 22 0.0000 *** 11.386919 32.613081 Given our latter result above we see that in the Standard presentation, scores based on Social lectures were significantly less than the other two. 11.7 Interpreting these results: Given everything above, there are several things that this data are telling us: Main effect for computer: on average people tended to perform better in the Computer presentation Main effect for Lecture: this main effect is not as clear. Overall, people may have performed worst on the Social lecture content, it’s quite apparent that the presence of this effect is muddled by presentation type. This is what the presence of our interaction is telling us. Simple effect of lecture type via computer: No differences in means suggests that people perform equally well on all lecture content types when administed via computer Simple effect of lecture type via standard lecture: significant simple effect and subsequent posthocs demonstrate that while students perform equallly as well on Physical Science and History content via standard lectures, they perform worse on tests of Social science content. Given this pattern of results two major conclusions become apparent: First, students overall perform better via Computer. This was the case for all three lecture types. True, how much better varies by condition, but in all cases scores were higher. Second, while performance via Computer was indifferent to Lecture type (all lecture content scores were nearly equal) there was an attrition for Social science content when provided via standard lecture. From this one might conclude that administering content via E-course is better for student outcomes, especially if the subject content is in the Social Sciences! 11.8 Example write-up: To test for the effects of Presentation style (Standard, Computer) and Lecture Type (Physics, History, Social) we ran a 2 × 3 factorial ANOVA. This ANOVA revealed main effects for both Presentation style, \\(F(1,30)=39.73, p&lt;.001, \\eta_p^2=.57\\), and Lecture type, \\(F(2,30)=10.74, p&lt;.001, \\eta_p^2=.42\\). As shown in Figure 1, these effects we qualified by a Presentation style × Lecture type interaction, \\(F(2,30)=6.49, p=.005, \\eta_p^2=.30\\). Differences in Lecture type were only observed when material was presented in the Standard presentation, \\(F(2,30)=15.36, p&lt;.001, \\eta_p^2=.51\\), where scores from the Social lecture were significantly different from the remaining two (Tukey HSD, p&lt; .05). No differeces were observed when the material was presented via computer (p&gt;.05). Overall, participants perdormed better when the material was presented via computer (M±SD: 41.33 ± 6.25) compared to standard presentations (25.67 ± 13.11). One thing you may notice is that I still stressed the main effect. This is to stress to the reader that presentation type did make a difference. 11.9 Performing this in SPSS (video) As I mentioned in class, given that we are doing our analyses via programming, we have the luxury of creating a function that can make all of our simple effects adjustments for us. Once created (and saved for later) , this function can be used over and over again. If you are using the GUI method in SPSS, you don’t have this luxury (although you can certainly create a similar function in SPSS!). Fear not, here I walk you through how to do this in SPSS, assuming you have Microsoft Excel: "],
["anova-v-higher-order-factorial-anova.html", "Week 12 ANOVA V: Higher Order Factorial ANOVA 12.1 Packages and data 12.2 EXAMPLE 1: no three-way interaction, single two way interaction 12.3 EXAMPLE 2: No three-way interaction, multiple main effects, multiple 2 way interactions: 12.4 EXAMPLE 3: OMG, multiple two way interactions and a nasty three-way!!!", " Week 12 ANOVA V: Higher Order Factorial ANOVA This walk-through is a continuation of our previous work with factorial ANOVA. If you haven’t already, please check the factorial ANOVA and Interaction vignettes. In truth, nothing terribly new is being introduced here; we are just ramping up the complexity of our ANOVA models. Whereas before we considered a scenario with just 2 factors, here we will consider a 3 factor ANOVA. For all practical purposes you typically do not want to go higher than a 3 or 4 factor ANOVA, simply because of the exponential increase in complexity. For example, consider below where each letter is an IV and the resulting tests in the full factorial omnibus ANOVA: 2 factor: a, b, a×b 3 factor: a, b, c, a×b, a×c, b×c, a×b×c 4 factor: a, b, c, d, a×b, a×c, a×d, b×c, b×d, c×d, a×b×c, a×b×d, a×c×d, b×c×d, a×b×c×d 5 factor: ummm, no… just don’t (unless you really want to) Increasing the number of factors not only increases the practical difficulty of analysis, but more importantly, makes it more difficult to interpret your results. The primary culprit here is the potential presence of multiple interactions. When dealing with interactions in a higher order factorial design you always start off with the highest order interaction. For example in the 3 factor case, if you have a 3-way interaction, that supercedes any other effects and interactions, and must be dealt with first. If there is no three-way interaction, then you can co about the business of addressing any two way interactions that are present. 12.1 Packages and data Let’s load in our necessary packages, scripts and some data and try some examples. This write-up requires the following packages: # required packages: pacman::p_load(tidyverse, afex, cowplot, agricolae) # custom simple effects ANOVA script (see interaction vignette): source(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsScripts/simpleEffectsAOV.R&quot;) And now let’s reaquaint ourselves with our dataset from last week, but with one new twist… Background: Given the ease of access for new technologies and increasing enrollment demands, many grade schools are advocating that teachers switch over to E-courses, where students view an online, pre-recorded lecture on the course topic in lieu of sitting in a classroom in a live lecture. Given this push, a critical question remains regarding the impact of E-courses on primary school student outcomes. More it may be the case that certain subject content more readily lends itself to E-course presentations than other subjects. To address this question we tested students performance on a test one week after listening to the related lecture. Lectures were either experienced via online (Computer) or in a live classroom (Standard). In addition, the lecture content varied in topic (Physical science, Social science, History). Finally, to assess whether age had an impact on expected outcomes we assessed students in 5th and 8th grades. Looking at the above we have a 2 (Grade: 5th, 8th) × 2 (Presentation: Computer, Standard) × 3 (Lecture: Physical, Social, History) design. Our omnibus ANOVA therefore will test for the following effects: Main Effects: Grade, Presentation, Lecture 2-way interactions: Grade × Presentation, Grade × Lecture, Lecture × Presentation 3-way interation: Grade × Presentation × Lecture 12.2 EXAMPLE 1: no three-way interaction, single two way interaction Let’s load in this data: higherEx1 &lt;- read_delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsData/ANOVA5_higherEx1.txt&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Subject = col_integer(), ## Grade = col_character(), ## Lecture = col_character(), ## Presentation = col_character(), ## Score = col_integer() ## ) 12.2.1 First plot: 3 way interaction plot One thing you may notice is since we have a more complex ANOVA, we have a more complex design with three factors we also have to construct a more complex plot. Whereas in the 2 factor case we could place one factor on the x-axis and group the other factor by line or shape or color; in the three factor case things become slightly more tricky. One thing to consider is the logic of what we are doing with our plots. When we have two factors, say A &amp; B, we are distinguishing between each level of B (by lines, shapes, colors) at each level of A (on the x-axis). In the 3 factor case where we have A,B, &amp; C, we are distinguishing the B×C interaction (through the combination of shapes, lines, colors) on each level of A. In the folling plot, let’s place Lecture type along the x-axis. From here, we can create 2 groups of lines (each with 2 levels) representing the Grade × Presentation interaction. We note this by specifying the interaction in group of our baseline ggplot call: require(ggplot2) p &lt;- ggplot(higherEx1, mapping = aes(x = Lecture, y = Score, group = interaction(Presentation, Grade))) From here, we can proceed as before. Note, however, depending on the complexity of your plot you may also need to specify the interaction in each stat_summary::aes() as necessary. Below I am changing the shape by Presentation and the linetype by Grade p &lt;- p + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.15), aes(linetype = Grade)) + theme_cowplot() + # adjusting the legend theme(legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.2, 0.25)) + # renaming axies xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + # adding whitespace around plot (optional) theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) show(p) OK, not exactly the best plot. Let’s see if we can create some more space to work with by forcing the y-axis to go to 0, instead of cutting off at 10. There’s also a trick that we can use to stack our legend keys horizontally, rather than vertically p &lt;- p + # take the previous plot and... # set the y-axis limits to 0 and 50 coord_cartesian(ylim=c(0,50)) + # direction to stack legend keys theme(legend.direction = &quot;horizontal&quot;, legend.position = c(.2,.25)) show(p) alternatively, we could also just stack the legend boxes side by side: # redoing intial steps p &lt;- ggplot(higherEx1, mapping = aes(x = Lecture, y = Score, group = interaction(Presentation, Grade))) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.15), aes(linetype = Grade)) + theme_cowplot() + theme(legend.position = c(0.25, 0.25)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + coord_cartesian(ylim = c(0, 50)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) 12.2.2 running the ANOVA: From here we run the omnibus as usual: omnibus.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = higherEx1, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;, &quot;Grade&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture, Presentation, Grade ## Contrasts set to contr.sum for the following variables: Lecture, Presentation, Grade omnibus.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 60 54.619 21.3327 0.41558 1.004e-07 ## Presentation 1 60 54.619 80.0287 0.57152 1.217e-12 ## Grade 1 60 54.619 0.0064 0.00011 0.9367 ## Lecture:Presentation 2 60 54.619 13.3660 0.30821 1.582e-05 ## Lecture:Grade 2 60 54.619 0.0033 0.00011 0.9967 ## Presentation:Grade 1 60 54.619 0.0023 0.00004 0.9620 ## Lecture:Presentation:Grade 2 60 54.619 0.0008 0.00003 0.9992 ## ## Lecture *** ## Presentation *** ## Grade ## Lecture:Presentation *** ## Lecture:Grade ## Presentation:Grade ## Lecture:Presentation:Grade ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In this case we only have Main effects for Lecture and Presentation, and a Lecture × Presentation interaction. Given this we can disregard Grade as its not contributing to any effects. 12.2.3 Replotting the 2-way interaction In some cases it is advisable to re-plot the data factoring out Grade by removing Grade from baseline group= as well as any summary_stat::aes(), (essentially treating this if it was a 2 factorial design like last week). For the sake of your sanity, and your readers’ sanity) I would also suggest being consistent with how you present your conditions. For example, in the original plot, Presentation was differentiated by shape. To be consistent, we need to do the same here). (You’ll note in the example below I’m also customing font size and weight ourely for personal aesthetics): p &lt;- ggplot(higherEx1, mapping = aes(x = Lecture, y = Score, group = Presentation)) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.25, 0.25)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + coord_cartesian(ylim = c(0, 50)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) ## Warning: Ignoring unknown aesthetics: shape show(p) From here I would deal with the follow-ups as we did in our 2×3 example from the last write-up: Run the simple effects and any necessary posthocs, being sure to correct using the error terms from the omnibus ANOVA. 12.3 EXAMPLE 2: No three-way interaction, multiple main effects, multiple 2 way interactions: Let’s load in a data set that is a little more complicated: higherEx2 &lt;- read_delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsData/ANOVA5_higherEx2.txt&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Subject = col_integer(), ## Grade = col_character(), ## Lecture = col_character(), ## Presentation = col_character(), ## Score = col_integer() ## ) 12.3.1 plotting the data p &lt;- ggplot(higherEx2, mapping = aes(x = Lecture, y = Score, group = interaction(Presentation, Grade))) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.15), aes(linetype = Grade)) + theme_classic() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.25, 0.25)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + coord_cartesian(ylim = c(0, 60)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) Looking at this plot its quite apparent that the pattern of results is different from Example 1. Perhaps most simply, there is a general shared pattern of effects for each of the shapes (Presentation) with the exception of the 8th Grade-Computer condition. For the remainder there is a dip for Social lectures, but in this one condition Social actually increases. Let’s run our ANOVA. 12.3.2 running the omnibus ANOVA: omnibus.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = higherEx2, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;, &quot;Grade&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture, Presentation, Grade ## Contrasts set to contr.sum for the following variables: Lecture, Presentation, Grade omnibus.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 60 54.619 7.0887 0.19113 0.001723 ## Presentation 1 60 54.619 80.0287 0.57152 1.217e-12 ## Grade 1 60 54.619 5.6454 0.08600 0.020711 ## Lecture:Presentation 2 60 54.619 13.3660 0.30821 1.582e-05 ## Lecture:Grade 2 60 54.619 5.5325 0.15570 0.006236 ## Presentation:Grade 1 60 54.619 0.0023 0.00004 0.962004 ## Lecture:Presentation:Grade 2 60 54.619 0.0008 0.00003 0.999237 ## ## Lecture ** ## Presentation *** ## Grade * ## Lecture:Presentation *** ## Lecture:Grade ** ## Presentation:Grade ## Lecture:Presentation:Grade ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our ANOVA reveals an abundance of results. There are main effects for each of our IVs. In addition there are two interaction effects: Lecture:Presentation and Lecture:Grade. Unless there is a strong theoretical reason not to (which I see none) we will need to examine each of these interactions in further detail. 12.3.3 testing the Lecture:Presentation interaction: 12.3.3.1 2-way interaction plot We can begin by recreating our interaction plot, this time only focusing on the IVs that are of interest (those that are interacting). In this first case, they are Lecture and Presentation. p &lt;- ggplot(higherEx2, mapping = aes(x = Lecture, y = Score, group = Presentation)) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_cl_normal&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.25, 0.25)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + coord_cartesian(ylim = c(0, 60)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) ## Warning: Ignoring unknown aesthetics: shape show(p) 12.3.3.2 simple effect ANOVAs: Provided what we see on this plot, it may make sense to first run a simple effects ANOVA for each Presentation and then run the appropriate follow-ups. I say this because the more obvious, and potentially easily interpret-able effects occur moving across the line series (i.e., lines trend up or down). 12.3.3.2.1 computer AOV and follow-ups: computer.data &lt;- filter(higherEx2, Presentation == &quot;Computer&quot;) computer.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = computer.data, between = c(&quot;Lecture&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture ## Contrasts set to contr.sum for the following variables: Lecture simpleEffectsAOV(omnibus.aov, computer.aov) ## Effect full_mse ftest ## [1,] &quot;Lecture&quot; &quot;54.619&quot; &quot;F(2,60) = 4.739, p = 0.012, pes=0.136&quot; Our simple effects ANOVA of Lecture on our computer.data was significant, suggesting that we should perform a post-hoc: agricolae::HSD.test(computer.data$Score, trt = computer.data$Lecture, MSerror = 54.62, DFerror = 60, group = T, console = T) ## ## Study: computer.data$Score ~ computer.data$Lecture ## ## HSD Test for computer.data$Score ## ## Mean Square Error: 54.62 ## ## computer.data$Lecture, means ## ## computer.data.Score std r Min Max ## History 38.00000 4.177864 12 33 45 ## Physical 45.91667 6.734691 12 33 53 ## Social 46.16667 7.814129 12 33 58 ## ## Alpha: 0.05 ; DF Error: 60 ## Critical Value of Studentized Range: 3.398661 ## ## Minimun Significant Difference: 7.25092 ## ## Treatments with the same letter are not significantly different. ## ## computer.data$Score groups ## Social 46.16667 a ## Physical 45.91667 a ## History 38.00000 b The results of the Tukey post-hoc suggests that History scores are significantly less than the other two groups for the Computer presentation. 12.3.3.2.2 standard AOV and follow-ups: standard.data &lt;- filter(higherEx2, Presentation == &quot;Standard&quot;) standard.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = standard.data, between = c(&quot;Lecture&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture ## Contrasts set to contr.sum for the following variables: Lecture simpleEffectsAOV(omnibus.aov, standard.aov) ## Effect full_mse ftest ## [1,] &quot;Lecture&quot; &quot;54.619&quot; &quot;F(2,60) = 15.715, p &lt; .001, pes=0.344&quot; Our simple effects ANOVA for cstandard.data was also significant, suggesting that we should perform a post-hoc: agricolae::HSD.test(standard.data$Score, trt = standard.data$Lecture, MSerror = 54.62, DFerror = 60, group = T, console = T) ## ## Study: standard.data$Score ~ standard.data$Lecture ## ## HSD Test for standard.data$Score ## ## Mean Square Error: 54.62 ## ## standard.data$Lecture, means ## ## standard.data.Score std r Min Max ## History 31.08333 9.624385 12 20 46 ## Physical 34.08333 10.483392 12 18 49 ## Social 18.16667 7.383438 12 6 29 ## ## Alpha: 0.05 ; DF Error: 60 ## Critical Value of Studentized Range: 3.398661 ## ## Minimun Significant Difference: 7.25092 ## ## Treatments with the same letter are not significantly different. ## ## standard.data$Score groups ## Physical 34.08333 a ## History 31.08333 a ## Social 18.16667 b For the standard presentation, scores in the History and Physical lecture were greater than the Social lecture. 12.3.4 testing the Lecture:Grade interaction: Now we do the same for the Lecture:Grade interaction. This time removing Presentation from our consideration. 12.3.4.1 2-way interaction plot You’ll notice that in this interaction plot, and the plot above, I am using the shape/linetype conventions that I established in my original plot. This will make things much easier if I wind up comparing these plots to my 3-way plot. I know I’ve said this before, but ONE MORE TIME WITH FEELING! p &lt;- ggplot(higherEx2, mapping = aes(x = Lecture, y = Score, group = Grade)) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, position = position_dodge(0.15), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.15), aes(linetype = Grade)) + theme_classic() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.25, 0.25)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + coord_cartesian(ylim = c(0, 60)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) 12.3.5 simple effect ANOVAs: Looking at this plot Provided what we see on this plot, it may make the best sense to examine the pairwise differences in each Grade group by Lecture. Whereas in the previous interaction, the compelling changes occurred across lines, here what is more compelling is the presence/absence of gaps between the lines. In particular, while there are practically no differences due to Grade in the History or Physical Lectures, there is a larger gap in the Social condition. 12.3.5.1 History AOV: history.data &lt;- filter(higherEx2, Lecture == &quot;History&quot;) history.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = history.data, between = c(&quot;Grade&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Grade ## Contrasts set to contr.sum for the following variables: Grade simpleEffectsAOV(omnibus.aov, history.aov) ## Effect full_mse ftest ## [1,] &quot;Grade&quot; &quot;54.619&quot; &quot;F(1,60) = 0.001, p = 0.978, pes=0&quot; 12.3.5.2 Physical AOV: physical.data &lt;- filter(higherEx2, Lecture == &quot;Physical&quot;) physical.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = physical.data, between = c(&quot;Grade&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Grade ## Contrasts set to contr.sum for the following variables: Grade simpleEffectsAOV(omnibus.aov, physical.aov) ## Effect full_mse ftest ## [1,] &quot;Grade&quot; &quot;54.619&quot; &quot;F(1,60) = 0, p = 1, pes=0&quot; 12.3.5.3 Social AOV: social.data &lt;- filter(higherEx2, Lecture == &quot;Social&quot;) social.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = social.data, between = c(&quot;Grade&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Grade ## Contrasts set to contr.sum for the following variables: Grade simpleEffectsAOV(omnibus.aov, social.aov) ## Effect full_mse ftest ## [1,] &quot;Grade&quot; &quot;54.619&quot; &quot;F(1,60) = 16.71, p &lt; .001, pes=0.218&quot; As anticipated from looking at the plot, there was a significant difference between our Grades in the Social group, but not the remainder. 12.3.6 Main takeaways / write-up It’s typically best to go back and look at all of your plots. the main effect for Presentation is meaningful: Regardless of what plays out on the other factors, students tended to perform better in the computer presentation. Best to report that. the Lecture:Presentation interaction resulted from the following: in the Computer presentations scores were lowest in the History lecture (indifferent in other two) in the Standard presentations scores were lowest in the Social lecture (indifferent in the other two) the Lecture:Grade interaction resulted from the 5th graders performing worse than the eight graders in Social lectures; while performance was not different in the other two lectures. With this in mind, an example write-up: Our results revealed main effects for each of our factors, in addition to Lecture × Presentation \\([F(2,60)=13.37, p&lt;.001, \\eta_p^2=.31]\\) and Lecture × Grade interactions \\([F(2,60)=5.53, p=.006, \\eta_p^2=.16]\\). Considering the former interaction, students presented material via computer format tended to perform poorest from the History Lecture, compared to Physical and Social (Tukey HSD, p&lt;.05). However, when material was presented in the standard format, students performed poorest in the Social condition (p&lt;.05), while the other two were not different. Considering the latter interaction, a detailed analysis of performance as a function of lecture revaled that 8th Graders performed better than their 5th grade counterparts only in the Social lecture, \\(F(1,60) = 16.71, p &lt; .001, \\eta_p^2=0.21\\). In all conditions, students performed better when presented the material via computer compared to the standard presentation, \\([F(2,60)=7.09, p=.002, \\eta_p^2=.19]\\) (see Figure 1). 12.4 EXAMPLE 3: OMG, multiple two way interactions and a nasty three-way!!! Finally let’s take a look at some data that is all over the place: higherEx3 &lt;- read_delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsData/ANOVA5_higherEx3.txt&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Subject = col_integer(), ## Grade = col_character(), ## Lecture = col_character(), ## Presentation = col_character(), ## Score = col_integer() ## ) 12.4.1 Plotting: p &lt;- ggplot(higherEx3, mapping = aes(x = Lecture, y = Score, group = interaction(Presentation, Grade))) + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, position = position_dodge(0.5), aes(shape = Presentation)) + stat_summary(geom = &quot;line&quot;, fun.y = &quot;mean&quot;, position = position_dodge(0.5), aes(linetype = Grade)) + theme_classic() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.25, 0.15)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + coord_cartesian(ylim = c(0, 50)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) 12.4.2 running the ANOVA: omnibus.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = higherEx3, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;, &quot;Grade&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture, Presentation, Grade ## Contrasts set to contr.sum for the following variables: Lecture, Presentation, Grade omnibus.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 60 42.861 11.6436 0.27960 5.336e-05 ## Presentation 1 60 42.861 36.1491 0.37597 1.171e-07 ## Grade 1 60 42.861 65.6189 0.52236 3.302e-11 ## Lecture:Presentation 2 60 42.861 4.7609 0.13696 0.0120482 ## Lecture:Grade 2 60 42.861 3.7835 0.11199 0.0283455 ## Presentation:Grade 1 60 42.861 16.2592 0.21321 0.0001583 ## Lecture:Presentation:Grade 2 60 42.861 3.8743 0.11437 0.0261541 ## ## Lecture *** ## Presentation *** ## Grade *** ## Lecture:Presentation * ## Lecture:Grade * ## Presentation:Grade *** ## Lecture:Presentation:Grade * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 O.M.F.G., everything is significant!!! What to do!!! Remember, the first thing that we do is tackle the highest-order interaction. In this case we should tease-out the three-way. Again, we should look to our plot to help guide us in what to do. Looking at the plot it appears that different things are happening by Grade. The Presentation:Lecture lines tend to stay parallel for the 5th graders, but not as much for the 8th graders. At the same time the gaps between the two lines are different by grade. So, my advice would be to attack the 3-way interaction by looking at the individual Lecture:Presentation interactions on each Grade. 12.4.3 Fifth Graders, Lecture:Presentation interaction. fifth.data &lt;- filter(higherEx3, Grade == &quot;5th&quot;) fifth.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = fifth.data, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture, Presentation ## Contrasts set to contr.sum for the following variables: Lecture, Presentation simpleEffectsAOV(omnibus.aov, fifth.aov) ## Effect full_mse ## [1,] &quot;Lecture&quot; &quot;42.861&quot; ## [2,] &quot;Presentation&quot; &quot;42.861&quot; ## [3,] &quot;Lecture:Presentation&quot; &quot;42.861&quot; ## ftest ## [1,] &quot;F(2,60) = 2.167, p = 0.123, pes=0.067&quot; ## [2,] &quot;F(1,60) = 1.96, p = 0.167, pes=0.032&quot; ## [3,] &quot;F(2,60) = 0.024, p = 0.976, pes=0.001&quot; You’ll note that our results include simple effects for Lecture and Presentation for 5th graders as well as their interaction. That said, there are no effects when looking at the fifth graders. 12.4.4 Eighth Graders, Lecture:Presentation interaction. eighth.data &lt;- subset(higherEx3, Grade == &quot;8th&quot;) eighth.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = eighth.data, between = c(&quot;Lecture&quot;, &quot;Presentation&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture, Presentation ## Contrasts set to contr.sum for the following variables: Lecture, Presentation simpleEffectsAOV(omnibus.aov, eighth.aov) ## Effect full_mse ## [1,] &quot;Lecture&quot; &quot;42.861&quot; ## [2,] &quot;Presentation&quot; &quot;42.861&quot; ## [3,] &quot;Lecture:Presentation&quot; &quot;42.861&quot; ## ftest ## [1,] &quot;F(2,60) = 13.261, p &lt; .001, pes=0.307&quot; ## [2,] &quot;F(1,60) = 50.448, p &lt; .001, pes=0.457&quot; ## [3,] &quot;F(2,60) = 8.611, p &lt; .001, pes=0.223&quot; The detailed Lecture:Presentation analysis for the 8th graders revealed both main effects and their interaction. We need to follow-up on this interaction in the 8th-grade data. In this case we would look at the effect of Lecture on each level of Presentation for the eighth graders. To do this we first subset the data further: eighth.computer.data &lt;- filter(eighth.data, Presentation == &quot;Computer&quot;) eighth.standard.data &lt;- filter(eighth.data, Presentation == &quot;Standard&quot;) 12.4.4.1 8th graders: Computer presentation ANOVA of Lecture on the 8th graders via Computer presentation: eighth.computer.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = eighth.computer.data, between = c(&quot;Lecture&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture ## Contrasts set to contr.sum for the following variables: Lecture simpleEffectsAOV(omnibus.aov, eighth.computer.aov) ## Effect full_mse ftest ## [1,] &quot;Lecture&quot; &quot;42.861&quot; &quot;F(2,60) = 2.264, p = 0.113, pes=0.07&quot; No effect for lecture type for 8th graders using the computer presentation. 12.4.4.2 8th graders: Standard presentation ANOVA of Lecture on the 8th graders via Standard presentation eighth.standard.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = eighth.standard.data, between = c(&quot;Lecture&quot;), within = NULL, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture ## Contrasts set to contr.sum for the following variables: Lecture simpleEffectsAOV(omnibus.aov, eighth.standard.aov) ## Effect full_mse ftest ## [1,] &quot;Lecture&quot; &quot;42.861&quot; &quot;F(2,60) = 19.607, p &lt; .001, pes=0.395&quot; This yielded an effect, so now the necessary follow-up post-hocs: agricolae::HSD.test(eighth.standard.data$Score, trt = eighth.standard.data$Lecture, DFerror = 60, MSerror = 42.86, console = T, group = T) ## ## Study: eighth.standard.data$Score ~ eighth.standard.data$Lecture ## ## HSD Test for eighth.standard.data$Score ## ## Mean Square Error: 42.86 ## ## eighth.standard.data$Lecture, means ## ## eighth.standard.data.Score std r Min Max ## History 31.16667 9.867455 6 21 44 ## Physical 34.16667 10.980285 6 19 49 ## Social 12.33333 3.723797 6 7 17 ## ## Alpha: 0.05 ; DF Error: 60 ## Critical Value of Studentized Range: 3.398661 ## ## Minimun Significant Difference: 9.083607 ## ## Treatments with the same letter are not significantly different. ## ## eighth.standard.data$Score groups ## Physical 34.16667 a ## History 31.16667 a ## Social 12.33333 b Scores for the Social test were less than the Physical and History. 12.4.5 Constructing a narrative: Given these results, what narrative do you think you can tell? Remember use the plots to unpack what the results are telling you. Our results focused on differences in the Lecture:Presentation analysis for each grade. I would recommend that your story start there. Something to the effect of Lecture and Presentation interact for 8th graders but not 5th graders. To remind ourselves: Main effects for Lecture, Presentation, and Grade Two way interactions Three-way interaction that we focus on The results of the 3-way interaction, by Grade: 5th graders = no effects 8th graders = Lecture × Presentation interaction, where there was a null effect for Lecture on the Computer presentation, but an effect for Lecture on the Standard Presentation. Now the narrative: … to answer this question we conducted a 2 (Grade) × 2 (Presentation) × 3 (Lecture) between effects ANOVA. Our ANOVA revealed a three-way, Grade × Presentation × Lecture type interaction, F(2, 60) = 3.87, p = .03, \\(\\eta_p^2\\) = .11. To address this interaction we conducted seperate Presentation × Lecture ANOVA for each Grade. Our results for 5th graders showed no main effects nor an interaction (p &gt; .05). For 8th graders, we found an Presentation × Lecture interaction, F(2, 60) = 8.61, p &lt; .001, \\(\\eta_p^2\\) = .22. While there were no differences due to Lecture type for 8th graders presented information via computer (p &gt; .05), 8th graders in the Standard presentation, F(2, 60) = 19.61, p &lt; .001, \\(\\eta_p^2\\) = .40 tended to perform worse for the Social lecture (M ± SD = 12.33 ± 3.72) than for the History (31.17 ± 9.87) and Physical (34.17 ± 10.98) lectures (Tukey HSD, p &lt; .05). "],
["anova6-within-subjects.html", "Week 13 ANOVA6: Within-Subjects 13.1 Within-Subject v. Between Subjects 13.2 EXAMPLE 1 13.3 EXAMPLE 2 13.4 EXAMPLE 3 13.5 How to perform a within subjects analysis in SPSS:", " Week 13 ANOVA6: Within-Subjects 13.1 Within-Subject v. Between Subjects Up until now we have considered ANOVA in between subjects designs. Data at each level of each factor comes from a different set of participants. This week we more onto within-subjects designs. As we mentioned in class we have a within-subject design whenever data from the same participants exists on at least two levels of a factor (or analogously occupied at least two cells within our interaction matrix). To contrast some important distinctions let’s revisit our familiar data set contrasting test outcomes for students as a function of Lecture. I realize before, Lecture was crossed with at least one other factor, but for the sake on simplicity let’s just consider data from this single factor. The goal of this first section is to contrast results as a function whether this data is considered within-subjects or between-subjects. within_between &lt;- read_delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsData/withinVbetween.txt&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## WithinSubjects = col_integer(), ## Lecture = col_character(), ## Score = col_integer(), ## BetweenSubjects = col_integer() ## ) within_between ## # A tibble: 36 x 4 ## WithinSubjects Lecture Score BetweenSubjects ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1 Physical 53 1 ## 2 2 Physical 49 2 ## 3 3 Physical 47 3 ## 4 4 Physical 42 4 ## 5 5 Physical 51 5 ## 6 6 Physical 34 6 ## 7 7 Physical 44 7 ## 8 8 Physical 48 8 ## 9 9 Physical 35 9 ## 10 10 Physical 18 10 ## # ... with 26 more rows 13.1.1 Within v Between ANOVA So we have our dataset within_between. You’ll note that there are two subjects columns WithinSubjects which imagines 12 participants each going through all 3 Lecture types and BetweenSubjects where each participant (N=36) is assigned to a single Lecture type. Previously, we might have treated this as a between subjects design. Looking at the ezDeign of this design we see that every BetweenSubject is assigned to a single condition (as evidenced by count = 1) ez::ezDesign(within_between, y = Lecture, x = BetweenSubjects) Skipping past the preliminaries (e.g., testing for assumptions) and straight to running the BS ANOVA: between.aov &lt;- afex::aov_ez(id = &quot;BetweenSubjects&quot;, dv = &quot;Score&quot;, data = within_between, between = &quot;Lecture&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) ## Converting to factor: Lecture ## Contrasts set to contr.sum for the following variables: Lecture between.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 33 139.36 4.2838 0.20611 0.02218 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 However, let’s assume instead that this data set comes a within design. That is, instead of different participants in each Lecture group, the same group of people went through all three lectures: ez::ezDesign(within_between, y = Lecture, x = WithinSubjects) We see that the ezDesign has changed. Instead of 36 participants each individually assigned to a single condition, we have 12 participants each assigned to all three conditions for a single trial (measure). Running the within-subjects ANOVA: ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 2 22 48.515 12.305 0.52801 0.000259 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (You may have noticed I’ve added a correction argument to my aov_ez call. More on this later) In both of these cases the data is exactly the same. What has changed is how we parse the variance (you’ll notice that the denominator degrees of freedom are different for the second ANOVA). In a within design, we need to take into account the “within-subject” variance. That is how individual subjects vary from one level of treatment to the other. In this respect, within designs are typically more powerful than analogous between designs. Whilet the inherent differences between individual subjects is present in both types of designs, your within-subjects ANOVA model includes it in its analysis. This increase in power is reflected by the lower MSE (48.52 v. 139.36) and subsequently, larger F-value (12.31 v. 4.28) and effect size (0.53 v. 0.21) in our within-subjects analysis. Well if that’s the case why not run within-subject (WS) designs all of the time. Well, typically psychologists do when the phenomena lends itself to WS-designs. BUT there are certainly times when they are not practical, for example if you are concerned about learning, practice, or carryover effects where exposure to a treatment on one level might impact the other levels. For example if you were studying radiation poisoning and had a placebo v. radiation dose condition, it be likely that you wouldn’t run your experiement as a within—or at the very least you wouldn’t give them the radiation first. It would also be likely that you’d be in violation several standards of ethics. Perhaps a little more inside baseball, there are underlying questions on whether or not we are indeed making the appropriate corrections in our WS models or correctly specifying our experiments units. For example, if participants data is collected over 4 (or any number) trials in each treactment condition, then both SPSS and R afex require you to collapse those trials to means, therefore turning what are truly 4 observations into 1 see Max et al., 1999 for details. In R, afex handles this automatically for data in long format. In SPSS you need to organize your data in another program (like Excel), calculate the means, and then send that data to SPSS. At the same time there are debates as to the importance of sphericity in the subjects data. One alternative method that avoids these issues is to invoke mixed models (e.g., lmer). However, if you really want to go down the rabbit hole check out Doug Bates reponse on appropriate dfs and p.values in lmer. You’ll not that these discussions were ten years ago and are still being debated (see here. For now, we won’t go down the rabbit hole and just focus on the practical issues confronted when running a repeated-measures ANOVA. 13.2 EXAMPLE 1 To start, we will use data from Howell’s example in the opening of Chapter 14. Here are data related to the effectiveness of relaxation therapy to the number of episodes that chronic migraine sufferers reported. Data was collected from each subject over the course of 5 weeks. After week 2 the therapy treatment was implemented effectively dividing the 5 weeks into two phases, Pre (Weeks 1 &amp; 2) and Post (Weeks 3,4, &amp; 5). The data that you see here represent a single observation per participant per week, so with this example there is no need to collapse multiple observations to a mean value per participant per week. We’ll confront an example where this is an issue later in the vignette. 13.2.1 loading in the data: example1 &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab14-3.dat&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Subject = col_integer(), ## Wk1 = col_integer(), ## Wk2 = col_integer(), ## Wk3 = col_integer(), ## Wk4 = col_integer(), ## Wk5 = col_integer() ## ) You’ll notice that the data set above is in wide format. Each subject is on a single row and each week is in its own column. Note that this is the preferred format for within subjects analysis for SPSS. However in R we want it in long format. example1_long &lt;- gather(example1, key = &quot;Week&quot;, value = &quot;Migraines&quot;, 2:6) example1_long$Week &lt;- as.factor(example1_long$Week) example1_long ## # A tibble: 45 x 3 ## Subject Week Migraines ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 Wk1 21 ## 2 2 Wk1 20 ## 3 3 Wk1 17 ## 4 4 Wk1 25 ## 5 5 Wk1 30 ## 6 6 Wk1 19 ## 7 7 Wk1 26 ## 8 8 Wk1 17 ## 9 9 Wk1 26 ## 10 1 Wk2 22 ## # ... with 35 more rows Ok, much better, each Subject × Week observation is on a single row. 13.2.2 plotting the data One additional concern that we must deal with when plotting within-subjects data is the error bars. Plotting the standard error or regular confidence intervals may be misleading for making statistical inferences. This is beause the values, normally caluculated do not account for within subject correlation. Luckily for us there is a correction that we can make using the Rmisc package. Please see Cousineau (2005) and Morey (2008) for details on this issue and subsequent correction. The practical steps for this correction include first norming the data to account for within subjects correlations. normedData &lt;- Rmisc::normDataWithin(data = example1_long, measurevar = &quot;Migraines&quot;, idvar = &quot;Subject&quot;, betweenvars = NULL) normedData ## Subject Week Migraines MigrainesNormed ## 1 1 Wk1 21 21.644444 ## 2 1 Wk4 6 6.644444 ## 3 1 Wk5 6 6.644444 ## 4 1 Wk3 8 8.644444 ## 5 1 Wk2 22 22.644444 ## 6 2 Wk5 4 5.844444 ## 7 2 Wk4 4 5.844444 ## 8 2 Wk2 19 20.844444 ## 9 2 Wk3 10 11.844444 ## 10 2 Wk1 20 21.844444 ## 11 3 Wk5 5 9.044444 ## 12 3 Wk2 15 19.044444 ## 13 3 Wk4 4 8.044444 ## 14 3 Wk3 5 9.044444 ## 15 3 Wk1 17 21.044444 ## 16 4 Wk4 12 5.844444 ## 17 4 Wk5 17 10.844444 ## 18 4 Wk3 13 6.844444 ## 19 4 Wk2 30 23.844444 ## 20 4 Wk1 25 18.844444 ## 21 5 Wk4 8 4.444444 ## 22 5 Wk1 30 26.444444 ## 23 5 Wk3 13 9.444444 ## 24 5 Wk5 6 2.444444 ## 25 5 Wk2 27 23.444444 ## 26 6 Wk5 4 4.244444 ## 27 6 Wk4 7 7.244444 ## 28 6 Wk3 8 8.244444 ## 29 6 Wk1 19 19.244444 ## 30 6 Wk2 27 27.244444 ## 31 7 Wk5 5 7.444444 ## 32 7 Wk3 5 7.444444 ## 33 7 Wk2 16 18.444444 ## 34 7 Wk4 2 4.444444 ## 35 7 Wk1 26 28.444444 ## 36 8 Wk5 5 8.444444 ## 37 8 Wk3 8 11.444444 ## 38 8 Wk4 1 4.444444 ## 39 8 Wk2 18 21.444444 ## 40 8 Wk1 17 20.444444 ## 41 9 Wk4 8 5.044444 ## 42 9 Wk3 14 11.044444 ## 43 9 Wk1 26 23.044444 ## 44 9 Wk2 24 21.044444 ## 45 9 Wk5 9 6.044444 We then calculate the se, sd, and ci values of our normed data, in this case MigrainesNormed. The final step is to make a correction on these Normed values (Morey (2008)). This is done by taking the number of levels of the within factor (nWithinGroups) and applying the following correction: # get the number of levels in Week: nWithinGroups &lt;- nlevels(example1_long$Week) # apply the correction factor: correctionFactor &lt;- sqrt(nWithinGroups/(nWithinGroups - 1)) The range of our corrected errorbars are the sd, se, ci multiplied by this correctionFactor. For example, the sd of participants’ Migraines in Wk1 is: sd(normedData$MigrainesNormed[normedData$Week == &quot;Wk1&quot;]) * correctionFactor ## [1] 3.588369 Fortunately there is a function in Rmisc that handles this correction for us, summarySEwithin. It is very similar to the summarySE function you are familiar with, but asks you to specify which IVs are within-subjects (withinvars), between-subjects(betweenvars) and which column contains subject IDs idvar. Using our original example1_long data: Rmisc::summarySEwithin(data = example1_long, measurevar = &quot;Migraines&quot;, withinvars = &quot;Week&quot;, betweenvars = NULL, idvar = &quot;Subject&quot;) ## Week N Migraines sd se ci ## 1 Wk1 9 22.333333 3.588369 1.1961229 2.758264 ## 2 Wk2 9 22.000000 2.993280 0.9977598 2.300838 ## 3 Wk3 9 9.333333 1.984663 0.6615545 1.525547 ## 4 Wk4 9 5.777778 1.474788 0.4915960 1.133623 ## 5 Wk5 9 6.777778 2.837252 0.9457507 2.180905 Unfortunately, to date I haven’t found a way to get this to play nice with stat_summary() in ggplot(). HOWEVER, there is a simple work around. Since stat_summary() is simply summarizing our means and error values from the dataset and summarySEwithin is doing the exact same thing, we can simply pull these values straight from summarySEwithin with one MAJOR caveat. summarySEwithin reports the normed means, however for plotting we need the original means. Note that this is not an issue when you are only dealing with within subjects factors, but if you are performing mixed ANOVA (combination within-subjects and between-subjects) these means can differ. To address this problem user Hause Lin created a custom function summarySEwithin2 that reports both normed and unnormed means. You can find this script on their Github site here. I would recommend copying and pasting the code you your own “.R” file for future use. In the meantime we can directly source this code from their site: source(&quot;https://gist.githubusercontent.com/hauselin/a83b6d2f05b0c90c0428017455f73744/raw/38e03ea4bf658d913cf11f4f1c18a1c328265a71/summarySEwithin2.R&quot;) A similar script may be found on this ggplot tutorial site, which forms the basis of this alternative plotting method. Note that this “alternative” is how I in fact create most of my own ANOVA plots. First we save the output of summarySEwithin2 to an object: summaryData &lt;- summarySEwithin2(data = example1_long, measurevar = &quot;Migraines&quot;, withinvars = &quot;Week&quot;, idvar = &quot;Subject&quot;) summaryData ## Week N Migraines MigrainesNormed sd se ci ## 1 Wk1 9 22.333333 22.333333 3.588369 1.1961229 2.758264 ## 2 Wk2 9 22.000000 22.000000 2.993280 0.9977598 2.300838 ## 3 Wk3 9 9.333333 9.333333 1.984663 0.6615545 1.525547 ## 4 Wk4 9 5.777778 5.777778 1.474788 0.4915960 1.133623 ## 5 Wk5 9 6.777778 6.777778 2.837252 0.9457507 2.180905 From here we can refer directly to summaryData in constructing the ggplot: p &lt;- ggplot(summaryData, mapping = aes(x = Week, y = Migraines, group = 1)) And now to construct the plot. rather than using summary_stat() we directly call each geom. For example, adding the means and as points: p &lt;- p + geom_point() p Connecting those points with lines: p &lt;- p + geom_line() p Adding error bars (SE): p &lt;- p + geom_errorbar(aes(ymin = Migraines - se, ymax = Migraines + se), width = 0) p (note that above I set width of the error bars to 0 to keep consistent with how we’ve been plotting pointranges). However, if you want caps, you can change the width. I recommend a width no higher than 0.2. From here you can use your familiar commands to whip it into APA format! 13.2.3 running a within ANOVA (afex): Running a WS ANOVA is just like running a BS ANOVA in afex. We call in our within subjects factors using the within= argument. within.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Migraines&quot;, data = example1_long, between = NULL, within = &quot;Week&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) within.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Migraines ## num Df den Df MSE F pes Pr(&gt;F) ## Week 2.7378 21.903 10.519 85.042 0.91402 5.773e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You’ll also notice in the code above that I’ve added a correction=&quot;none&quot; call to my anova_table argument. Otherwise afex defaults to applying the Greenhouse-Geisser correction link. There is an argument to be made that you should always make a correction to guard against deviations from sphericity. The correction becomes larger the further your data is from sphericity. However, standard practice in the psychology literature is to only apply the correction if our data fail Mauchly’s Test link. The outcome of this test can be obtained by using the summary() function summary(within.aov) ## Warning in summary.Anova.mlm(object$Anova, multivariate = FALSE): HF eps &gt; ## 1 treated as 1 ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 7893.7 1 486.71 8 129.747 3.186e-06 *** ## Week 2449.2 4 230.40 32 85.042 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## Week 0.28236 0.53699 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## Week 0.68446 5.773e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## Week 1.075624 1.39444e-16 This gives us our original ANOVA, the Mauchly Test, and the Greenhouse-Geisser and Huynh-Feldt corrections. In this case the Mauchly Test p = .54, so no corrections are necessary. If your elect to make the correction (i.e., the data fails Mauchly’s Test) then you could multiply your original degrees of freedom by the GG eps (or HF eps). The p.value next to the correction values tells you the p.value for your ANOVA effect assuming corrections. In this case even with the corrections the effect of Week is significant. Alternatively you could just rerun the afex function and specifying the correction= call. ## GG correction: within.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Migraines&quot;, data = example1_long, between = NULL, within = &quot;Week&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;GG&quot;)) within.aov ## Anova Table (Type 3 tests) ## ## Response: Migraines ## Effect df MSE F pes p.value ## 1 Week 2.74, 21.90 10.52 85.04 *** .91 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG ## HF correction (note that any HF eps &gt; 1 will be treated as 1): within.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Migraines&quot;, data = example1_long, between = NULL, within = &quot;Week&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;HF&quot;)) ## Warning: HF eps &gt; 1 treated as 1 within.aov ## Anova Table (Type 3 tests) ## ## Response: Migraines ## Effect df MSE F pes p.value ## 1 Week 4, 32 7.20 85.04 *** .91 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: HF 13.2.4 planned contrasts We can also perform planned contrasts. For example, in this case it make the most sense to examine differences between the pre-treatment weeks (1,2) and post-treatment weeks (3,4,5). To do this we can set up a contrast vector and and using the emmeans() function as below. You call in the within.aov, specify the custom contrast, and apply the contrast to the within factor. The resulting output gives you means data for each level as well as a t.test on the contrast. before.v.treat &lt;- c(-1/2, -1/2, 1/3, 1/3, 1/3) within.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Migraines&quot;, data = example1_long, between = NULL, within = &quot;Week&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;)) # custom.comp = custom contrasts note that I&#39;ve named the contrast custom.comp = list(before.v.treat = c(-1/2, -1/2, 1/3, 1/3, 1/3)) # original aov, , and apply it to Week: emmeans::emmeans(within.aov, specs = &quot;Week&quot;) %&gt;% contrast(., custom.comp) ## contrast estimate SE df t.ratio p.value ## before.v.treat -14.87037 0.8164966 32 -18.212 &lt;.0001 `` 13.3 EXAMPLE 2 Let’s take a look at another example, using a experimental paragdigm we are familiar with: 13.3.1 loading in the data You note that this data is already in long format so no need to adjust. example2_long &lt;- read_delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsData/ANOVA6_withinEx2.txt&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Subject = col_integer(), ## Lecture = col_character(), ## Score = col_integer() ## ) example2_long ## # A tibble: 36 x 3 ## Subject Lecture Score ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 Physical 53 ## 2 2 Physical 49 ## 3 3 Physical 47 ## 4 4 Physical 42 ## 5 5 Physical 51 ## 6 6 Physical 34 ## 7 7 Physical 44 ## 8 8 Physical 48 ## 9 9 Physical 35 ## 10 10 Physical 18 ## # ... with 26 more rows 13.3.2 plotting the data Let’s plot this data. This time, I’ll make my points a little larger, lines a little thicker, and add caps to the error bars: summaryData &lt;- summarySEwithin2(data = example2_long, measurevar = &quot;Score&quot;, withinvars = &quot;Lecture&quot;, idvar = &quot;Subject&quot;) ## Automatically converting the following non-factors to factors: Lecture p &lt;- ggplot(summaryData, mapping = aes(x = Lecture, y = Score, group = 1)) + geom_point(size = 2) + geom_line(size = 2) + geom_errorbar(aes(ymin = Score - se, ymax = Score + se), width = 0.15) + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.25, 0.25)) + scale_color_manual(values = c(&quot;black&quot;, &quot;grey50&quot;)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) Maybe not the best plot, but wanted to show you how to tweak things. 13.3.3 running the ANOVA: As before, let’s run this using afex: within.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = example2_long, between = NULL, within = &quot;Lecture&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) summary(within.aov) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 40401 1 3531.7 11 125.836 2.319e-07 *** ## Lecture 1194 2 1067.3 22 12.305 0.000259 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## Lecture 0.41524 0.012345 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## Lecture 0.63101 0.00225 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## Lecture 0.6748954 0.00173583 You’ll notice that in this example the data failed Mauchly Tests for Sphericity. As I mentioned above in this case you’ll need to make the appropriate corrections. GG corrections are the “industry standard” (that is, I typically see these). HF corrections are not as conservative, and are appropriate in instances where the GG eps &gt; 0.75. In this case we’ll use the GG correction. My advice, just rerun the ANOVA with correction=&quot;GG&quot;: within.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = example2_long, between = NULL, within = &quot;Lecture&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;GG&quot;, sig_symbols = rep(&quot;&quot;, 4))) within.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Score ## num Df den Df MSE F pes Pr(&gt;F) ## Lecture 1.262 13.882 76.885 12.305 0.52801 0.00225 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.3.4 post-hocs Post-hoc comparisons of means take a different form for repeated measures ANOVA. Typical methods such as Tukey HSD were designed for between-subjects effects, where it makes sense (assuming homogeneity of variance) to use a pooled error term. However, for within-subjects (or repeated measures) effects, the error term is the Treatment x Subjects interaction, and the nature of the TxS interaction across all treatment levels can be very different than it is for any particular pair of treatment levels. So the usual recommendation for carrying out pair-wise contrasts for a within-subjects factor is to use ordinary paired t-tests with an error term based only on the levels being compared. This can be most simply accomplished by running a seperate ANOVA for each comparison (this guarantees use of the appropriate error terms), dropping any unused levels from the analysis. For example, comparing Physical v. Social: physical_v_social &lt;- filter(example2_long, Lecture != &quot;History&quot;) afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Score&quot;, data = physical_v_social, between = NULL, within = &quot;Lecture&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Anova Table (Type 3 tests) ## ## Response: Score ## Effect df MSE F pes p.value ## 1 Lecture 1, 11 54.36 21.63 *** .66 .0007 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Note that tis p-value is NOT adjusted. A simple way of addressing this is using a Bonferroni correction. For example, if we run the 3 possible tests for this analysis then the critical p would need to be adjusted to .05/3, or .016. In this case I’m still good (.0007 &lt; .016) 13.4 EXAMPLE 3 Let’s ramp-up the complexity. Here is a dataset testing Recall over three days as a function of depth of processing (Lo, Med, High). In this case we have 2 within factors. 13.4.1 loading in the data: example3 &lt;- read_delim(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsData/ANOVA6_withinEx3.txt&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Subject = col_integer(), ## Recalled = col_integer(), ## ProcessDepth = col_character(), ## Day = col_character() ## ) example3 ## # A tibble: 90 x 4 ## Subject Recalled ProcessDepth Day ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 14 High Day1 ## 2 2 10 High Day1 ## 3 3 13 High Day1 ## 4 4 14 High Day1 ## 5 5 12 High Day1 ## 6 6 8 High Day1 ## 7 7 13 High Day1 ## 8 8 11 High Day1 ## 9 9 9 High Day1 ## 10 10 12 High Day1 ## # ... with 80 more rows 13.4.2 plotting the data Again, we need to make the appropriate correction for our error bars. In this case the number of within groups is the number of cells that is formed by crossing Day (1,2,3) and Processing Depth (Lo, Med, Hi). As this is a 3 * 3 design there are 9 cells, or nWithinGroups=9. As before this is handled auto-magically in summarySEwithin2 summaryData &lt;- summarySEwithin2(data = example3_long, measurevar = &quot;Recalled&quot;, withinvars = c(&quot;Day&quot;, &quot;ProcessDepth&quot;), idvar = &quot;Subject&quot;) p &lt;- ggplot(summaryData, mapping = aes(x = Day, y = Recalled, group = ProcessDepth)) + geom_point(size = 2) + geom_line(size = 1) + geom_errorbar(aes(ymin = Recalled - se, ymax = Recalled + se), width = 0.1) + theme_cowplot() + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.25, 0.25)) + scale_color_manual(values = c(&quot;black&quot;, &quot;grey50&quot;)) + xlab(&quot;Lecture&quot;) + ylab(&quot;Score&quot;) + theme(plot.margin = unit(c(0.25, 0.25, 0.25, 0.25), &quot;in&quot;)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) 13.4.3 running the omnibus ANOVA: omnibus.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Recalled&quot;, data = example3_long, between = NULL, within = c(&quot;ProcessDepth&quot;, &quot;Day&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) summary(omnibus.aov) ## Warning in summary.Anova.mlm(object$Anova, multivariate = FALSE): HF eps &gt; ## 1 treated as 1 ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 13493.4 1 41.067 9 2957.153 1.209e-12 *** ## ProcessDepth 432.2 2 61.400 18 63.345 7.137e-09 *** ## Day 254.5 2 95.733 18 23.925 8.521e-06 *** ## ProcessDepth:Day 110.4 4 93.400 36 10.636 8.567e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## ProcessDepth 0.94884 0.81054 ## Day 0.81573 0.44277 ## ProcessDepth:Day 0.22026 0.27277 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## ProcessDepth 0.95133 1.559e-08 *** ## Day 0.84440 3.539e-05 *** ## ProcessDepth:Day 0.71617 0.0001178 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## ProcessDepth 1.199509 7.136593e-09 ## Day 1.018163 8.520701e-06 ## ProcessDepth:Day 1.085791 8.566883e-06 Here we have two main effects and an interaction. Let’s unpack the interaction by taking a look at whether Recall inceases over sucessive days. 13.4.4 running the simple effects ANOVAs It’s recommended that one avoid the use of pooled error terms when performing simple effects analysis of within-subjects ANOVA. This is definitely the case when the spherecity assumption is violated, but as a general rule, even when the assumption is not technically violated, deviations from spherecity can artificially adjust Type I error. So in this case, simple follow up ANOVAs are justified. One way to do this would be, as we have done in the past, seperate out the data and run seperate simple effects ANOVAs: hi.data &lt;- filter(example3_long, ProcessDepth == &quot;High&quot;) med.data &lt;- filter(example3_long, ProcessDepth == &quot;Med&quot;) lo.data &lt;- filter(example3_long, ProcessDepth == &quot;Lo&quot;) hi.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Recalled&quot;, data = hi.data, between = NULL, within = &quot;Day&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) hi.aov ## Anova Table (Type 3 tests) ## ## Response: Recalled ## Effect df MSE F pes p.value ## 1 Day 2, 18 4.59 30.88 *** .77 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 med.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Recalled&quot;, data = med.data, between = NULL, within = &quot;Day&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) med.aov ## Anova Table (Type 3 tests) ## ## Response: Recalled ## Effect df MSE F pes p.value ## 1 Day 2, 18 3.51 11.01 *** .55 .0008 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 lo.aov &lt;- afex::aov_ez(id = &quot;Subject&quot;, dv = &quot;Recalled&quot;, data = lo.data, between = NULL, within = &quot;Day&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) lo.aov ## Anova Table (Type 3 tests) ## ## Response: Recalled ## Effect df MSE F pes p.value ## 1 Day 2, 18 2.40 0.85 .09 .45 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 I’m only printing the ANOVA and not the tests of Sphericity here for the same of brevity, though it’s always a good idea to check those. How about you go ahead an do that right now… I’ll wait. In this spirit of self-reliance, I also leave it to you to perform any necessary post hocs on the hi and med data. Remember however that for each post-hoc you run you need to correct your critical p-value. I imagine you’ll end up running 6 tests, so critical p needs to be .05/6 Bottom line: The simple effects ANOVAs (and subsequent post-hocs) confirm what the figure suggest; that recall increased over sucessive days in the High and Med processing conditions, but not the Low processing condition. 13.5 How to perform a within subjects analysis in SPSS: I’ll likely but together my own vid, but it won’t be much better than this: https://www.youtube.com/watch?v=9TDvejlOP0Q "],
["anova-vi-mixed-effects-anova-bs-ws.html", "Week 14 ANOVA VI: Mixed-effects ANOVA (BS + WS) 14.1 Example 1: 14.2 Example 2: 14.3 to pool or not to pool, this is the yada yada yada", " Week 14 ANOVA VI: Mixed-effects ANOVA (BS + WS) For our final trick, we will be covering mixed effects designs. Mixed models sometimes go by many names (mixed effects model, multi-level model, growth-curve models) depending on the structure of the data that is being analyzed. For this week we will be focusing on Mixed effects ANOVA. This walk-though assumes the following packages: pacman::p_load(tidyverse, cowplot, readr, plyr, afex, multcomp, multcompView, Rmisc) In terms of new concepts, we are continuing our theme of “well, nothing terribly new being raised this week”. You’ve done between-subjects (BS) ANOVA, you’ve done within-subjects (WS) ANOVA, you’ve done simple linear regression… now we are simply combining what you know. We will use the two examples from Howell’s text (Chapter 14): 14.1 Example 1: 14.1.1 data import and wrangling First we import the data, this data and background is presented in Table 14.4 of your Howell text: example1 &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab14-4.dat&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Group = col_integer(), ## Int1 = col_integer(), ## Int2 = col_integer(), ## Int3 = col_integer(), ## Int4 = col_integer(), ## Int5 = col_integer(), ## Int6 = col_integer() ## ) example1 ## # A tibble: 24 x 7 ## Group Int1 Int2 Int3 Int4 Int5 Int6 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 150 44 71 59 132 74 ## 2 1 335 270 156 160 118 230 ## 3 1 149 52 91 115 43 154 ## 4 1 159 31 127 212 71 224 ## 5 1 159 0 35 75 71 34 ## 6 1 292 125 184 246 225 170 ## 7 1 297 187 66 96 209 74 ## 8 1 170 37 42 66 114 81 ## 9 2 346 175 177 192 239 140 ## 10 2 426 329 236 76 102 232 ## # ... with 14 more rows The data above is in wide format. I need to get it into long format before submitting it for further analysis. Before doing so, however, I also need to add a SubjectID to let R know which data belongs to which subject. If you are presented with this sort of situation it makes sense to create your SubjectID column BEFORE you gather(): # create subject column example1$SubjectID &lt;- 1:nrow(example1) # data is in wide format, needs to be long format for R, notice that I only need # to collapse the &#39;Interval&#39; columns (2-7): example1_long &lt;- tidyr::gather(data = example1, key = &quot;Interval&quot;, value = &quot;Activity&quot;, 2:7) # convert &#39;Interval&#39; to factor: example1_long$Interval &lt;- as.factor(example1_long$Interval) # Name the dummy variables for &#39;Group&#39; &amp; convert to factor: example1_long$Group &lt;- recode_factor(example1_long$Group, `1` = &quot;Control&quot;, `2` = &quot;Same&quot;, `3` = &quot;Different&quot;) print(example1_long) ## # A tibble: 144 x 4 ## Group SubjectID Interval Activity ## &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 Control 1 Int1 150 ## 2 Control 2 Int1 335 ## 3 Control 3 Int1 149 ## 4 Control 4 Int1 159 ## 5 Control 5 Int1 159 ## 6 Control 6 Int1 292 ## 7 Control 7 Int1 297 ## 8 Control 8 Int1 170 ## 9 Same 9 Int1 346 ## 10 Same 10 Int1 426 ## # ... with 134 more rows 14.1.2 plotting the data Now we can plot (not worrying about APA here). Given that interval is a within subjects variable we need to make the appropriate corrections to the error bars. For this we call on Morey (2008) recommendations. Corrections can easily be made by calling a customized version of summarySEwithin2 (author: Hause Lin). Running this function on our data yields the following data.table (note that I specify BOTH betweenvars and withinvars in this case: # grabbing custom function source(&quot;https://gist.githubusercontent.com/hauselin/a83b6d2f05b0c90c0428017455f73744/raw/38e03ea4bf658d913cf11f4f1c18a1c328265a71/summarySEwithin2.R&quot;) # creating a summary table repdata &lt;- summarySEwithin2(data = example1_long, measurevar = &quot;Activity&quot;, betweenvars = &quot;Group&quot;, withinvars = &quot;Interval&quot;, idvar = &quot;SubjectID&quot;) show(repdata) ## Group Interval N Activity ActivityNormed sd se ## 1 Control Int1 8 213.875 252.0208 42.93727 15.180619 ## 2 Control Int2 8 93.250 131.3958 60.97203 21.556867 ## 3 Control Int3 8 96.500 134.6458 29.69636 10.499249 ## 4 Control Int4 8 128.625 166.7708 50.29608 17.782348 ## 5 Control Int5 8 122.875 161.0208 61.46319 21.730519 ## 6 Control Int6 8 130.125 168.2708 57.99589 20.504645 ## 7 Different Int1 8 290.125 314.4792 62.57831 22.124775 ## 8 Different Int2 8 98.250 122.6042 37.02406 13.089982 ## 9 Different Int3 8 108.500 132.8542 35.38455 12.510329 ## 10 Different Int4 8 109.000 133.3542 47.96854 16.959440 ## 11 Different Int5 8 123.500 147.8542 24.64742 8.714178 ## 12 Different Int6 8 138.625 162.9792 42.17568 14.911355 ## 13 Same Int1 8 354.625 292.1250 81.80616 28.922844 ## 14 Same Int2 8 266.250 203.7500 58.83091 20.799867 ## 15 Same Int3 8 221.000 158.5000 27.46850 9.711581 ## 16 Same Int4 8 170.000 107.5000 61.61729 21.785000 ## 17 Same Int5 8 198.625 136.1250 56.19523 19.868015 ## 18 Same Int6 8 178.625 116.1250 54.79954 19.374564 ## ci ## 1 35.89646 ## 2 50.97389 ## 3 24.82678 ## 4 42.04857 ## 5 51.38451 ## 6 48.48578 ## 7 52.31678 ## 8 30.95289 ## 9 29.58223 ## 10 40.10270 ## 11 20.60576 ## 12 35.25975 ## 13 68.39166 ## 14 49.18387 ## 15 22.96424 ## 16 51.51334 ## 17 46.98039 ## 18 45.81356 This contains the means (Activity), normed means (ActivityNormed), and estimates of distribution for each Group × Interval Condition. The normed means are calculated by removing the between-subject variability. This is accomplished be ensuring that each participant have the same average (see this link for background and calculations link and re-plot). Values of se, ci, and sd are then calculated on this normed data. For or resulting plot, we use the raw data for our means and the corrected sd, se, or ci for our error bars. Using ggplot, this can be accomplished by using the data from our summary table repdata and using direct calls instead of summary_stat: # create universal position dodge dodge_all &lt;- position_dodge(0.3) # now plot: p &lt;- ggplot(data = repdata, mapping = aes(x = Interval, y = Activity, group = Group)) + geom_pointrange(aes(shape = Group, ymin = repdata$Activity - repdata$se, ymax = repdata$Activity + repdata$se), size = 0.5, position = dodge_all) + geom_line(aes(linetype = Group), size = 1, position = dodge_all) show(p) Now THESE are the error bars we’re looking for! Rememeber however, when reporting the error values, you need to use the actual values and NOT the corrected ones from this plot. For this you could call upon the psych::describeBy function. You could also built your own table using the by() function. My personal preference is a custom tweak of the summarySEwithin2.R that I call withinSummary(): source(&quot;https://raw.githubusercontent.com/tehrandavis/statsRepo/master/statsScripts/withinSummary.R&quot;) You can take a look at what modifications I’m made by comparing both code sources on their respective Github sites. Using this function is the same as summarySEwithin2: repdata &lt;- withinSummary(data = example1_long, measurevar = &quot;Activity&quot;, betweenvars = &quot;Group&quot;, withinvars = &quot;Interval&quot;, idvar = &quot;SubjectID&quot;) repdata ## $Actual ## Group Interval N Activity sd se ci ## 1 Control Int1 8 213.875 79.21118 28.00538 66.22220 ## 2 Control Int2 8 93.250 93.27341 32.97713 77.97852 ## 3 Control Int3 8 96.500 54.10308 19.12833 45.23130 ## 4 Control Int4 8 128.625 70.31346 24.85956 58.78352 ## 5 Control Int5 8 122.875 65.24117 23.06624 54.54299 ## 6 Control Int6 8 130.125 74.54708 26.35637 62.32292 ## 7 Same Int1 8 354.625 89.91415 31.78945 75.17011 ## 8 Same Int2 8 266.250 109.68754 38.78040 91.70108 ## 9 Same Int3 8 221.000 69.88153 24.70685 58.42242 ## 10 Same Int4 8 170.000 78.10798 27.61534 65.29991 ## 11 Same Int5 8 198.625 66.22243 23.41317 55.36334 ## 12 Same Int6 8 178.625 83.59415 29.55500 69.88646 ## 13 Different Int1 8 290.125 69.32210 24.50906 57.95473 ## 14 Different Int2 8 98.250 53.47563 18.90649 44.70674 ## 15 Different Int3 8 108.500 62.66236 22.15449 52.38704 ## 16 Different Int4 8 109.000 52.52754 18.57129 43.91413 ## 17 Different Int5 8 123.500 50.10275 17.71400 41.88695 ## 18 Different Int6 8 138.625 56.01514 19.80434 46.82983 ## ## $Corrected ## Group Interval N Activity ActivityNormed sd se ## 1 Control Int1 8 213.875 252.0208 42.93727 15.180619 ## 2 Control Int2 8 93.250 131.3958 60.97203 21.556867 ## 3 Control Int3 8 96.500 134.6458 29.69636 10.499249 ## 4 Control Int4 8 128.625 166.7708 50.29608 17.782348 ## 5 Control Int5 8 122.875 161.0208 61.46319 21.730519 ## 6 Control Int6 8 130.125 168.2708 57.99589 20.504645 ## 7 Different Int1 8 290.125 314.4792 62.57831 22.124775 ## 8 Different Int2 8 98.250 122.6042 37.02406 13.089982 ## 9 Different Int3 8 108.500 132.8542 35.38455 12.510329 ## 10 Different Int4 8 109.000 133.3542 47.96854 16.959440 ## 11 Different Int5 8 123.500 147.8542 24.64742 8.714178 ## 12 Different Int6 8 138.625 162.9792 42.17568 14.911355 ## 13 Same Int1 8 354.625 292.1250 81.80616 28.922844 ## 14 Same Int2 8 266.250 203.7500 58.83091 20.799867 ## 15 Same Int3 8 221.000 158.5000 27.46850 9.711581 ## 16 Same Int4 8 170.000 107.5000 61.61729 21.785000 ## 17 Same Int5 8 198.625 136.1250 56.19523 19.868015 ## 18 Same Int6 8 178.625 116.1250 54.79954 19.374564 ## ci ## 1 35.89646 ## 2 50.97389 ## 3 24.82678 ## 4 42.04857 ## 5 51.38451 ## 6 48.48578 ## 7 52.31678 ## 8 30.95289 ## 9 29.58223 ## 10 40.10270 ## 11 20.60576 ## 12 35.25975 ## 13 68.39166 ## 14 49.18387 ## 15 22.96424 ## 16 51.51334 ## 17 46.98039 ## 18 45.81356 This gives me both the $Actual and $Corrected values. I can use the repdata$Corrected data for plotting, for example: p &lt;- ggplot(data = repdata$Corrected, mapping = aes(x = Interval, y = Activity, group = Group)) + geom_pointrange(aes(shape = Group, ymin = Activity - se, ymax = Activity + se), size = 0.5, position = dodge_all) + geom_line(aes(linetype = Group), size = 1, position = dodge_all) show(p) and use the values from repdata$Actual when writing my results. 14.1.3 Running our ANOVA: Running the ANOVA in afex is same as before, we just specify BOTH within and between IVs: ex1.aov &lt;- afex::aov_ez(id = &quot;SubjectID&quot;, dv = &quot;Activity&quot;, data = example1_long, between = &quot;Group&quot;, within = &quot;Interval&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Contrasts set to contr.sum for the following variables: Group ex1.aov ## Anova Table (Type 3 tests) ## ## Response: Activity ## Effect df MSE F pes p.value ## 1 Group 2, 21 18320.10 7.80 ** .43 .003 ## 2 Interval 5, 105 2678.09 29.85 *** .59 &lt;.0001 ## 3 Group:Interval 10, 105 2678.09 3.02 ** .22 .002 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 summary(ex1.aov) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 4113798 1 384722 21 224.5511 1.097e-12 *** ## Group 285815 2 384722 21 7.8006 0.002928 ** ## Interval 399737 5 281199 105 29.8524 &lt; 2.2e-16 *** ## Group:Interval 80820 10 281199 105 3.0178 0.002164 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## Interval 0.21121 0.0088268 ## Group:Interval 0.21121 0.0088268 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## Interval 0.65694 4.469e-13 *** ## Group:Interval 0.65694 0.009185 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## Interval 0.793258 2.449168e-15 ## Group:Interval 0.793258 5.142508e-03 Note that sphericity has been violated. I therefore need to make the appropriate corrections for my ANOVA (see ANOVA 5 vignette). In this case a simple re-run of the aov_ez setting correction=&quot;GG&quot; should suffice. 14.1.4 simple effects There are two ways that I can attack the interaction. I can take a look at Group effects on the different levels of Interval; or I can take a look at Interval effects on the different levels of Group. In the first scenario, I’m looking for between effects (Group) on a level of a within factor (Interval). In the second scenario I’m looking for within effects on a level of a between factor. How my simple effects ANOVA nests my within and between factors has implications for how I do my follow-up. 14.1.4.1 by groups (repeated measures) If I’m looking at a within effect, nested within a single level of a between factor (scenario 2), then I only need to run simple within-subjects ANOVAs for each between level that I’m interested in. So, for example if I’m interested in the effect of interval in all three groups, then I just run the separate within subjects ANOVA(s) and call it a day. First we separate the data. You’ve been using filter() from the tidyverse to isolate different groups. You can also use split() like so: byGroup &lt;- split(example1_long, example1_long$Group) Running names(byGroup) reveals this object holds three groups named “Control”, “Same”, &amp; “Different”. Then we run each group separate (note that my output only looks at the ANOVA assuming spherecity. In your own work you should also check spherecity using summary(aov.object)) 14.1.4.2 Control group (byGroup$Control) control.aov &lt;- afex::aov_ez(id = &quot;SubjectID&quot;, dv = &quot;Activity&quot;, data = byGroup$Control, within = &quot;Interval&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) %&gt;% print() ## Anova Table (Type 3 tests) ## ## Response: Activity ## Effect df MSE F pes p.value ## 1 Interval 5, 35 2685.67 5.69 *** .45 .0006 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 14.1.4.3 Same group (byGroup$Same) same.aov &lt;- afex::aov_ez(id = &quot;SubjectID&quot;, dv = &quot;Activity&quot;, data = byGroup$Same, within = &quot;Interval&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) %&gt;% print() ## Anova Table (Type 3 tests) ## ## Response: Activity ## Effect df MSE F pes p.value ## 1 Interval 5, 35 3477.57 11.10 *** .61 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 14.1.4.4 Different group (byGroup$Different) different.aov &lt;- afex::aov_ez(id = &quot;SubjectID&quot;, dv = &quot;Activity&quot;, data = byGroup$Different, within = &quot;Interval&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) %&gt;% print() ## Anova Table (Type 3 tests) ## ## Response: Activity ## Effect df MSE F pes p.value ## 1 Interval 5, 35 1871.03 22.56 *** .76 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 14.1.4.5 by intervals (between measures): In this case we are running between-subjects simple effects for Group on each level of Interval. So let’s start with splitting the data: byInterval &lt;- split(example1_long, example1_long$Interval) As before, when running a simple effects BS ANOVA, we can make corrections to the F-ratio based upon the appropriate Mean Square Error (MSE). However, unlike our examples in the purely BS case, here we cannot simply reach back to our omnibus ANOVA and pluck out that MSE. This is because in the original ANOVA, our error term confounds our between factor Group with our within factor Interval. To conceptually see why this is an issue, I like to point out that within factors have (and are a part of) histories. So say, for example, you were interested in looking at a Group effect on the sixth Interval. I think most of us would agree that what happens on Interval 6 is in some part related to, and dare I say influenced by Intervals 1-5. So, we can’t honestly take a look at Interval 6 without taking into account how individual subjects changed across intervals. Fear not, all is not lost. If we choose (more on this below) to make a correction we do have the means, but it’s slightly more complicated than what we’ve done before. Before proceeding I feel it important to note that I WOULD NOT RECOMMEND using the omnibus error term method for this data. Why? Violations of the spherecity assumptions abound, which under normal circumstances would preclude the following. Having said that, we will proceed for the sake of example. 14.1.4.5.1 making the correction As articulated in Howell, in order to make the correction we need to calculate the \\(MS_{w/in\\space cell}\\) where: \\[SS_{w/in\\space cell} = SS_{Ss\\space w/in\\space group} + SS_{I\\times Ss\\space w/in\\space groups}\\] and \\[MS_{w/in\\space cell} = \\frac{SS_{w/in\\space cell}}{df_{Ss\\space w/in\\space group}+df_{I\\times Ss\\space w/in\\space groups}}\\] To do this, we can simply grab the Error sum of squares of the between variable (Group) and interaction (Group:Interval) in the omnibus ANOVA. The SS can be accessed by performing a summary call on the ex1.aov object, and calling the ‘univariate.tests’ table (this could be found using the attributes function) summary(ex1.aov)$univariate.tests ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 4113798 1 384722 21 224.5511 1.097e-12 *** ## Group 285815 2 384722 21 7.8006 0.002928 ** ## Interval 399737 5 281199 105 29.8524 &lt; 2.2e-16 *** ## Group:Interval 80820 10 281199 105 3.0178 0.002164 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And from here I can type in the values by hand: SSwincell &lt;- 384722 + 281199 MSwincell &lt;- SSwincell/(21 + 105) print(MSwincell) ## [1] 5285.087 Alternatively, I can take what I know about this relationship, \\(SS=MS \\times df\\) and pull the values from ex1.aov$anova_table omnibus.aov &lt;- ex1.aov$anova_table %&gt;% print() ## Anova Table (Type 3 tests) ## ## Response: Activity ## num Df den Df MSE F pes Pr(&gt;F) ## Group 2 21 18320.1 7.8006 0.42625 0.002928 ** ## Interval 5 105 2678.1 29.8524 0.58704 &lt; 2.2e-16 *** ## Group:Interval 10 105 2678.1 3.0178 0.22325 0.002164 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From here we can pull the appropriate values by using indexing. In this case we are indexing via the row names and column names: MSEgroup &lt;- omnibus.aov[&quot;Group&quot;, &quot;MSE&quot;] MSEinter &lt;- omnibus.aov[&quot;Group:Interval&quot;, &quot;MSE&quot;] Dfgroup &lt;- omnibus.aov[&quot;Group&quot;, &quot;den Df&quot;] Dfinter &lt;- omnibus.aov[&quot;Group:Interval&quot;, &quot;den Df&quot;] SSgroup &lt;- (MSEgroup * Dfgroup) SSinter &lt;- (MSEinter * Dfinter) ## now run the calculations as above: SSwincell &lt;- SSgroup + SSinter MSwincell &lt;- SSwincell/(Dfgroup + Dfinter) print(MSwincell) ## [1] 5285.09 This second method is useful as it potentially allows for me to create a function to automatically take care of this in the future (much like we did a few weeks aback). I may get around to that, someday… From here, we can now calculate our corrected F-ratios using this MSE and values obtained from the Simple effects AOV. Here I’m looking at Interval 1: int1.aov &lt;- afex::aov_ez(id = &quot;SubjectID&quot;, dv = &quot;Activity&quot;, data = byInterval$Int1, between = &quot;Group&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Contrasts set to contr.sum for the following variables: Group int1.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Activity ## num Df den Df MSE F pes Pr(&gt;F) ## Group 2 21 6388.2 6.2167 0.37188 0.007576 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 , in particular that our \\(MS_{group}=MSE \\times F\\): MSgroup &lt;- int1.aov$anova_table[&quot;Group&quot;, &quot;MSE&quot;] * int1.aov$anova_table[&quot;Group&quot;, &quot;F&quot;] and recalculate the F-ratio: Fcorrected &lt;- MSgroup/MSwincell print(Fcorrected) ## [1] 7.514189 Ok, not quite done yet, we also need to calculate the appropriate Df \\((g,f\\prime)\\) (see Howell 14.7), where \\(g\\) is the treatment Df and \\[f\\prime=\\frac{(u+v)^2}{\\frac{u^2}{df_u}+\\frac{v^2}{df_v}}\\] Fortunately, in the equation above, \\(u\\) and \\(v\\) are already known to us. They are the SSgroup and SSinter that we calculated from the omnibus ANOVA. So we can calculate \\(f\\prime\\) as: u &lt;- SSgroup v &lt;- SSinter f.prime &lt;- (u + v)^2/((u^2/Dfgroup) + (v^2)/Dfinter) show(f.prime) ## [1] 56.84382 Finally we can calculate our appropriate p-value using pf(): p.corrected &lt;- 1 - pf(Fcorrected, 2, f.prime) show(p.corrected) ## [1] 0.001271868 And there it is. Now go back and do it for Intervals 2-6. Actually don’t (see Type 1 error inflation). Choose another comparison that you think is important and run that one! 14.2 Example 2: Ok, let’s ramp up our complexity here. This time we’re using data from Howell (Table 14.7) with 1 within factor and 2 between factors. 14.2.1 data import and wrangling example2 &lt;- read_delim(&quot;https://www.uvm.edu/~dhowell/methods8/DataFiles/Tab14-7.dat&quot;, delim = &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Person = col_character(), ## Condition = col_integer(), ## Sex = col_integer(), ## Pretest = col_character(), ## Posttest = col_character(), ## FU6 = col_character(), ## FU12 = col_character() ## ) ## Warning in rbind(names(probs), probs_f): number of columns of result is not ## a multiple of vector length (arg 2) ## Warning: 1 parsing failure. ## row # A tibble: 1 x 5 col row col expected actual file expected &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; actual 1 34 &lt;NA&gt; 7 columns 8 colum… &#39;https://www.uvm.edu/~dhowell/methods8/D… file # A tibble: 1 x 5 example2 ## # A tibble: 40 x 7 ## Person Condition Sex Pretest Posttest FU6 FU12 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01 1 1 07 22 13 14 ## 2 02 1 1 25 10 17 24 ## 3 03 1 1 50 36 49 23 ## 4 04 1 1 16 38 34 24 ## 5 05 1 1 33 25 24 25 ## 6 06 1 1 10 07 23 26 ## 7 07 1 1 13 33 27 24 ## 8 08 1 1 22 20 21 11 ## 9 09 1 1 04 00 12 00 ## 10 10 1 1 17 16 20 10 ## # ... with 30 more rows You note that this time around, there is a subject ID, Person so no need to add that. From here we can gather the data (in columns 4-7) into long format with Time as the created factor: example2_long &lt;- tidyr::gather(data = example2, key = &quot;Time&quot;, value = &quot;Freq&quot;, 4:7) and give names to our dummy variables: example2_long$Condition &lt;- recode_factor(example2_long$Condition, `1` = &quot;BST&quot;, `2` = &quot;EC&quot;) example2_long$Sex &lt;- recode_factor(example2_long$Sex, `1` = &quot;M&quot;, `2` = &quot;F&quot;) example2_long ## # A tibble: 160 x 5 ## Person Condition Sex Time Freq ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01 BST M Pretest 07 ## 2 02 BST M Pretest 25 ## 3 03 BST M Pretest 50 ## 4 04 BST M Pretest 16 ## 5 05 BST M Pretest 33 ## 6 06 BST M Pretest 10 ## 7 07 BST M Pretest 13 ## 8 08 BST M Pretest 22 ## 9 09 BST M Pretest 04 ## 10 10 BST M Pretest 17 ## # ... with 150 more rows Also, for some reason this data set wants to treat our Freq values as a character string. This was the case from the first import, but it’s much easier to wait and address it now (only a single column call): example2_long$Freq &lt;- as.numeric(example2_long$Freq) 14.2.2 plotting the data First we need to create the data for the appropriate error bars repdata &lt;- withinSummary(data = example2_long, measurevar = &quot;Freq&quot;, betweenvars = c(&quot;Condition&quot;, &quot;Sex&quot;), withinvars = &quot;Time&quot;, idvar = &quot;Person&quot;) ## Automatically converting the following non-factors to factors: Time show(repdata) ## $Actual ## Condition Sex Time N Freq sd se ci ## 1 BST M FU12 10 18.1 8.812239 2.786675 6.303896 ## 2 BST M FU6 10 24.0 10.923980 3.454466 7.814544 ## 3 BST M Posttest 10 20.7 12.728359 4.025060 9.105319 ## 4 BST M Pretest 10 19.7 13.727912 4.341147 9.820356 ## 5 BST F FU12 10 10.2 12.682446 4.010542 9.072476 ## 6 BST F FU6 10 13.6 8.821690 2.789663 6.310657 ## 7 BST F Posttest 10 9.8 8.978988 2.839405 6.423181 ## 8 BST F Pretest 10 7.2 11.193252 3.539617 8.007170 ## 9 EC M FU12 10 15.1 14.231811 4.500494 10.180824 ## 10 EC M FU6 10 7.5 10.480140 3.314111 7.497040 ## 11 EC M Posttest 10 18.8 23.757104 7.512656 16.994809 ## 12 EC M Pretest 10 29.5 25.123031 7.944600 17.971933 ## 13 EC F FU12 10 9.5 15.211472 4.810290 10.881632 ## 14 EC F FU6 10 9.7 12.138552 3.838547 8.683397 ## 15 EC F Posttest 10 10.0 13.063945 4.131182 9.345383 ## 16 EC F Pretest 10 10.1 13.008117 4.113528 9.305446 ## ## $Corrected ## Condition Sex Time N Freq FreqNormed sd se ci ## 1 BST F FU12 10 10.2 14.59375 8.346656 2.639444 5.970838 ## 2 BST F FU6 10 13.6 17.99375 6.493872 2.053543 4.645436 ## 3 BST F Posttest 10 9.8 14.19375 7.837706 2.478500 5.606757 ## 4 BST F Pretest 10 7.2 11.59375 8.382080 2.650646 5.996179 ## 5 BST M FU12 10 18.1 12.06875 8.148449 2.576766 5.829049 ## 6 BST M FU6 10 24.0 17.96875 4.914548 1.554116 3.515656 ## 7 BST M Posttest 10 20.7 14.66875 8.122956 2.568704 5.810812 ## 8 BST M Pretest 10 19.7 13.66875 9.046229 2.860669 6.471282 ## 9 EC F FU12 10 9.5 14.26875 10.573421 3.343609 7.563769 ## 10 EC F FU6 10 9.7 14.46875 11.847996 3.746665 8.475545 ## 11 EC F Posttest 10 10.0 14.76875 6.878644 2.175218 4.920685 ## 12 EC F Pretest 10 10.1 14.86875 4.814042 1.522334 3.443758 ## 13 EC M FU12 10 15.1 11.96875 13.049230 4.126529 9.334857 ## 14 EC M FU6 10 7.5 4.36875 14.446869 4.568501 10.334668 ## 15 EC M Posttest 10 18.8 15.66875 15.586111 4.928761 11.149632 ## 16 EC M Pretest 10 29.5 26.36875 14.284445 4.517138 10.218476 Now we can plot (let’s do some APA here): # create universal position dodge dodge_all &lt;- position_dodge(0.3) # now plot: p &lt;- ggplot(data = repdata$Corrected, mapping = aes(x = Time, y = Freq, group = interaction(Condition, Sex))) + geom_pointrange(aes(shape = Condition, ymin = Freq - se, ymax = Freq + se), size = 0.5, position = dodge_all) + geom_line(aes(linetype = Sex), size = 1, position = dodge_all) + theme_cowplot() + # aesthetics theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.2, 0.85)) + scale_color_manual(values = c(&quot;black&quot;, &quot;grey50&quot;)) + xlab(&quot;\\n Time&quot;) + ylab(&quot;Freq \\n&quot;) + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), &quot;in&quot;)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) The order along the x-axis is the reverse of what I’d like. Since I’m plotting using repdata I need to change the order of my levels there. I can change this by: repdata$Corrected$Time &lt;- factor(repdata$Corrected$Time, levels = c(&quot;Pretest&quot;, &quot;Posttest&quot;, &quot;FU6&quot;, &quot;FU12&quot;)) and now re-plot (adjusting the position of my legend accordingly): # create universal position dodge dodge_all &lt;- position_dodge(0.3) # now plot: p &lt;- ggplot(data = repdata$Corrected, mapping = aes(x = Time, y = Freq, group = interaction(Condition, Sex))) + geom_pointrange(aes(shape = Condition, ymin = Freq - se, ymax = Freq + se), size = 0.5, position = dodge_all) + geom_line(aes(linetype = Sex), size = 1, position = dodge_all) + theme_cowplot() + # APA ify theme(axis.title = element_text(size = 16, face = &quot;bold&quot;, lineheight = 0.55), axis.text = element_text(size = 12), legend.title = element_text(size = 12, face = &quot;bold&quot;), legend.position = c(0.2, 0.85)) + scale_color_manual(values = c(&quot;black&quot;, &quot;grey50&quot;)) + xlab(&quot;\\n Time&quot;) + ylab(&quot;Freq \\n&quot;) + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), &quot;in&quot;)) + # stack legend boxes horizontally: theme(legend.box = &quot;horizontal&quot;) show(p) On to our ANOVAs!! 14.2.3 Run your omnibus ANOVA: ex2.aov &lt;- afex::aov_ez(id = &quot;Person&quot;, dv = &quot;Freq&quot;, data = example2_long, between = c(&quot;Sex&quot;, &quot;Condition&quot;), within = &quot;Time&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Contrasts set to contr.sum for the following variables: Sex, Condition ex2.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Freq ## num Df den Df MSE F pes Pr(&gt;F) ## Sex 1 36 498.92 6.7306 0.157512 0.013623 * ## Condition 1 36 498.92 0.2150 0.005936 0.645687 ## Sex:Condition 1 36 498.92 0.1278 0.003537 0.722825 ## Time 3 108 101.91 0.8965 0.024297 0.445578 ## Sex:Time 3 108 101.91 2.5511 0.066174 0.059435 . ## Condition:Time 3 108 101.91 4.5068 0.111259 0.005097 ** ## Sex:Condition:Time 3 108 101.91 1.5583 0.041491 0.203741 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And here we see a main effect for Sex and a Condition:Time interaction. Now to follow-up… 14.2.4 simple effects: Similar to our first example there are two ways in which we can address that Interaction. We can look for simple (within) effects for Time on each of the between factors (Condition) that we are interested in, or we can look at simple between effects for Condition on each Time level of interest. 14.2.4.1 by condition (within nested in between): Here we’re running a simple effects within ANOVA for Time on each level of Condition. Importantly, as Time is also nested within Sex, we need to also include Sex in our simple effects follow-ups: byCondition &lt;- split(example2_long, example2_long$Condition) ex2.bst.aov &lt;- afex::aov_ez(id = &quot;Person&quot;, dv = &quot;Freq&quot;, data = byCondition$BST, within = &quot;Time&quot;, between = &quot;Sex&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Contrasts set to contr.sum for the following variables: Sex ex2.bst.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Freq ## num Df den Df MSE F pes Pr(&gt;F) ## Sex 1 18 315.307 6.8936 0.276924 0.01715 * ## Time 3 54 60.237 1.8756 0.094367 0.14466 ## Sex:Time 3 54 60.237 0.3018 0.016490 0.82395 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our analysis of the BST group uncovers a simple main effect for Sex; this is in agreement with the result of our omnibus ANOVA. Simply, for the BST group, Sex matters. ex2.ec.aov &lt;- afex::aov_ez(id = &quot;Person&quot;, dv = &quot;Freq&quot;, data = byCondition$EC, within = &quot;Time&quot;, between = &quot;Sex&quot;, type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Contrasts set to contr.sum for the following variables: Sex ex2.ec.aov$anova_table ## Anova Table (Type 3 tests) ## ## Response: Freq ## num Df den Df MSE F pes Pr(&gt;F) ## Sex 1 18 682.54 1.8288 0.092227 0.19302 ## Time 3 54 143.58 3.0482 0.144818 0.03633 * ## Sex:Time 3 54 143.58 2.7901 0.134204 0.04916 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our result for the EC group yields a Sex × Time interaction. Again, we needed to include Sex in this simple effects ANOVA as Time was nested underneath it. That is, we cannot take into account our Time effects without understanding that they are confounded with Sex. This actually ends up being important here as the interaction tells us that in the EC group, Time matters more for one Sex than it does another. You could then tease apart this interaction as you would typically do in a repeated measures ANOVA. 14.2.4.2 by Time (between nested in within): Now to look at the alternative, our Condition effects on each level of Time. Here, for brevity I’m only running Pretest: byTime &lt;- split(example2_long, example2_long$Time) ex2.pretest.aov &lt;- afex::aov_ez(id = &quot;Person&quot;, dv = &quot;Freq&quot;, data = byTime$Pretest, between = c(&quot;Sex&quot;, &quot;Condition&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Contrasts set to contr.sum for the following variables: Sex, Condition summary(ex2.pretest.aov) ## Anova Table (Type 3 tests) ## ## Response: Freq ## num Df den Df MSE F pes Pr(&gt;F) ## Sex 1 36 278.53 9.1337 0.202371 0.004601 ** ## Condition 1 36 278.53 1.4477 0.038659 0.236753 ## Sex:Condition 1 36 278.53 0.4273 0.011731 0.517455 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 and Posttest: ex2.posttest.aov &lt;- afex::aov_ez(id = &quot;Person&quot;, dv = &quot;Freq&quot;, data = byTime$Posttest, between = c(&quot;Sex&quot;, &quot;Condition&quot;), type = 3, return = &quot;afex_aov&quot;, anova_table = list(es = &quot;pes&quot;, correction = &quot;none&quot;)) ## Contrasts set to contr.sum for the following variables: Sex, Condition summary(ex2.posttest.aov) ## Anova Table (Type 3 tests) ## ## Response: Freq ## num Df den Df MSE F pes Pr(&gt;F) ## Sex 1 36 244.43 3.9694 0.099311 0.05396 . ## Condition 1 36 244.43 0.0296 0.000820 0.86446 ## Sex:Condition 1 36 244.43 0.0451 0.001251 0.83301 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 14.3 to pool or not to pool, this is the yada yada yada At this point we have a decision to make. In Example 1, when taking a look at different levels of a between-factor (Sex, Condition) on a level of a within we elected to make corrections for our Error terms, and subsequently or F-ratio and degrees of freedom. We made this choice as we get more power if we pool our variances, and the omnibus ANOVA is a perfect source as it is the source for a subsequent analyses. With a between-subjects ANOVA we can feel safe about doing this as long as we have no serious violations of the homogeneity of variance. That is if all variances are (approximately) equal then we can feel comfortable about collapsing them to a single value. However, if there are significant deviations (as reported in Levine’s test) then it would be unwise to do so (even in the purely between-subjects ANOVA case). The week that we did within-subjects ANOVA, we noted that in this case we don’t make a correction. At present, hopefully you see that the correction is a pooling of variance. In within-subject designs we are almost guaranteed to see some deviations in compound symmetry (and sphericity). In fact, the entire point of the analysis is too account for this. Put another way, the within-subjects ANOVA relaxes the homogeneity assumption, AND implicitly concedes that when conducting your within-subject simple effects follow-ups that you are not granted that variances are similar from one level to the next. So, in within subjects designs the recommendation is to NOT pool your variances and thus, no need for corrections on the simple effects ANOVA(s), see Boik (1981). This goes for post-hocs as well. So… when running purely between ANOVA you pool (assuming no homogeneity of varience violation), when running purely within ANOVA you don’t pool—what to do when running a mixed ANOVA with between and within factor(s). Well when running a simple effects within ANOVA on each level of the between factor the answer is easy. As we noted in Example 1 above, you treat each analysis as a separate within-subjects and no pooling / correction necessary. What about when taking a look at differences in the between factor(s) on each level of within? In Example 1 we pooled (made our corrections), noting the issues with pooling a between nested in within. The detailed process of our corrections is a best attempt to overcome this issue. Typically in simple designs (and assuming no serious violations of sphericity and homogeneity) this is the best option. Our second example our design is slightly more complex. With more complex designs you are more likely to implicitly violate your assumptions of homogeneity. So what to do? As Howell notes, if the variances are indeed homogeneous, then you are well within your right to pool the between subjects variances. Again doing this essentially gives you a power boost making you more likely to uncover effects that are present. For example making our correction in related to ex2.posttest.aov might be enough to get that Sex effect below .05. BUT if homogeneity is violated then you should not pool. "],
["plotting-histograms-and-probability-distributions.html", "A Plotting histograms and probability distributions A.1 The Basics A.2 A note about your BONUS on Week 4. A.3 Advanced stuff", " A Plotting histograms and probability distributions Hi all, a few of you have asked for something a little more concrete with respect to producing APA tables and figures in R. Before continuing on, let me say that the “Intro to tidyverse” and “Data Visualization with ggplot, pts 1 &amp; 2” provide excellent tutorials on using ggplot to contruct a wide variety of figures. If you are having issues with the fundamentals of ggplot then I would suggest starting there (with the acknowledgement that Data Visualization with ggplot, pt 2 may be a little overkill for this course). Here I’m going to walk you thru the construction of a histogram plots and lay out the logic of getting those plots in appropriate APA format. For the purposes of keeping things simple I’m going to focus on the types of data that we have encountered in class so far, namely frequency data and data with two means. As own designs and analyses become more complex, I will have sections at the end of each week that build upon what we do here (scatterplots, regression plots, interaction plots, oh my!) There are a few packages that we will introduce in this walkthrough, including: devtools, cowplot, and plotly. I’ll go into further detail what they can do for us as we move along, but for now let’s just load in the one we are most familiar with: pacman::p_load(tidyverse) For the purposes of this walk-through I will be using the FacultyIncome.txt dataset from Week 3’s homework. If you want to follow along, you’ll need to import that data (I’ve saved it to the object FacultyIncome) A.1 The Basics In general, the basic procedure for constructing a plot goes throu the following steps: link to the data and tell ggplot how to structure its output tell ggplot what kind of plot to make adjust the axes and labels if necessary adjust the appearance of the plot for APA (or other appropriate format) add a legend if necessary save or export the plot We won’t be doing much in terms of a legend today, but we will address legends as they become necessary (on the other side of the midterm). A.1.1 Step 1: Building the canvas Here you need to be thinking about what form you want the plot to take. Key points: data =: what data set you will be pulling from. This needs to be in the form of a data_frame(), with names in the headers. For most data that we will be working with from here on out. That will be the case, though for constructed / simulation data you may have to do this manually. If you are unsure of the header names you can see them by: names(FacultyIncome) ## [1] &quot;Gender&quot; &quot;Height&quot; &quot;Income&quot; Note that depending on what guide you follow you may also include mapping=aes() here as well. This would include telling R eventually what info goes on each axis, how data is grouped, etc. BUT this info can also be relayed in Step 2, and I think that conceptually it make more sense to put it there. So for Step 1, just tell ggplot what data we are using and save our Step 1 to an object called p p &lt;- ggplot(data = FacultyIncome) show(p) As you can see we’ve created a blank canvas—nothing else to see here. A.1.2 Step 2: Tell ggplot what kind of plot to make. Plots can take several forms, or geoms (geometries), most common include: + histogram: geom_histogram + boxplot: geom_boxplot + scatter: geom_point + bar: geom_bar + line: geom_line Here we are creating a histogram, so in Step 2 we add geom_histogram() to our original plot p. We also need to tell ggplot how to go about constructing our histogram. This info would be including in the mapping = aes() argument. This tells ggplot about the general layout of the plot, including: + x: what is on the x-axis + y: what’s on the y-axis (usually your dv, although for histograms this ends up being frequency and x is your dv) + group: how should the data be grouped? This will become important for more complex designs. What aes() options you choose in large part is determined by what kind of plot you intend to make. For our histogram we want bins of Income on the x-axis and frequency ..count.. on the y-axis. # Repeating previous step for clarity: p &lt;- p &lt;- ggplot(data = FacultyIncome) # new step: p &lt;- p + geom_histogram(mapping = aes(x = Income, y = ..count..)) # take our original &#39;p&#39;, add a geom, save the new plot to &#39;p&#39; # show the result show(p) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The default geom_histogram() produces the above plot. However, we can tweak several arguments in geom_histogram() to change the presentation of data, including: binwidth: the width of each bin, or… bins: the number of bins color: the color of the perimeter of each bin/bar (note color refers to lines) fill: the color of the bin/bar itself (note fill refers to spaces) mapping = aes(): change the mapping (see below) For example, toget this in APA format we would like light gray bars with black lines. Rather than the default 30 bins, we elect to use the rule of thumb \\(\\sqrt{N}\\). p &lt;- p &lt;- ggplot(data = FacultyIncome) #step 1 # before plotting get the get the number of observations N &lt;- nrow(FacultyIncome) # step 2: p &lt;- p + geom_histogram(mapping = aes(x = Income, y = ..count..), fill = &quot;light gray&quot;, color = &quot;black&quot;, bins = sqrt(N)) # show the result show(p) Not sure if increasing the number of bins here actually improves things (it’s really a subjective choice), but let’s stick with this. You may elect to convey the information in probability rather than frequency count. To do this you can modify the aes() within geom_histogram. For example, modifying the previous chunk: p &lt;- p &lt;- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1 N &lt;- nrow(FacultyIncome) # get the number of observations p &lt;- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N fill=&quot;light gray&quot;, color=&quot;black&quot;, bins = sqrt(N)) # show the result show(p) A.1.3 Step 3: Adjust the axes and labels The plot at the end of Step 2 is almost there, but there are a few issues that remain. First, our axis labels could be more desciptive than “count/N” and “Income”. This is solved by adding the arguments xlab() and ylab() to our plot. For example: # step 1: p &lt;- p &lt;- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1 # step 2: N &lt;- nrow(FacultyIncome) # get the number of observations p &lt;- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N fill=&quot;light gray&quot;, color=&quot;black&quot;, bins = sqrt(N)) # step 3: p &lt;- p + xlab(&quot;FY2018 Salary&quot;) + ylab(&quot;Proportion of UC Professors&quot;) show(p) Another issue may be that gap that sits between the x-axis (x=0) and the axis scale. This can be remedied by adding the following to our plot p: p &lt;- p + coord_cartesian(xlim = c(10000, 80000), ylim = c(0, 0.06), expand = FALSE) The coord_cartesian() command allows us to zoom in or zoom out of our axes. There are several arguments that this command takes including: xlim=: takes a pair of values c(lower,upper) for limits of x-axis ylim=: same as above, but for y-axis expand=: do you want to create additional whitespace between your data and axes? For example if I wanted to only show Incomees with Proportions of UC Professors between .05 and .06 I can just zoom into my plot (with needing to go back and filter my original data) p &lt;- p + coord_cartesian(xlim = c(15000, 80000), ylim = c(0.05, 0.06), expand = FALSE) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. show(p) Or if I instead wanted to only focus on those Profs making between 35K and 65K: p &lt;- p + coord_cartesian(xlim = c(35000, 65000), ylim = c(0, 0.06), expand = FALSE) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. show(p) Not something I would in this case, but just an example. OK, let’s get back to our working plot: # step 1: p &lt;- p &lt;- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1 # step 2: N &lt;- nrow(FacultyIncome) # get the number of observations p &lt;- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N fill=&quot;light gray&quot;, color=&quot;black&quot;, bins = sqrt(N)) # step 3: p &lt;- p + xlab(&quot;FY2018 Salary&quot;) + ylab(&quot;Proportion of UC Professors&quot;) + coord_cartesian(xlim=c(15000, 80000), ylim=c(0,.06), expand = FALSE) show(p) One last thing to consider regarding our axes are the location of labels on the scale. For example the y-axis has labels at every .01 and the x-axis at every 20K. What if we want to change the labels on the x-axis? Here we can add scale_y_continuous to our plot p. We need to tell scale_y_continuous what our desired sequence is. You’ve done sequences before, like the sequence from 0 to 10: 0:10 ## [1] 0 1 2 3 4 5 6 7 8 9 10 However, if we want to say count by 2’s the command is a little more involved seq(0, 10, 2) ## [1] 0 2 4 6 8 10 where seq(start, stop, by). So to start at 20K and stop at 70K going by 10K in our plot we add breaks= seq(20000, 80000, 10000): p &lt;- p &lt;- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1 # step 2: N &lt;- nrow(FacultyIncome) # get the number of observations p &lt;- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N fill=&quot;light gray&quot;, color=&quot;black&quot;, bins = sqrt(N)) # step 3: p &lt;- p + xlab(&quot;FY2018 Salary&quot;) + ylab(&quot;Proportion of UC Professors&quot;) + coord_cartesian(xlim=c(15000, 80000), ylim=c(0,.06), expand = FALSE) + scale_x_continuous(breaks = seq(20000,70000,10000)) show(p) A.1.4 Step 4: Adjusting for APA While the plot at the end of Step 3 is almost there, there are a few issues remaining, Including that grey grid in the background and the lack of axis lines. In the past these changes would need to be done line by line. But fortunately for us we are in the glorious present, and there is a library that does a lot of this cleaning for you. Introducting cowplot!!!! pacman::p_load(cowplot) OK, we’ve loaded cowplot. Now what? Simply re-run the previous chunk, but this time adding theme_cowplot() to the end. Note that running ? theme_cowplot shows that it includes arguments for font size, font family (type), and line size. Below I’m just going to ask for size “15” font. I typically leave the font type alone, but if I do change it I may occasionally use font_family=&quot;Times&quot;: p &lt;- ggplot(data = FacultyIncome, mapping = aes(x = Income)) #step 1 # step 2: N &lt;- nrow(FacultyIncome) # get the number of observations p &lt;- p + geom_histogram(mapping = aes(x=Income,y=..count../N), # divide count by total N fill=&quot;light gray&quot;, color=&quot;black&quot;, bins = sqrt(N)) # step 3: p &lt;- p + xlab(&quot;FY2018 Salary&quot;) + ylab(&quot;Proportion of UC Professors&quot;) + coord_cartesian(xlim=c(15000, 80000), ylim=c(0,.06), expand = FALSE) + scale_x_continuous(breaks = seq(20000,70000,10000)) + theme_cowplot(font_size = 15) show(p) Easy-peasy! You’ll note that cowplot adjusted your fonts, fixed your axes, and removed that nasty background. Even more, once cowplot is loaded it doesn’t need to be called to do this. It just sits in the background and automatically adjusts the format of any plot you make using ggplot. If you ever wanted to return to the default plot style you can run the following: theme_set(theme_gray()) But for now cowplot FTW!!! (other cool cowplot things will show up in the advanced section). A.1.5 Step 5: Add and adjust the legend Doesn’t make sense in this context so not going to spend a lot of time here, but more on this when it’s relevent (see you in a few weeks) A.1.6 Step 6: Save the ggplot. Within command-line, plots can be saved using the ggsave() function. Note that both cowplot and ggplot libraries have a ggsave() function. They for the most part do the same thing. If you’ve installed cowplot, your computer will just default to that. Lets get some help on ggsave() to see exactly what the parameters are: `?`(ggsave()) from the result the important arguments are: filename = Filename of plot plot = Plot to save, defaults to last plot displayed. device = what kind of file, depending on extension. path = Path to save plot to (defaults to project folder). scale = Scaling factor. width = Width (defaults to the width of current plotting window). height = Height (defaults to the height of current plotting window). units = Units for width and height (in, cm, or mm). dpi = DPI to use for raster graphics. So to save the previous plot to my project folder, p as a .pdf to a file names “histogram.pdf” with the dimensions 8-in by 6-in, and a DPI of 300 (300 is usually the min you want for print) ggsave(filename = &quot;histogram.pdf&quot;, plot = p, width = 8, height = 6, units = &quot;in&quot;, dpi = 300) if I wanted an image file like a .png, I just change the filename extension: ggsave(filename = &quot;histogram.png&quot;, plot = p, width = 8, height = 6, units = &quot;in&quot;, dpi = 300) Note that you can also copy and save plots that are printed in your Notebook and Plots tab. In the notebook, simply right click on top of the plot for a quick Copy or Download. For plots printed to the Plots tab, click the Export button (this gives you all the options as ggsave). A.2 A note about your BONUS on Week 4. In the example histogram we generated a frequency plot based on counts from our data. We converted it to probability data by dividing our counts by the total number of observations. If you were to run sufficiently large simulation of draws and follow the steps above) modifying as needed) you should be create something approximating what is asked for Week 4’s bonus. However, you can also produce a plot from the table you created using the dbinom() function by modifying the mapping of y. Here, I’m going to re-create the probTable of possible outcomes from Week 4.5: # 1. range of possibilites numberHeads &lt;- 0:10 # 2. prob of outcome probHead &lt;- dbinom(x = numberHeads, size = 10, prob = 0.65) # 3. combine to data frame probTable &lt;- data_frame(numberHeads, probHead) # 4. Show the data frame (table) show(probTable) ## # A tibble: 11 x 2 ## numberHeads probHead ## &lt;int&gt; &lt;dbl&gt; ## 1 0 0.0000276 ## 2 1 0.000512 ## 3 2 0.00428 ## 4 3 0.0212 ## 5 4 0.0689 ## 6 5 0.154 ## 7 6 0.238 ## 8 7 0.252 ## 9 8 0.176 ## 10 9 0.0725 ## 11 10 0.0135 So following my steps, Step 1: p &lt;- ggplot(data = probTable) names(probTable) # I want to see my header names for step 2 ## [1] &quot;numberHeads&quot; &quot;probHead&quot; Step 2: Here, if I try to use geom_histogram, the software won’t let me because the data that I have is not count data. What can I do? Creat a column plot instead. So substituting geom_col: # Step 2 p &lt;- p + geom_col(mapping = aes(x = numberHeads, y = probHead)) # show the result show(p) From here you can follow Steps 3 and 4 to achieve your desired result. A.3 Advanced stuff For other cool stuff that can be done with cowplot() including placing multiple plots side by side, check out the signette links below written by its author, Claus O. Wilke: Introduction to cowplot Changing the axis positions Plot annotations Arranging plots in a grid Shared legends "]
]
